{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54bac1e6",
   "metadata": {},
   "source": [
    "# Using LLMs in Humanities Research via API\n",
    "\n",
    "## Session 3 16.00-17.30 - Intermediate LLM Usage and Advanced API Features\n",
    "\n",
    "Additionally, we will examine challenges associated with historical digitized texts, including optical character recognition (OCR) errors, which may affect compatibility with language models. Participants will gain insights into how these models can be leveraged for error correction and translation, enhancing the usability of imperfect textual data.\n",
    "\n",
    "## Session Outline\n",
    "\n",
    "- **Advanced API Features**: Exploring advanced features of the OpenRouter API, including model selection, temperature settings, and response formatting.\n",
    "- **Handling OCR Errors**: Discussing the challenges of working with historical digitized texts, including OCR errors, and how LLMs can assist in correcting these errors.\n",
    "- **Practical Exercises**: Hands-on exercises to apply the concepts learned, including making API calls with advanced parameters and processing responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4575360",
   "metadata": {},
   "source": [
    "I'll add the corpus loading and verification code for Rigasche Zeitung to session 3, following the same structure as session 2 but adapted for the historical German newspaper corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e2f139",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "\n",
    "## BSSDH 2025 Workshop Data - Rigasche Zeitung Corpus\n",
    "\n",
    "Before we dive into advanced LLM features and OCR error correction, let's load and examine our corpus of historical documents. For this session, we'll be working with the **Rigasche Zeitung** corpus, which presents unique challenges due to its historical nature and OCR quality.\n",
    "\n",
    "Data for workshops in [Baltic Summer School of Digital Humanities 2025](https://www.digitalhumanities.lv/bssdh/2025/about/)\n",
    "\n",
    "**Repository:** https://github.com/LNB-DH/BSSDH_2025_workshop_data\n",
    "\n",
    "## CORPUS OVERVIEW - RIGASCHE ZEITUNG\n",
    "\n",
    "### 1. SOURCE MATERIAL\n",
    "\n",
    "| Periodical | Details |\n",
    "|------------|---------|\n",
    "| **\"Rigasche Zeitung\" (RZei) (1918‚Äì1919)** | - **Data file:** `Rigasche_Zeitung_1918_1919.zip`<br>- **Download Rigasche Zeitung:** https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Rigasche_Zeitung_1918_1919.zip<br>- Morning newspaper, intermittently published from 1778 to 1919 in Riga.<br>- **Language:** German (Fraktur script)<br>- Once the most popular morning paper in the Baltic provinces of the Russian Empire.<br>- Covered general political and economic news in Riga, the Baltics, the Russian Empire, and internationally.<br>- **Historical context:** World War I, Latvian War of Independence.<br>- Link: https://periodika.lv/#periodicalMeta:234;-1<br>- More info: https://enciklopedija.lv/skirklis/163962 |\n",
    "\n",
    "### 2. CORPUS CHARACTERISTICS\n",
    "\n",
    "| Metric | RZei (1918-1919) |\n",
    "|--------|------------------|\n",
    "| **Token Count (words)** | 5.37 million |\n",
    "| **Issue Count** | 359 issues |\n",
    "| **Segment (Article ‚ü∑ File) Count** | 4,597 |\n",
    "| **Language** | German |\n",
    "| **Script** | Fraktur |\n",
    "| **OCR Quality** | Lower than modern texts (historical challenges) |\n",
    "\n",
    "**Filename Structure:**\n",
    "Format: `[periodical][year][volume#][issue#]_[page#]_[plaintext]_[segment#]`\n",
    "\n",
    "Example: `rzei1918s01n001_001_plaintext_s01.txt`\n",
    "         ‚Üí 1st segment from RZei, Issue 1, 1918, page 1.\n",
    "\n",
    "### 3. HISTORICAL CONTEXT & CHALLENGES\n",
    "\n",
    "#### **Why Rigasche Zeitung is Perfect for OCR Error Studies:**\n",
    "\n",
    "1. **Historical Script:** Fraktur typeface presents unique OCR challenges\n",
    "2. **Wartime Period:** 1918-1919 covers end of WWI and Latvian independence\n",
    "3. **Print Quality:** Historical printing technology affects digitization quality\n",
    "4. **Language Complexity:** Early 20th century German with period-specific terminology\n",
    "5. **Physical Degradation:** Age-related paper deterioration impacts OCR accuracy\n",
    "\n",
    "#### **Research Applications:**\n",
    "- **OCR Error Correction:** Testing LLM capabilities on historical text\n",
    "- **Historical Event Analysis:** WWI aftermath, Latvian independence movement\n",
    "- **Language Evolution:** German language usage in the Baltic region\n",
    "- **Cross-cultural Studies:** German-language perspective on Baltic events\n",
    "- **Translation Challenges:** Historical German to modern languages\n",
    "\n",
    "### 4. METHODOLOGY\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| **4.1. Source Access** | Digitised issues obtained from the National Library of Latvia (https://periodika.lv/) |\n",
    "| **4.2. Processing & OCR** | CCS docWORKS & ABBYY FineReader 9.0<br>- **Note:** RZei has lower OCR quality than modern texts<br>- No further data cleaning/normalization (preserves authentic OCR errors) |\n",
    "| **4.3. Metadata Added** | Fields: title, author, uri<br>- **Author info:** 325 cases (7.05%)<br>- **Title availability:** 99.15%<br>- **URI coverage:** 100%<br>- URIs point to LNB DOM system |\n",
    "\n",
    "**üí° Important Note:** The deliberate preservation of OCR errors in this corpus makes it ideal for testing LLM-based error correction techniques, which we'll explore in this session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a8d4c",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "\n",
    "## Extracting Historical Documents\n",
    "\n",
    "Working with historical corpora requires careful handling of data extraction and validation. Unlike modern digital texts, historical documents present unique challenges that we need to account for in our workflow.\n",
    "\n",
    "### Additional Considerations for Historical Documents\n",
    "\n",
    "#### **Data Integrity Concerns:**\n",
    "* **Encoding Issues:** Historical texts may contain unusual characters or encoding problems\n",
    "* **File Structure:** Complex directory hierarchies may reflect archival organization\n",
    "* **Metadata Preservation:** Historical context information must be maintained\n",
    "* **Version Control:** Track which OCR version or processing method was used\n",
    "\n",
    "#### **Technical Considerations:**\n",
    "* **Storage Location:** Consistent paths for local and cloud environments (Google Colab)\n",
    "* **File Naming:** Preserve original archival naming conventions while ensuring accessibility\n",
    "* **Error Handling:** Graceful handling of corrupted or incomplete files\n",
    "* **Validation:** Verify extracted content matches expected corpus characteristics\n",
    "\n",
    "#### **Research Workflow Integration:**\n",
    "* **Reproducibility:** Document exact extraction procedures for research replication\n",
    "* **Scalability:** Prepare for processing large numbers of historical documents\n",
    "* **Compatibility:** Ensure extracted data works with downstream LLM processing\n",
    "* **Backup Strategy:** Maintain original data integrity while allowing experimentation\n",
    "\n",
    "### Extracting Rigasche Zeitung Corpus\n",
    "\n",
    "For this session, we'll extract the **Rigasche Zeitung (RZei)** corpus, which contains 4,597 text segments from 359 newspaper issues. This corpus is particularly valuable for studying OCR error patterns and testing correction strategies.\n",
    "\n",
    "We'll use a robust Python function that handles:\n",
    "- **Secure download** from the GitHub repository\n",
    "- **Automatic extraction** to a standardized directory structure\n",
    "- **Error reporting** for troubleshooting\n",
    "- **Performance monitoring** to track processing time\n",
    "\n",
    "The function will download the zip file, extract it to an appropriate location, and return information about the extracted files for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b2f442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will extract Rigasche Zeitung historical corpus from https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Rigasche_Zeitung_1918_1919.zip\n",
      "üöÄ Starting Rigasche Zeitung corpus extraction...\n",
      "üîÑ Starting download at 2025-08-01 13:13:26.133700\n",
      "üì• Downloading from: https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Rigasche_Zeitung_1918_1919.zip\n",
      "‚úÖ Download completed at 2025-08-01 13:13:28.131537\n",
      "‚è±Ô∏è  Download duration: 2.00 seconds\n",
      "üìä Downloaded 16,776,843 bytes\n",
      "üìÇ Starting extraction to 'data' at 2025-08-01 13:13:28.132537\n",
      "‚úÖ Extraction completed at 2025-08-01 13:13:51.411734\n",
      "‚è±Ô∏è  Extraction duration: 23.28 seconds\n",
      "üéØ Total process duration: 25.28 seconds\n",
      "üìÅ Files extracted to: data\n",
      "\n",
      "üéâ Rigasche Zeitung corpus successfully extracted!\n",
      "üìö Ready to analyze historical German newspaper content from 1918-1919\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = \"https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Rigasche_Zeitung_1918_1919.zip\"\n",
    "print(\"Will extract Rigasche Zeitung historical corpus from\", url)\n",
    "\n",
    "# Define a function to download and extract the zip file, making it reusable for other corpora\n",
    "# We set default values for optional arguments to make the function flexible\n",
    "def extract_zip(url, output_dir=\"data\", verbose=False):\n",
    "    \"\"\"\n",
    "    Download and extract a zip file from a URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to download the zip file from\n",
    "        output_dir (str): Directory to extract files to (default: \"data\")\n",
    "        verbose (bool): Whether to print detailed progress information\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    # Import required libraries (keeping script self-contained)\n",
    "    import requests\n",
    "    from zipfile import ZipFile\n",
    "    from io import BytesIO\n",
    "    from datetime import datetime\n",
    "\n",
    "    try:\n",
    "        if verbose:\n",
    "            download_start = datetime.now()\n",
    "            print(f\"üîÑ Starting download at {download_start.strftime('%Y-%m-%d %H:%M:%S.%f')}\")\n",
    "            print(f\"üì• Downloading from: {url}\")\n",
    "        \n",
    "        # Download the file\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if verbose:\n",
    "            download_finish = datetime.now()\n",
    "            download_duration = download_finish - download_start\n",
    "            print(f\"‚úÖ Download completed at {download_finish.strftime('%Y-%m-%d %H:%M:%S.%f')}\")\n",
    "            print(f\"‚è±Ô∏è  Download duration: {download_duration.total_seconds():.2f} seconds\")\n",
    "            print(f\"üìä Downloaded {len(response.content):,} bytes\")\n",
    "        \n",
    "        # Check if download was successful\n",
    "        if response.status_code == 200:\n",
    "            if verbose:\n",
    "                extract_start = datetime.now()\n",
    "                print(f\"üìÇ Starting extraction to '{output_dir}' at {extract_start.strftime('%Y-%m-%d %H:%M:%S.%f')}\")\n",
    "            \n",
    "            # Extract the zip file\n",
    "            with ZipFile(BytesIO(response.content)) as zf:\n",
    "                zf.extractall(output_dir)\n",
    "            \n",
    "            if verbose:\n",
    "                extract_end = datetime.now()\n",
    "                extract_duration = extract_end - extract_start\n",
    "                total_duration = extract_end - download_start\n",
    "                print(f\"‚úÖ Extraction completed at {extract_end.strftime('%Y-%m-%d %H:%M:%S.%f')}\")\n",
    "                print(f\"‚è±Ô∏è  Extraction duration: {extract_duration.total_seconds():.2f} seconds\")\n",
    "                print(f\"üéØ Total process duration: {total_duration.total_seconds():.2f} seconds\")\n",
    "                print(f\"üìÅ Files extracted to: {output_dir}\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to download data. HTTP status code: {response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during download/extraction: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Extract the Rigasche Zeitung corpus\n",
    "print(\"üöÄ Starting Rigasche Zeitung corpus extraction...\")\n",
    "success = extract_zip(url, verbose=True)\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéâ Rigasche Zeitung corpus successfully extracted!\")\n",
    "    print(\"üìö Ready to analyze historical German newspaper content from 1918-1919\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Extraction failed. Please check your internet connection and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966c61f5",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "\n",
    "### Verifying Extracted Historical Documents\n",
    "\n",
    "After extracting historical corpora, it's crucial to verify that the extraction process completed successfully and that we have access to the expected files. This verification step is particularly important for historical documents where:\n",
    "\n",
    "- **File integrity** may be affected by long-term digital preservation processes\n",
    "- **Complex directory structures** might reflect archival organization systems\n",
    "- **Encoding issues** could affect file accessibility\n",
    "- **Large file counts** require systematic verification approaches\n",
    "\n",
    "### Verification Process\n",
    "\n",
    "We'll use Python's `pathlib` module to systematically check:\n",
    "1. **Directory existence** and accessibility\n",
    "2. **File count** and structure validation\n",
    "3. **File naming patterns** to ensure they match expected conventions\n",
    "4. **Initial content sampling** to verify readability\n",
    "\n",
    "This verification step helps us catch any issues early in our workflow, before we begin the more computationally expensive LLM processing steps.\n",
    "\n",
    "**Expected Structure for Rigasche Zeitung:**\n",
    "- Base directory: `data/`\n",
    "- Corpus subdirectory: `Rigasche_Zeitung_1918_1919/`\n",
    "- Individual files: `rzei[year]s[volume]n[issue]_[page]_plaintext_s[segment].txt`\n",
    "- Expected count: ~4,597 text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55fbde9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verifying extracted Rigasche Zeitung corpus...\n",
      "============================================================\n",
      "üìÅ Found 2 items in extraction directory: data\n",
      "\n",
      "üìã Contents of extraction directory:\n",
      "----------------------------------------\n",
      "  üìÅ Latvian_Economic_Review/ (contains 419 files)\n",
      "  üìÅ Rigasche_Zeitung_1918_1919/ (contains 4597 files)\n",
      "\n",
      "‚úÖ Extraction directory verified successfully\n",
      "üéØ Found expected corpus directory: Rigasche_Zeitung_1918_1919\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up the path to our extracted data\n",
    "extract_dir = Path(\"data\")  # Relative path to the extraction directory\n",
    "\n",
    "print(\"üîç Verifying extracted Rigasche Zeitung corpus...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if the extraction directory exists\n",
    "if extract_dir.exists():\n",
    "    # List all items in the extraction directory\n",
    "    items = list(extract_dir.glob(\"*\"))\n",
    "    print(f\"üìÅ Found {len(items)} items in extraction directory: {extract_dir}\")\n",
    "    print(\"\\nüìã Contents of extraction directory:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for item in sorted(items):\n",
    "        if item.is_dir():\n",
    "            # Count files in subdirectory to give size indication\n",
    "            subfiles = list(item.rglob(\"*\"))\n",
    "            file_count = len([f for f in subfiles if f.is_file()])\n",
    "            print(f\"  üìÅ {item.name}/ (contains {file_count} files)\")\n",
    "        else:\n",
    "            # Show file size for individual files\n",
    "            file_size = item.stat().st_size\n",
    "            print(f\"  üìÑ {item.name} ({file_size:,} bytes)\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Extraction directory verified successfully\")\n",
    "    \n",
    "    # Check if we have the expected Rigasche Zeitung directory\n",
    "    expected_corpus_dir = extract_dir / \"Rigasche_Zeitung_1918_1919\"\n",
    "    if expected_corpus_dir.exists():\n",
    "        print(f\"üéØ Found expected corpus directory: {expected_corpus_dir.name}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Expected 'Rigasche_Zeitung_1918_1919' directory not found\")\n",
    "        print(\"üìù Available directories:\")\n",
    "        for item in items:\n",
    "            if item.is_dir():\n",
    "                print(f\"    - {item.name}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ùå Extraction directory '{extract_dir}' does not exist!\")\n",
    "    print(\"üîß Please verify that the extraction process completed successfully.\")\n",
    "    print(\"üí° Try running the extraction cell again if needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03253184",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "\n",
    "### Comprehensive Analysis of Historical Corpus Structure\n",
    "\n",
    "Now that we've confirmed the basic extraction, let's perform a detailed analysis of our Rigasche Zeitung corpus. This comprehensive analysis will help us understand:\n",
    "\n",
    "#### **Corpus Composition:**\n",
    "- **Total file count** and distribution by type\n",
    "- **Text file inventory** (our primary working material)\n",
    "- **Directory structure** and organization\n",
    "- **File size distribution** to identify potential outliers\n",
    "\n",
    "#### **Quality Assessment:**\n",
    "- **File naming pattern verification** to ensure consistency\n",
    "- **Sample content inspection** to check encoding and readability\n",
    "- **Size analysis** to identify unusually small/large files that might indicate OCR issues\n",
    "\n",
    "#### **Research Planning:**\n",
    "- **Workload estimation** based on total file count and sizes\n",
    "- **Sampling strategy** for initial testing and development\n",
    "- **Processing priority** based on file characteristics\n",
    "\n",
    "This analysis is particularly important for historical corpora like Rigasche Zeitung because:\n",
    "- **OCR quality varies** across different issues and pages\n",
    "- **Historical printing variations** affect digitization success\n",
    "- **File size anomalies** often indicate OCR problems worth investigating\n",
    "- **Systematic overview** helps plan computational resource allocation\n",
    "\n",
    "The analysis function below provides both summary statistics and detailed file listings to support informed research decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44199d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting comprehensive corpus analysis...\n",
      "‚è±Ô∏è  This may take a moment for large corpora...\n",
      "\n",
      "üîç Analyzing corpus structure...\n",
      "üìä CORPUS ANALYSIS REPORT\n",
      "======================================================================\n",
      "üìÅ Directory: data\n",
      "üî¢ Total items (files + directories): 5,018\n",
      "üìÑ Total files: 5,016\n",
      "üìÅ Total subdirectories: 2\n",
      "üíæ Total corpus size: 33,554,554 bytes (32.0 MB)\n",
      "üìù Text files (.txt): 5,016\n",
      "\n",
      "üìã FILE TYPE DISTRIBUTION:\n",
      "----------------------------------------\n",
      "  .txt            5,016 files (100.0%)\n",
      "\n",
      "üóÇÔ∏è  DIRECTORY STRUCTURE:\n",
      "----------------------------------------\n",
      "  üìÅ Latvian_Economic_Review (419 files)\n",
      "  üìÅ Rigasche_Zeitung_1918_1919 (4,597 files)\n",
      "\n",
      "üìù TEXT FILE STATISTICS:\n",
      "----------------------------------------\n",
      "  Total text files: 5,016\n",
      "  Average file size: 6,690 bytes\n",
      "  Smallest file: 85 bytes\n",
      "  Largest file: 55,914 bytes\n",
      "  Total text content: 33,554,554 bytes (32.0 MB)\n",
      "\n",
      "üìã SAMPLE TEXT FILES (showing 10 of 5016):\n",
      "----------------------------------------------------------------------\n",
      "  üìÑ Latvian_Economic_Review\\lerq1936s01n01_003_plaintext_s01.txt  8,662 bytes\n",
      "  üìÑ Latvian_Economic_Review\\lerq1936s01n01_006_plaintext_s02.txt  5,190 bytes\n",
      "  üìÑ Latvian_Economic_Review\\lerq1936s01n01_008_plaintext_s03.txt  6,066 bytes\n",
      "  üìÑ Latvian_Economic_Review\\lerq1936s01n01_009_plaintext_s04.txt  5,523 bytes\n",
      "  üìÑ Latvian_Economic_Review\\lerq1936s01n01_013_plaintext_s05.txt  3,866 bytes\n",
      "  üìÑ Latvian_Economic_Review\\lerq1936s01n01_014_plaintext_s06.txt  1,144 bytes\n",
      "  üìÑ Latvian_Economic_Review\\lerq1936s01n01_014_plaintext_s07.txt    629 bytes\n",
      "  üìÑ Latvian_Economic_Review\\lerq1936s01n01_015_plaintext_s08.txt  7,520 bytes\n",
      "  üìÑ Latvian_Economic_Review\\lerq1936s01n01_017_plaintext_s09.txt    951 bytes\n",
      "  üìÑ Latvian_Economic_Review\\lerq1936s01n01_018_plaintext_s10.txt  7,625 bytes\n",
      "  ... and 5,006 more text files\n",
      "\n",
      "‚úÖ Corpus analysis completed successfully!\n",
      "üéØ Ready to work with 5,016 historical text files\n",
      "‚ö†Ô∏è  File count (5,016) differs from expected (4,597)\n",
      "   This might be normal depending on the corpus version or processing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def analyze_directory_contents(directory_path, verbose=True):\n",
    "    \"\"\"\n",
    "    Comprehensively analyze the contents of a directory recursively.\n",
    "    Particularly useful for historical corpus analysis.\n",
    "    \n",
    "    Args:\n",
    "        directory_path: Path object or string path to the directory\n",
    "        verbose: If True, print detailed information about file types and structure\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing comprehensive analysis results\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    directory = Path(directory_path)\n",
    "    \n",
    "    if not directory.exists():\n",
    "        print(f\"‚ùå Directory {directory} does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üîç Analyzing corpus structure...\")\n",
    "    \n",
    "    # Get all items recursively\n",
    "    all_items = list(directory.rglob(\"*\"))\n",
    "    \n",
    "    # Separate files from directories\n",
    "    files_only = [f for f in all_items if f.is_file()]\n",
    "    directories_only = [f for f in all_items if f.is_dir()]\n",
    "    \n",
    "    # Analyze file extensions\n",
    "    file_extensions = {}\n",
    "    total_size = 0\n",
    "    for file in files_only:\n",
    "        ext = file.suffix.lower()\n",
    "        if ext == '':\n",
    "            ext = '(no extension)'\n",
    "        file_extensions[ext] = file_extensions.get(ext, 0) + 1\n",
    "        total_size += file.stat().st_size\n",
    "    \n",
    "    # Focus on .txt files for text analysis\n",
    "    txt_files = [f for f in files_only if f.suffix.lower() == '.txt']\n",
    "    \n",
    "    # Calculate text file statistics\n",
    "    txt_sizes = [f.stat().st_size for f in txt_files] if txt_files else []\n",
    "    \n",
    "    # Compile analysis results\n",
    "    results = {\n",
    "        'total_items': len(all_items),\n",
    "        'total_files': len(files_only),\n",
    "        'total_directories': len(directories_only),\n",
    "        'total_size_bytes': total_size,\n",
    "        'txt_files_count': len(txt_files),\n",
    "        'txt_files_paths': txt_files,\n",
    "        'file_extensions': file_extensions,\n",
    "        'txt_file_sizes': txt_sizes\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üìä CORPUS ANALYSIS REPORT\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"üìÅ Directory: {directory}\")\n",
    "        print(f\"üî¢ Total items (files + directories): {results['total_items']:,}\")\n",
    "        print(f\"üìÑ Total files: {results['total_files']:,}\")\n",
    "        print(f\"üìÅ Total subdirectories: {results['total_directories']:,}\")\n",
    "        print(f\"üíæ Total corpus size: {results['total_size_bytes']:,} bytes ({results['total_size_bytes']/1024/1024:.1f} MB)\")\n",
    "        print(f\"üìù Text files (.txt): {results['txt_files_count']:,}\")\n",
    "        print()\n",
    "        \n",
    "        # File extension breakdown\n",
    "        print(\"üìã FILE TYPE DISTRIBUTION:\")\n",
    "        print(\"-\" * 40)\n",
    "        for ext, count in sorted(file_extensions.items(), key=lambda x: x[1], reverse=True):\n",
    "            percentage = (count / len(files_only)) * 100 if files_only else 0\n",
    "            print(f\"  {ext:15} {count:5,} files ({percentage:5.1f}%)\")\n",
    "        print()\n",
    "        \n",
    "        # Directory structure\n",
    "        if directories_only:\n",
    "            print(\"üóÇÔ∏è  DIRECTORY STRUCTURE:\")\n",
    "            print(\"-\" * 40)\n",
    "            for dir_path in sorted(directories_only):\n",
    "                relative_path = dir_path.relative_to(directory_path)\n",
    "                file_count = len([f for f in dir_path.rglob(\"*\") if f.is_file()])\n",
    "                print(f\"  üìÅ {relative_path} ({file_count:,} files)\")\n",
    "            print()\n",
    "        \n",
    "        # Text file analysis\n",
    "        if txt_files:\n",
    "            if txt_sizes:\n",
    "                avg_size = sum(txt_sizes) / len(txt_sizes)\n",
    "                min_size = min(txt_sizes)\n",
    "                max_size = max(txt_sizes)\n",
    "                \n",
    "                print(\"üìù TEXT FILE STATISTICS:\")\n",
    "                print(\"-\" * 40)\n",
    "                print(f\"  Total text files: {len(txt_files):,}\")\n",
    "                print(f\"  Average file size: {avg_size:,.0f} bytes\")\n",
    "                print(f\"  Smallest file: {min_size:,} bytes\")\n",
    "                print(f\"  Largest file: {max_size:,} bytes\")\n",
    "                print(f\"  Total text content: {sum(txt_sizes):,} bytes ({sum(txt_sizes)/1024/1024:.1f} MB)\")\n",
    "                print()\n",
    "            \n",
    "            # Sample file listing\n",
    "            sample_count = min(10, len(txt_files))\n",
    "            print(f\"üìã SAMPLE TEXT FILES (showing {sample_count} of {len(txt_files)}):\")\n",
    "            print(\"-\" * 70)\n",
    "            for txt_file in sorted(txt_files)[:sample_count]:\n",
    "                relative_path = txt_file.relative_to(directory_path)\n",
    "                file_size = txt_file.stat().st_size\n",
    "                print(f\"  üìÑ {str(relative_path):50} {file_size:6,} bytes\")\n",
    "            \n",
    "            if len(txt_files) > sample_count:\n",
    "                print(f\"  ... and {len(txt_files) - sample_count:,} more text files\")\n",
    "            print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Perform comprehensive analysis of the extracted Rigasche Zeitung corpus\n",
    "print(\"üöÄ Starting comprehensive corpus analysis...\")\n",
    "print(\"‚è±Ô∏è  This may take a moment for large corpora...\")\n",
    "print()\n",
    "\n",
    "analysis_results = analyze_directory_contents(extract_dir, verbose=True)\n",
    "\n",
    "if analysis_results:\n",
    "    print(\"‚úÖ Corpus analysis completed successfully!\")\n",
    "    print(f\"üéØ Ready to work with {analysis_results['txt_files_count']:,} historical text files\")\n",
    "    \n",
    "    # Quick validation against expected corpus size\n",
    "    expected_files = 4597  # Expected number of segments for Rigasche Zeitung\n",
    "    actual_files = analysis_results['txt_files_count']\n",
    "    \n",
    "    if actual_files == expected_files:\n",
    "        print(f\"‚úÖ File count matches expected corpus size ({expected_files:,} files)\")\n",
    "    elif actual_files > 0:\n",
    "        print(f\"‚ö†Ô∏è  File count ({actual_files:,}) differs from expected ({expected_files:,})\")\n",
    "        print(\"   This might be normal depending on the corpus version or processing\")\n",
    "    else:\n",
    "        print(f\"‚ùå No text files found! Please check the extraction process.\")\n",
    "else:\n",
    "    print(\"‚ùå Corpus analysis failed. Please check the extraction directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1dcf120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['total_items', 'total_files', 'total_directories', 'total_size_bytes', 'txt_files_count', 'txt_files_paths', 'file_extensions', 'txt_file_sizes'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca45b425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç SELECTING SAMPLE DOCUMENT FOR ANALYSIS\n",
      "==================================================\n",
      "üìÑ Selected file: rzei1919s01n60_002_plaintext_s05.txt\n",
      "üìä File size: 23,852 bytes\n",
      "üìÅ Full path: data\\Rigasche_Zeitung_1918_1919\\rzei1919s01n60_002_plaintext_s05.txt\n",
      "\n",
      "üìñ DOCUMENT CONTENT PREVIEW:\n",
      "----------------------------------------------------------------------\n",
      "üìä Content statistics:\n",
      "   Characters: 22,823\n",
      "   Words (approx): 3,190\n",
      "   Lines: 565\n",
      "\n",
      "üîç Content preview (first 500 characters):\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "title: Ausland.\n",
      "author: \n",
      "uri: http://dom.lndb.lv/data/obj/318311\n",
      "\n",
      "\n",
      "\n",
      "Deutsches Reich.\n",
      "Annahme der Verfassung.\n",
      "Die Nationalversammlung nahm am 31. Juli\n",
      "die Versassung des neuen Deutschen Reichs in\n",
      "dritter Lesung durch namentliche Abstimmung\n",
      "\n",
      "endg√ºltig an. Die Mehrheit des Parlaments,\n",
      "aus Sozialdemokraten, Zentrum uud der\n",
      "Deutschen demokratischen Partei bestehend, gab\n",
      "262 Stimmen daf√ºr ab. Dagegen stimmten die\n",
      "Deutschnationale Volkspartei, die Deutsche-Volks-\n",
      "Partei und die Unabh√§ngige sozialdemokr\n",
      "\n",
      "... (document continues for 22,323 more characters)\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üîç PRELIMINARY OCR QUALITY ASSESSMENT:\n",
      "--------------------------------------------------\n",
      "üìà OCR Quality Indicators:\n",
      "   Digit density: 0.004 (lower is usually better)\n",
      "   Unusual punctuation marks: 2\n",
      "   Multiple spaces: 0\n",
      "   Mixed case words: 1106\n",
      "‚úÖ OCR quality appears relatively good\n",
      "\n",
      "üéØ This document will be excellent for testing LLM-based OCR correction!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select the 6th text file for examination (good size for sample analysis)\n",
    "# We choose the 6th file as it's likely to be representative but not too large for display\n",
    "\n",
    "print(\"üîç SELECTING SAMPLE DOCUMENT FOR ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "text_files_list = analysis_results['txt_files_paths']\n",
    "\n",
    "if len(text_files_list) >= 6:\n",
    "    sixth_text_file = text_files_list[-6]  # 6th file from end (from start would be text_files_list[5])\n",
    "    file_size = sixth_text_file.stat().st_size\n",
    "    \n",
    "    print(f\"üìÑ Selected file: {sixth_text_file.name}\")\n",
    "    print(f\"üìä File size: {file_size:,} bytes\")\n",
    "    print(f\"üìÅ Full path: {sixth_text_file}\")\n",
    "    print()\n",
    "    \n",
    "    # Read and display the file contents\n",
    "    try:\n",
    "        print(\"üìñ DOCUMENT CONTENT PREVIEW:\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        with sixth_text_file.open('r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Display file statistics\n",
    "        word_count = len(content.split())\n",
    "        line_count = len(content.splitlines())\n",
    "        char_count = len(content)\n",
    "        \n",
    "        print(f\"üìä Content statistics:\")\n",
    "        print(f\"   Characters: {char_count:,}\")\n",
    "        print(f\"   Words (approx): {word_count:,}\")\n",
    "        print(f\"   Lines: {line_count:,}\")\n",
    "        print()\n",
    "        \n",
    "        # Show first 500 characters to preview content and potential OCR issues\n",
    "        preview_length = 500\n",
    "        print(f\"üîç Content preview (first {preview_length} characters):\")\n",
    "        print(\"‚îÄ\" * 70)\n",
    "        print(content[:preview_length])\n",
    "        \n",
    "        if len(content) > preview_length:\n",
    "            print(f\"\\n... (document continues for {len(content) - preview_length:,} more characters)\")\n",
    "        \n",
    "        print(\"‚îÄ\" * 70)\n",
    "        \n",
    "        # Quick OCR quality assessment\n",
    "        print(\"\\nüîç PRELIMINARY OCR QUALITY ASSESSMENT:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Check for common OCR error indicators\n",
    "        ocr_indicators = {\n",
    "            'scattered_digits': len([c for c in content if c.isdigit()]) / len(content) if content else 0,\n",
    "            'unusual_punctuation': content.count('~') + content.count('|') + content.count('#'),\n",
    "            'repeated_spaces': content.count('  '),\n",
    "            'mixed_case_words': len([w for w in content.split() if w and any(c.isupper() for c in w) and any(c.islower() for c in w)]),\n",
    "        }\n",
    "        \n",
    "        print(f\"üìà OCR Quality Indicators:\")\n",
    "        print(f\"   Digit density: {ocr_indicators['scattered_digits']:.3f} (lower is usually better)\")\n",
    "        print(f\"   Unusual punctuation marks: {ocr_indicators['unusual_punctuation']}\")\n",
    "        print(f\"   Multiple spaces: {ocr_indicators['repeated_spaces']}\")\n",
    "        print(f\"   Mixed case words: {ocr_indicators['mixed_case_words']}\")\n",
    "        \n",
    "        # Assess overall quality\n",
    "        if ocr_indicators['scattered_digits'] < 0.05 and ocr_indicators['unusual_punctuation'] < 10:\n",
    "            print(\"‚úÖ OCR quality appears relatively good\")\n",
    "        elif ocr_indicators['scattered_digits'] < 0.1 and ocr_indicators['unusual_punctuation'] < 25:\n",
    "            print(\"‚ö†Ô∏è  OCR quality appears moderate - some errors expected\")\n",
    "        else:\n",
    "            print(\"üîß OCR quality appears challenging - significant errors likely\")\n",
    "        \n",
    "        print(f\"\\nüéØ This document will be excellent for testing LLM-based OCR correction!\")\n",
    "        \n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"‚ùå Encoding error reading file: {e}\")\n",
    "        print(\"üîß This indicates potential character encoding issues in the historical text\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading file: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå Insufficient files in corpus. Found {len(text_files_list)} files, need at least 6.\")\n",
    "    if len(text_files_list) > 0:\n",
    "        print(\"üìã Available files:\")\n",
    "        for i, file_path in enumerate(text_files_list[:5]):\n",
    "            print(f\"   {i+1}. {file_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50309fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
