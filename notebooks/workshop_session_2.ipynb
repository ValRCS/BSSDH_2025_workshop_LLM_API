{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61c6cb6a",
   "metadata": {},
   "source": [
    "# Using LLMs in Humanities Research via API\n",
    "\n",
    "## Session 2 14.00-15.30 - Working with LLMs via API\n",
    "\n",
    "Through practical examples, we will explore prompt engineering techniques for tasks such as concept mining and named entity recognition in textual data.\n",
    "\n",
    "## Session Outline\n",
    "\n",
    "- **Prompt Engineering**: Techniques for crafting effective prompts to guide LLMs in generating relevant and accurate responses.\n",
    "- **Concept Mining**: Using LLMs to extract key concepts from text, enabling researchers to identify important themes and ideas.\n",
    "- **Named Entity Recognition (NER)**: Implementing NER to identify and classify entities in text, such as people, organizations, and locations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3328f6",
   "metadata": {},
   "source": [
    "## BSSDH 2025 Workshop Data\n",
    "\n",
    "Before we start exploring the API, let's take a look at the corpus of documents we will be working. \n",
    "Data for workshops in [Baltic Summer School of Digital Humanities 2025](https://www.digitalhumanities.lv/bssdh/2025/about/)\n",
    "\n",
    "**Repository:** https://github.com/LNB-DH/BSSDH_2025_workshop_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## CORPUS OVERVIEW\n",
    "\n",
    "\n",
    "1. SOURCE MATERIAL\n",
    "------------------\n",
    "\n",
    "| Periodical | Details |\n",
    "|------------|---------|\n",
    "| \"Rigasche Zeitung\" (RZei) (1918‚Äì1919) | - **Data file:** `Rigasche_Zeitung_1918_1919.zip`<br>- **Download Rigasche Zeitung:** https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Rigasche_Zeitung_1918_1919.zip<br>- Morning newspaper, intermittently published from 1778 to 1919 in Riga.<br>- Language: German (Fraktur script)<br>- Once the most popular morning paper in the Baltic provinces of the Russian Empire.<br>- Covered general political and economic news in Riga, the Baltics, the Russian Empire, and internationally.<br>- Historical context: World War I, Latvian War of Independence.<br>- Link: https://periodika.lv/#periodicalMeta:234;-1<br>- More info: https://enciklopedija.lv/skirklis/163962 |\n",
    "| \"Latvian Economic Review\" (LERQ) (1936‚Äì1940) | - **Data file:** `Latvian_Economic_Review_1936_1940.zip`<br>- **Download Latvian Economic Review:** https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Latvian_Economic_Review_1936_1940.zip<br>- Full title: \"Latvian Economic Review: A quarterly review of trade, industry and agriculture\".<br>- Language: English (modern)<br>- Published by the Latvian Chamber of Commerce and Industry (established 1934).<br>- Focused on cross-border representation of Latvian economy during the Great Depression, increasing state control, push for autarky, and start of WWII.<br>- Link: https://periodika.lv/#periodicalItem:620 |\n",
    "\n",
    "2. CORPUS INFORMATION\n",
    "----------------------\n",
    "\n",
    "| Metric | RZei | LERQ |\n",
    "|--------|------|------|\n",
    "| Token Count (words) | 5.37 million | 0.5 million |\n",
    "| Issue Count | 359 issues | 18 issues |\n",
    "| Segment (Article <=> File) Count | 4,597 | 419 |\n",
    "| Language | German | English |\n",
    "| Script | Fraktur | Modern |\n",
    "\n",
    "Filename Structure:\n",
    "-------------------\n",
    "Format: [periodical][year][volume#*][issue#]_[page#]_[[plaintext]]_[segment#]\n",
    "\n",
    "Example: `lerq1936s01n02_031_plaintext_s17.txt`\n",
    "         ‚Üí 17th segment from LERQ, Issue 2, 1936, page 31.\n",
    "\n",
    "*Volume value in corpus is one in all cases.\n",
    "\n",
    "3. METHODOLOGY\n",
    "---------------\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| 3.1. Source Access | Digitised issues obtained from the National Library of Latvia (https://periodika.lv/) |\n",
    "| 3.2. Processing & OCR | CCS docWORKS & ABBYY FineReader 9.0<br>- LERQ has better OCR quality than RZei<br>- No further data cleaning/normalization |\n",
    "| 3.3. Metadata Added | Fields: title, author, uri<br>- Author info available in:<br>&nbsp;&nbsp;&nbsp;&nbsp;LERQ: 4 cases (0.95%)<br>&nbsp;&nbsp;&nbsp;&nbsp;RZei: 325 cases (7.05%)<br>- Title availability:<br>&nbsp;&nbsp;&nbsp;&nbsp;LERQ: 95.7%, RZei: 99.15%<br>- URI coverage: 100% for both<br>- URIs point to LNB DOM system |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a419d247",
   "metadata": {},
   "source": [
    "## Extracting documents\n",
    "\n",
    "We could extract documents manually by downloading the appropriate zip file and extracting files by *hand* using file extracting capabilities built into your Operating System(Windows has built in extractor) or using external program such as 7-zip, WinRAR, etc. However, it is more replicable and convenient to use a script that will do this for us. We would supply a url or file name and the script would download the file, extract it to approparite location, and return a list of files that were extracted.\n",
    "\n",
    "### Additional considerations when extracting documents\n",
    "\n",
    "* Where will be extracted files be stored? - Ideally we would have a same relative structure when extracting files locally and on remote server such as Google Colab.\n",
    "* How will we handle file names? - Usually we would like to keep the original file names, but we might want to add some additional information such as source or date of extraction.\n",
    "* How will we handle errors? - We should consider what to do if the file cannot be downloaded or extracted. Should we skip it or raise an error?\n",
    "\n",
    "### Extracting Latvian Economic Review\n",
    "\n",
    "For this session we will extract Latvian Economic Review (LERQ) corpus. We will use a script that will download the file, extract it to appropriate location, and return a list of files that were extracted.\n",
    "\n",
    "We will write a function in Python that will do this for us. The function will take a URL or file name as an argument and will download file from url and then extract it. We will have a default location where the files will be extracted, but we can also specify a different location if needed.\n",
    "\n",
    "```python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2b4ecf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will extract data from https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Latvian_Economic_Review_1936_1940.zip\n",
      "Starting download at 2025-08-04 21:45:44.098537\n",
      "Starting download from https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Latvian_Economic_Review_1936_1940.zip\n",
      "Download finished at 2025-08-04 21:45:44.895403 taking 0:00:00.796866 seconds\n",
      "Starting extraction to data at 2025-08-04 21:45:44.895403\n",
      "Extraction finished at 2025-08-04 21:45:45.059081 taking 0:00:00.163678 seconds\n",
      "Total time taken: 0:00:00.960544 seconds\n"
     ]
    }
   ],
   "source": [
    "url = \"https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Latvian_Economic_Review_1936_1940.zip\"\n",
    "print(\"Will extract data from\", url)\n",
    "# next we define a function that will download and extract the zip file, this way we can reuse it later if needed\n",
    "# we can also set some default values for the arguments, so we do not have to specify\n",
    "# default values always come after the mandatory arguments in Python functions\n",
    "def extract_zip(url, output_dir=\"data\", verbose=False):\n",
    "    # we could have imported these at the top, but we want to keep the script self-contained\n",
    "    import requests  # this should be cached by notebooks, so it **should** not require importing it every time\n",
    "    from zipfile import ZipFile\n",
    "    from io import BytesIO\n",
    "\n",
    "    # In verbose mode let's some extra information about the download and extraction process\n",
    "    # This is useful for debugging and understanding the flow of the script\n",
    "    from datetime import datetime\n",
    "    if verbose:\n",
    "        download_start = datetime.now()\n",
    "        # we print start time including milliseconds\n",
    "        print(f\"Starting download at {download_start.strftime('%Y-%m-%d %H:%M:%S.%f')}\")\n",
    "        print(\"Starting download from\", url)\n",
    "    response = requests.get(url)\n",
    "    if verbose:\n",
    "        download_finish = datetime.now()\n",
    "        print(f\"Download finished at {download_finish.strftime('%Y-%m-%d %H:%M:%S.%f')} taking {download_finish - download_start} seconds\")\n",
    "    if response.status_code == 200: # it is possible a request fails, e.g. if the URL is incorrect\n",
    "        if verbose:\n",
    "            extract_start = datetime.now()\n",
    "            print(f\"Starting extraction to {output_dir} at {extract_start.strftime('%Y-%m-%d %H:%M:%S.%f')}\")\n",
    "        with ZipFile(BytesIO(response.content)) as zf:\n",
    "            zf.extractall(output_dir)\n",
    "        if verbose:\n",
    "            extract_end = datetime.now()\n",
    "            print(f\"Extraction finished at {extract_end.strftime('%Y-%m-%d %H:%M:%S.%f')} taking {extract_end - extract_start} seconds\")\n",
    "            print(f\"Total time taken: {extract_end - download_start} seconds\")\n",
    "    else:\n",
    "        print(\"Failed to download data:\", response.status_code)\n",
    "\n",
    "# now that we have our function defined, we can call it immediately\n",
    "# note we do not supply all arguments, first one is mandatory, the rest are optional\n",
    "# so we skip over output_dir in this case\n",
    "extract_zip(url, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bc187d",
   "metadata": {},
   "source": [
    "### Getting information about extracted files\n",
    "\n",
    "It is a good practice to double check what files were extracted and where they are located. We can do this by listing the files in the directory where we extracted them. We can use Python pathlib to do this. \n",
    "The goal is to double check that what we extacted matches what we expected. We can also check the file names and their structure to make sure they are correct.\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd0d77bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1 files to data\n",
      "Latvian_Economic_Review\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "extract_dir = Path(\"data\") # note this is a a relative path, relative to the current working directory for the notebook\n",
    "# let's check if the directory exists and how many files it contains\n",
    "if extract_dir.exists():\n",
    "    files = list(extract_dir.glob(\"*\"))  # this will list all files in the directory\n",
    "    print(f\"Extracted {len(files)} files to {extract_dir}\")\n",
    "    for file in files:\n",
    "        print(file.name)  # print the name of each file \n",
    "else:\n",
    "    print(f\"Directory {extract_dir} does not exist. Please check the extraction process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7ade9a",
   "metadata": {},
   "source": [
    "### Getting information about subfolders in the extracted directory\n",
    "Looks like we only have a single file but it is actually not a file but a directory. This is because we extracted a zip file that contains files under a single directory. \n",
    "\n",
    "Next we want to check how many total files we have and also how many files we have with *.txt extension. This will help us to understand how many files we can work with and if there are any files that we might want to exclude from our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55d05596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing extracted data directory...\n",
      "üìÅ Directory Analysis: data\n",
      "==================================================\n",
      "Total items found (files + directories): 420\n",
      "Total files: 419\n",
      "Total directories: 1\n",
      "Text files (.txt): 419\n",
      "\n",
      "üìä File Extensions Summary:\n",
      "------------------------------\n",
      "  .txt: 419 files\n",
      "\n",
      "üìÇ Directory Structure:\n",
      "------------------------------\n",
      "  üìÅ Latvian_Economic_Review\n",
      "\n",
      "üìÑ First 10 .txt files (out of 419 total):\n",
      "------------------------------\n",
      "  üìÑ Latvian_Economic_Review\\lerq1936s01n01_003_plaintext_s01.txt (8,662 bytes)\n",
      "  üìÑ Latvian_Economic_Review\\lerq1936s01n01_006_plaintext_s02.txt (5,190 bytes)\n",
      "  üìÑ Latvian_Economic_Review\\lerq1936s01n01_008_plaintext_s03.txt (6,066 bytes)\n",
      "  üìÑ Latvian_Economic_Review\\lerq1936s01n01_009_plaintext_s04.txt (5,523 bytes)\n",
      "  üìÑ Latvian_Economic_Review\\lerq1936s01n01_013_plaintext_s05.txt (3,866 bytes)\n",
      "  üìÑ Latvian_Economic_Review\\lerq1936s01n01_014_plaintext_s06.txt (1,144 bytes)\n",
      "  üìÑ Latvian_Economic_Review\\lerq1936s01n01_014_plaintext_s07.txt (629 bytes)\n",
      "  üìÑ Latvian_Economic_Review\\lerq1936s01n01_015_plaintext_s08.txt (7,520 bytes)\n",
      "  üìÑ Latvian_Economic_Review\\lerq1936s01n01_017_plaintext_s09.txt (951 bytes)\n",
      "  üìÑ Latvian_Economic_Review\\lerq1936s01n01_018_plaintext_s10.txt (7,625 bytes)\n",
      "  ... and 409 more .txt files\n"
     ]
    }
   ],
   "source": [
    "# let's check how many files total we have and how many files with *.txt extension counting all subfolders \n",
    "# this means we will perform a recursive search for all files in the directory\n",
    "\n",
    "def analyze_directory_contents(directory_path, verbose=True):\n",
    "    \"\"\"\n",
    "    Analyze the contents of a directory recursively and provide detailed information.\n",
    "    \n",
    "    Args:\n",
    "        directory_path: Path object or string path to the directory\n",
    "        verbose: If True, print detailed information about file types and structure\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing analysis results\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    directory = Path(directory_path)\n",
    "    \n",
    "    if not directory.exists():\n",
    "        print(f\"Directory {directory} does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    # Get all files recursively\n",
    "    all_files = list(directory.rglob(\"*\"))\n",
    "    \n",
    "    # Separate files from directories\n",
    "    files_only = [f for f in all_files if f.is_file()]\n",
    "    directories_only = [f for f in all_files if f.is_dir()]\n",
    "    \n",
    "    # Count files by extension\n",
    "    file_extensions = {}\n",
    "    for file in files_only:\n",
    "        ext = file.suffix.lower()\n",
    "        if ext == '':\n",
    "            ext = '(no extension)'\n",
    "        file_extensions[ext] = file_extensions.get(ext, 0) + 1\n",
    "    \n",
    "    # Count .txt files specifically\n",
    "    txt_files = [f for f in files_only if f.suffix.lower() == '.txt']\n",
    "    \n",
    "    # Analysis results\n",
    "    results = {\n",
    "        'total_items': len(all_files),\n",
    "        'total_files': len(files_only),\n",
    "        'total_directories': len(directories_only),\n",
    "        'txt_files': len(txt_files),\n",
    "        'file_extensions': file_extensions,\n",
    "        'txt_file_paths': txt_files\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üìÅ Directory Analysis: {directory}\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total items found (files + directories): {results['total_items']}\")\n",
    "        print(f\"Total files: {results['total_files']}\")\n",
    "        print(f\"Total directories: {results['total_directories']}\")\n",
    "        print(f\"Text files (.txt): {results['txt_files']}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"üìä File Extensions Summary:\")\n",
    "        print(\"-\" * 30)\n",
    "        for ext, count in sorted(file_extensions.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {ext}: {count} files\")\n",
    "        print()\n",
    "        \n",
    "        if len(directories_only) > 0:\n",
    "            print(\"üìÇ Directory Structure:\")\n",
    "            print(\"-\" * 30)\n",
    "            for directory in sorted(directories_only):\n",
    "                # Show relative path from the base directory\n",
    "                relative_path = directory.relative_to(directory_path)\n",
    "                print(f\"  üìÅ {relative_path}\")\n",
    "            print()\n",
    "        \n",
    "        if len(txt_files) > 0 and len(txt_files) <= 10:\n",
    "            print(\"üìÑ Sample .txt files:\")\n",
    "            print(\"-\" * 30)\n",
    "            for txt_file in sorted(txt_files)[:10]:\n",
    "                relative_path = txt_file.relative_to(directory_path)\n",
    "                file_size = txt_file.stat().st_size\n",
    "                print(f\"  üìÑ {relative_path} ({file_size:,} bytes)\")\n",
    "        elif len(txt_files) > 10:\n",
    "            print(f\"üìÑ First 10 .txt files (out of {len(txt_files)} total):\")\n",
    "            print(\"-\" * 30)\n",
    "            for txt_file in sorted(txt_files)[:10]:\n",
    "                relative_path = txt_file.relative_to(directory_path)\n",
    "                file_size = txt_file.stat().st_size\n",
    "                print(f\"  üìÑ {relative_path} ({file_size:,} bytes)\")\n",
    "            print(f\"  ... and {len(txt_files) - 10} more .txt files\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Now let's analyze our extracted directory\n",
    "print(\"Analyzing extracted data directory...\")\n",
    "analysis_results = analyze_directory_contents(extract_dir, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e283717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6th text file: lerq1936s01n01_014_plaintext_s06.txt (1,144 bytes)\n"
     ]
    }
   ],
   "source": [
    "# let's get 6th text file from analysis_results dictionary text_files_paths key\n",
    "# why 6th? because it seems a bit smaller\n",
    "text_files_list = analysis_results['txt_file_paths']\n",
    "if len(text_files_list) >= 6:\n",
    "    sixth_text_file = text_files_list[5]  # 6th file, index starts from 0\n",
    "    print(f\"6th text file: {sixth_text_file.name} ({sixth_text_file.stat().st_size:,} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e18da885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of lerq1936s01n01_014_plaintext_s06.txt:\n",
      "\n",
      "title: Gypsum\n",
      "author: \n",
      "uri: http://dom.lndb.lv/data/obj/159411\n",
      "\n",
      "\n",
      "\n",
      "There are numerous extensive layers of gypseous\n",
      "stone in Latvia, but only a few of them are being\n",
      "exploited, viz., the quarries at Kalnciems, Sloka\n",
      "(about 33 km. from Riga), Salaspils (about 20 km.\n",
      "from Riga) and Naves sala (about 25 km. from Riga).\n",
      "The gypseous stone is exported both in raw condition\n",
      "(gypsum), principally for the manufacture of\n",
      "cement, and in the form of Plaster of Paris. The export\n",
      "of gypsum totalled 69,000 tons\n",
      "\n",
      "... (truncated output)\n"
     ]
    }
   ],
   "source": [
    "# let's print out its contents\n",
    "with sixth_text_file.open('r', encoding='utf-8') as f:\n",
    "    content = f.read() # read whole file content into memory\n",
    "# file is closed here automatically due to the with statement\n",
    "print(f\"Contents of {sixth_text_file.name}:\\n\")\n",
    "print(content[:500])  # print first 500 characters to avoid too much output\n",
    "print(\"\\n... (truncated output)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449270d3",
   "metadata": {},
   "source": [
    "## Setting up our LLM API functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee5d53aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for OPENROUTER_API_KEY in environment variables...\n",
      "OpenRouter API key loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# let's try loading open_router_api_key first from system environment variables,\n",
    "#  then from .env file\n",
    "# and finally we will prompt user to enter it manually if not found\n",
    "\n",
    "import os # we already imported this, but let's do it again for clarity - it is cached so no harm done\n",
    "print(\"Checking for OPENROUTER_API_KEY in environment variables...\")\n",
    "# lets try loading the environment variable from system environment variables\n",
    "open_router_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "# if not found, try loading from .env file\n",
    "if not open_router_api_key:\n",
    "    print(\"OPENROUTER_API_KEY not found in environment variables. Trying to load from .env file...\")\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        # Load environment variables from .env file\n",
    "        load_dotenv()\n",
    "        print(\"Environment variables loaded from .env file.\")\n",
    "    except ImportError:\n",
    "        print(\"dotenv module is not installed. You can install it using 'pip install python-dotenv'.\")\n",
    "\n",
    "    open_router_api_key = os.getenv('OPENROUTER_API_KEY') # we try loading again after loading .env file\n",
    "\n",
    "# if still not found, prompt user to enter it manually\n",
    "if not open_router_api_key:\n",
    "    open_router_api_key = input(\"Please enter your OpenRouter API key: \")\n",
    "    # save it to .env file for future use\n",
    "    # note Google Colab will destroy .env file after session ends, so you will need to enter it again next time\n",
    "    # this can be useful if you re-run the notebook and want to avoid entering the key again\n",
    "    print(\"Saving Open Router API key to .env file...\")\n",
    "    with open('.env', 'a') as f:\n",
    "        f.write(f'OPENROUTER_API_KEY={open_router_api_key}\\n')\n",
    "    print(\"Open Router API key saved to .env file.\")\n",
    "\n",
    "# we now should have the OpenRouter API key available\n",
    "if open_router_api_key:\n",
    "    print(\"OpenRouter API key loaded successfully.\")\n",
    "else:\n",
    "    print(\"OpenRouter API key not found. Please make sure you have it set in your environment variables or .env file.\")\n",
    "    print(\"You can also enter it manually when prompted during API calls.\")\n",
    "\n",
    "# key point we do not print it publicly it is stored as a variable under the name open_router_api_key - of course you can change the name to something more descriptive\n",
    "# but do not print it to the console or logs, as it is sensitive information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38b5ca4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from OpenRouter API:\n",
      "Here are the named entities found in the text:\n",
      "\n",
      "*   Kalnciems\n",
      "*   Sloka\n",
      "*   Riga\n",
      "*   Salaspils\n",
      "*   Naves sala\n",
      "*   Plaster of Paris\n",
      "*   Norway\n",
      "*   Sweden\n",
      "*   Denmark\n",
      "*   Finland\n",
      "*   England\n"
     ]
    }
   ],
   "source": [
    "# let's define a generic function for OpenRouter API requests\n",
    "# it should have tshould define a new function get_openrouter_response it should have following parameters system_prompt, user_prompt,\n",
    "#  model defaulting to ChatGPT 3.5 and finally api_key which defaults to open_router_api_key .\n",
    "#  The function get_openrouter_response should function just like analyze_latvian_text_with_openrouter except with parameters.\n",
    "import requests  # we need to import requests to make API calls\n",
    "\n",
    "def get_openrouter_response(system_prompt, user_prompt, \n",
    "                            model=\"google/gemini-2.5-flash-lite\", \n",
    "                            api_key=open_router_api_key,\n",
    "                            max_tokens=1000,\n",
    "                            temperature=0.5,):\n",
    "    \"\"\"\n",
    "    Generic function to make requests to OpenRouter API with specified parameters.\n",
    "    \n",
    "    :param system_prompt: The system prompt to guide the model's behavior.\n",
    "    :param user_prompt: The user query or text to analyze.\n",
    "    :param model: The model to use for the request (default is GPT-3.5).\n",
    "    :param api_key: The OpenRouter API key (default is loaded from environment).\n",
    "    :return: The response from the OpenRouter API.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up the API endpoint and headers\n",
    "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"HTTP-Referer\": \"https://www.digitalhumanities.lv/bssdh/2025/\",  # Your project URL\n",
    "        \"X-Title\": \"BSSDH 2025 LLM Workshop - Generic OpenRouter Request\"\n",
    "    }\n",
    "    \n",
    "    # Create the request payload\n",
    "    request_data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "    \n",
    "    # Make the API request\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=request_data, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        result = response.json()\n",
    "        \n",
    "        if 'choices' in result and len(result['choices']) > 0:\n",
    "            return result['choices'][0]['message']['content']\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ùå Error: No response returned from the API\")\n",
    "            return None\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Request Error: {e}\")\n",
    "        return None\n",
    "    \n",
    "# let's test it on simple Meaning of Life question\n",
    "\n",
    "system_prompt = \"You are a digital humanities researcher specializing in named entity recognition and text analysis. You will analyze the text and provide all named entities as a list.\"\n",
    "user_prompt = content\n",
    "\n",
    "response = get_openrouter_response(system_prompt, user_prompt) # note we did not pass model or api_key, so it will use defaults of \"openai/gpt-3.5-turbo\" and open_router_api_key\n",
    "\n",
    "if response:\n",
    "    print(\"Response from OpenRouter API:\")\n",
    "    print(response)\n",
    "else:\n",
    "    print(\"Failed to get a response from OpenRouter API.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489a99af",
   "metadata": {},
   "source": [
    "## Adjusting system prompts \n",
    "\n",
    "We got our named entities but let's also have categories for them.\n",
    "We can do that by adjusting our system prompt to include categories for named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "223485c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted system prompt: You are a digital humanities researcher specializing in named entity recognition and text analysis. You will analyze the text and provide all named entities as a list. Please categorize the named entities into PERSON, ORGANIZATION, LOCATION, and MISC.\n"
     ]
    }
   ],
   "source": [
    "# let's adjust our system prompt by adding extra instruction to add categories for named entities.\n",
    "extra_instruction = \"Please categorize the named entities into PERSON, ORGANIZATION, LOCATION, and MISC.\"\n",
    "system_prompt = f\"{system_prompt} {extra_instruction}\"\n",
    "print(f\"Adjusted system prompt: {system_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfdbae4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with adjusted system prompt:\n",
      "Here are the named entities from the text, categorized as requested:\n",
      "\n",
      "**PERSON:**\n",
      "* None\n",
      "\n",
      "**ORGANIZATION:**\n",
      "* None\n",
      "\n",
      "**LOCATION:**\n",
      "* Latvia\n",
      "* Kalnciems\n",
      "* Sloka\n",
      "* Riga\n",
      "* Naves sala\n",
      "* Norway\n",
      "* Sweden\n",
      "* Denmark\n",
      "* Finland\n",
      "* England\n",
      "\n",
      "**MISC:**\n",
      "* Gypsum\n",
      "* Plaster of Paris\n"
     ]
    }
   ],
   "source": [
    "# let's see our response with adjusted system prompt\n",
    "response = get_openrouter_response(system_prompt, user_prompt)\n",
    "if response:\n",
    "    print(\"Response with adjusted system prompt:\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9a1940",
   "metadata": {},
   "source": [
    "## Getting a summary of the text\n",
    "\n",
    "One of the basic tasks we can do with LLMs is to get a summary of the text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24312220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the text:\n",
      "Here's a summary of the provided text about gypsum:\n",
      "\n",
      "*   **Gypsum Deposits:** Latvia possesses extensive layers of gypseous stone, with active exploitation occurring in quarries at Kalnciems, Sloka, Salaspils, and Naves sala.\n",
      "*   **Exports:** The gypseous stone is exported both raw (as gypsum) and processed into Plaster of Paris.\n",
      "*   **Raw Gypsum Exports (1934):**\n",
      "    *   Totaled 69,000 tons.\n",
      "    *   Generated Ls 395,000.\n",
      "    *   Destinations included Norway, Sweden, Denmark, Finland, and England.\n",
      "    *   The export volume was even higher in the preceding year.\n",
      "*   **Gypsum Quality:** Latvian gypsum averages 93% purity, with some layers reaching up to 99%.\n",
      "*   **Reasons for Popularity:** Its firm structure is highly valued abroad as it prevents machinery clogging during milling.\n",
      "*   **Plaster of Paris Exports (1934):**\n",
      "    *   Amounted to approximately 6,500 tons.\n",
      "    *   Generated Ls 85,000.\n",
      "    *   A 100% increase in exports was anticipated for the following year.\n",
      "    *   The primary destination was England, where it's used for manufacturing gypsum plates and other items.\n"
     ]
    }
   ],
   "source": [
    "summary_system_prompt = \"\"\"You are a digital humanities researcher specializing in text summarization. \n",
    "You will summarize the text and provide a concise summary in structured bullet point format.\"\"\"\n",
    "response = get_openrouter_response(summary_system_prompt, content)\n",
    "if response:\n",
    "    print(\"Summary of the text:\")\n",
    "    print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
