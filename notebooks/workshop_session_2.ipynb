{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "61c6cb6a",
      "metadata": {
        "id": "61c6cb6a"
      },
      "source": [
        "# Using LLMs in Humanities Research via API\n",
        "\n",
        "## Session 2 14.00-15.30 - Working with LLMs via API\n",
        "\n",
        "Through practical examples, we will explore prompt engineering techniques for tasks such as concept mining and named entity recognition in textual data.\n",
        "\n",
        "## Session Outline\n",
        "\n",
        "- **Prompt Engineering**: Techniques for crafting effective prompts to guide LLMs in generating relevant and accurate responses.\n",
        "- **Concept Mining**: Using LLMs to extract key concepts from text, enabling researchers to identify important themes and ideas.\n",
        "- **Named Entity Recognition (NER)**: Implementing NER to identify and classify entities in text, such as people, organizations, and locations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb3328f6",
      "metadata": {
        "id": "fb3328f6"
      },
      "source": [
        "## BSSDH 2025 Workshop Data\n",
        "\n",
        "Before we start exploring the API, let's take a look at the corpus of documents we will be working.\n",
        "Data for workshops in [Baltic Summer School of Digital Humanities 2025](https://www.digitalhumanities.lv/bssdh/2025/about/)\n",
        "\n",
        "**Repository:** https://github.com/LNB-DH/BSSDH_2025_workshop_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## CORPUS OVERVIEW\n",
        "\n",
        "\n",
        "1. SOURCE MATERIAL\n",
        "------------------\n",
        "\n",
        "| Periodical | Details |\n",
        "|------------|---------|\n",
        "| \"Rigasche Zeitung\" (RZei) (1918–1919) | - **Data file:** `Rigasche_Zeitung_1918_1919.zip`<br>- **Download Rigasche Zeitung:** https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Rigasche_Zeitung_1918_1919.zip<br>- Morning newspaper, intermittently published from 1778 to 1919 in Riga.<br>- Language: German (Fraktur script)<br>- Once the most popular morning paper in the Baltic provinces of the Russian Empire.<br>- Covered general political and economic news in Riga, the Baltics, the Russian Empire, and internationally.<br>- Historical context: World War I, Latvian War of Independence.<br>- Link: https://periodika.lv/#periodicalMeta:234;-1<br>- More info: https://enciklopedija.lv/skirklis/163962 |\n",
        "| \"Latvian Economic Review\" (LERQ) (1936–1940) | - **Data file:** `Latvian_Economic_Review_1936_1940.zip`<br>- **Download Latvian Economic Review:** https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Latvian_Economic_Review_1936_1940.zip<br>- Full title: \"Latvian Economic Review: A quarterly review of trade, industry and agriculture\".<br>- Language: English (modern)<br>- Published by the Latvian Chamber of Commerce and Industry (established 1934).<br>- Focused on cross-border representation of Latvian economy during the Great Depression, increasing state control, push for autarky, and start of WWII.<br>- Link: https://periodika.lv/#periodicalItem:620 |\n",
        "\n",
        "2. CORPUS INFORMATION\n",
        "----------------------\n",
        "\n",
        "| Metric | RZei | LERQ |\n",
        "|--------|------|------|\n",
        "| Token Count (words) | 5.37 million | 0.5 million |\n",
        "| Issue Count | 359 issues | 18 issues |\n",
        "| Segment (Article <=> File) Count | 4,597 | 419 |\n",
        "| Language | German | English |\n",
        "| Script | Fraktur | Modern |\n",
        "\n",
        "Filename Structure:\n",
        "-------------------\n",
        "Format: [periodical][year][volume#*][issue#]_[page#]_[[plaintext]]_[segment#]\n",
        "\n",
        "Example: `lerq1936s01n02_031_plaintext_s17.txt`\n",
        "         → 17th segment from LERQ, Issue 2, 1936, page 31.\n",
        "\n",
        "*Volume value in corpus is one in all cases.\n",
        "\n",
        "3. METHODOLOGY\n",
        "---------------\n",
        "\n",
        "| Step | Description |\n",
        "|------|-------------|\n",
        "| 3.1. Source Access | Digitised issues obtained from the National Library of Latvia (https://periodika.lv/) |\n",
        "| 3.2. Processing & OCR | CCS docWORKS & ABBYY FineReader 9.0<br>- LERQ has better OCR quality than RZei<br>- No further data cleaning/normalization |\n",
        "| 3.3. Metadata Added | Fields: title, author, uri<br>- Author info available in:<br>&nbsp;&nbsp;&nbsp;&nbsp;LERQ: 4 cases (0.95%)<br>&nbsp;&nbsp;&nbsp;&nbsp;RZei: 325 cases (7.05%)<br>- Title availability:<br>&nbsp;&nbsp;&nbsp;&nbsp;LERQ: 95.7%, RZei: 99.15%<br>- URI coverage: 100% for both<br>- URIs point to LNB DOM system |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a419d247",
      "metadata": {
        "id": "a419d247"
      },
      "source": [
        "## Extracting documents\n",
        "\n",
        "We could extract documents manually by downloading the appropriate zip file and extracting files by *hand* using file extracting capabilities built into your Operating System(Windows has built in extractor) or using external program such as 7-zip, WinRAR, etc. However, it is more replicable and convenient to use a script that will do this for us. We would supply a url or file name and the script would download the file, extract it to approparite location, and return a list of files that were extracted.\n",
        "\n",
        "### Additional considerations when extracting documents\n",
        "\n",
        "* Where will be extracted files be stored? - Ideally we would have a same relative structure when extracting files locally and on remote server such as Google Colab.\n",
        "* How will we handle file names? - Usually we would like to keep the original file names, but we might want to add some additional information such as source or date of extraction.\n",
        "* How will we handle errors? - We should consider what to do if the file cannot be downloaded or extracted. Should we skip it or raise an error?\n",
        "\n",
        "### Extracting Latvian Economic Review\n",
        "\n",
        "For this session we will extract Latvian Economic Review (LERQ) corpus. We will use a script that will download the file, extract it to appropriate location, and return a list of files that were extracted.\n",
        "\n",
        "We will write a function in Python that will do this for us. The function will take a URL or file name as an argument and will download file from url and then extract it. We will have a default location where the files will be extracted, but we can also specify a different location if needed.\n",
        "\n",
        "```python\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f2b4ecf6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2b4ecf6",
        "outputId": "01b6ddb5-0701-4570-8d37-a9e42ae8febe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Will extract data from https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Latvian_Economic_Review_1936_1940.zip\n",
            "Starting download at 2025-08-06 17:30:34.620765\n",
            "Starting download from https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Latvian_Economic_Review_1936_1940.zip\n",
            "Download finished at 2025-08-06 17:30:35.247864 taking 0:00:00.627099 seconds\n",
            "Starting extraction to data at 2025-08-06 17:30:35.248003\n",
            "Extraction finished at 2025-08-06 17:30:35.472019 taking 0:00:00.224016 seconds\n",
            "Total time taken: 0:00:00.851254 seconds\n"
          ]
        }
      ],
      "source": [
        "url = \"https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Latvian_Economic_Review_1936_1940.zip\"\n",
        "print(\"Will extract data from\", url)\n",
        "# next we define a function that will download and extract the zip file, this way we can reuse it later if needed\n",
        "# we can also set some default values for the arguments, so we do not have to specify\n",
        "# default values always come after the mandatory arguments in Python functions\n",
        "def extract_zip(url, output_dir=\"data\", verbose=False):\n",
        "    # we could have imported these at the top, but we want to keep the script self-contained\n",
        "    import requests  # this should be cached by notebooks, so it **should** not require importing it every time\n",
        "    from zipfile import ZipFile\n",
        "    from io import BytesIO\n",
        "\n",
        "    # In verbose mode let's some extra information about the download and extraction process\n",
        "    # This is useful for debugging and understanding the flow of the script\n",
        "    from datetime import datetime\n",
        "    if verbose:\n",
        "        download_start = datetime.now()\n",
        "        # we print start time including milliseconds\n",
        "        print(f\"Starting download at {download_start.strftime('%Y-%m-%d %H:%M:%S.%f')}\")\n",
        "        print(\"Starting download from\", url)\n",
        "    response = requests.get(url)\n",
        "    if verbose:\n",
        "        download_finish = datetime.now()\n",
        "        print(f\"Download finished at {download_finish.strftime('%Y-%m-%d %H:%M:%S.%f')} taking {download_finish - download_start} seconds\")\n",
        "    if response.status_code == 200: # it is possible a request fails, e.g. if the URL is incorrect\n",
        "        if verbose:\n",
        "            extract_start = datetime.now()\n",
        "            print(f\"Starting extraction to {output_dir} at {extract_start.strftime('%Y-%m-%d %H:%M:%S.%f')}\")\n",
        "        with ZipFile(BytesIO(response.content)) as zf:\n",
        "            zf.extractall(output_dir)\n",
        "        if verbose:\n",
        "            extract_end = datetime.now()\n",
        "            print(f\"Extraction finished at {extract_end.strftime('%Y-%m-%d %H:%M:%S.%f')} taking {extract_end - extract_start} seconds\")\n",
        "            print(f\"Total time taken: {extract_end - download_start} seconds\")\n",
        "    else:\n",
        "        print(\"Failed to download data:\", response.status_code)\n",
        "\n",
        "# now that we have our function defined, we can call it immediately\n",
        "# note we do not supply all arguments, first one is mandatory, the rest are optional\n",
        "# so we skip over output_dir in this case\n",
        "extract_zip(url, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82bc187d",
      "metadata": {
        "id": "82bc187d"
      },
      "source": [
        "### Getting information about extracted files\n",
        "\n",
        "It is a good practice to double check what files were extracted and where they are located. We can do this by listing the files in the directory where we extracted them. We can use Python pathlib to do this.\n",
        "The goal is to double check that what we extacted matches what we expected. We can also check the file names and their structure to make sure they are correct.\n",
        "\n",
        "```python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dd0d77bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd0d77bf",
        "outputId": "4d9fa301-0e1e-487a-c746-79c72a5d23bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 1 files to data\n",
            "Latvian_Economic_Review\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "extract_dir = Path(\"data\") # note this is a a relative path, relative to the current working directory for the notebook\n",
        "# let's check if the directory exists and how many files it contains\n",
        "if extract_dir.exists():\n",
        "    files = list(extract_dir.glob(\"*\"))  # this will list all files in the directory\n",
        "    print(f\"Extracted {len(files)} files to {extract_dir}\")\n",
        "    for file in files:\n",
        "        print(file.name)  # print the name of each file\n",
        "else:\n",
        "    print(f\"Directory {extract_dir} does not exist. Please check the extraction process.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae7ade9a",
      "metadata": {
        "id": "ae7ade9a"
      },
      "source": [
        "### Getting information about subfolders in the extracted directory\n",
        "Looks like we only have a single file but it is actually not a file but a directory. This is because we extracted a zip file that contains files under a single directory.\n",
        "\n",
        "Next we want to check how many total files we have and also how many files we have with *.txt extension. This will help us to understand how many files we can work with and if there are any files that we might want to exclude from our analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "55d05596",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55d05596",
        "outputId": "492fbede-4666-4a44-b3a6-16e9676cccd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing extracted data directory...\n",
            "📁 Directory Analysis: data\n",
            "==================================================\n",
            "Total items found (files + directories): 420\n",
            "Total files: 419\n",
            "Total directories: 1\n",
            "Text files (.txt): 419\n",
            "\n",
            "📊 File Extensions Summary:\n",
            "------------------------------\n",
            "  .txt: 419 files\n",
            "\n",
            "📂 Directory Structure:\n",
            "------------------------------\n",
            "  📁 Latvian_Economic_Review\n",
            "\n",
            "📄 First 10 .txt files (out of 419 total):\n",
            "------------------------------\n",
            "  📄 Latvian_Economic_Review/lerq1936s01n01_003_plaintext_s01.txt (8,662 bytes)\n",
            "  📄 Latvian_Economic_Review/lerq1936s01n01_006_plaintext_s02.txt (5,190 bytes)\n",
            "  📄 Latvian_Economic_Review/lerq1936s01n01_008_plaintext_s03.txt (6,066 bytes)\n",
            "  📄 Latvian_Economic_Review/lerq1936s01n01_009_plaintext_s04.txt (5,523 bytes)\n",
            "  📄 Latvian_Economic_Review/lerq1936s01n01_013_plaintext_s05.txt (3,866 bytes)\n",
            "  📄 Latvian_Economic_Review/lerq1936s01n01_014_plaintext_s06.txt (1,144 bytes)\n",
            "  📄 Latvian_Economic_Review/lerq1936s01n01_014_plaintext_s07.txt (629 bytes)\n",
            "  📄 Latvian_Economic_Review/lerq1936s01n01_015_plaintext_s08.txt (7,520 bytes)\n",
            "  📄 Latvian_Economic_Review/lerq1936s01n01_017_plaintext_s09.txt (951 bytes)\n",
            "  📄 Latvian_Economic_Review/lerq1936s01n01_018_plaintext_s10.txt (7,625 bytes)\n",
            "  ... and 409 more .txt files\n"
          ]
        }
      ],
      "source": [
        "# let's check how many files total we have and how many files with *.txt extension counting all subfolders\n",
        "# this means we will perform a recursive search for all files in the directory\n",
        "\n",
        "def analyze_directory_contents(directory_path, verbose=True):\n",
        "    \"\"\"\n",
        "    Analyze the contents of a directory recursively and provide detailed information.\n",
        "\n",
        "    Args:\n",
        "        directory_path: Path object or string path to the directory\n",
        "        verbose: If True, print detailed information about file types and structure\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing analysis results\n",
        "    \"\"\"\n",
        "    from pathlib import Path\n",
        "\n",
        "    directory = Path(directory_path)\n",
        "\n",
        "    if not directory.exists():\n",
        "        print(f\"Directory {directory} does not exist.\")\n",
        "        return None\n",
        "\n",
        "    # Get all files recursively\n",
        "    all_files = list(directory.rglob(\"*\"))\n",
        "\n",
        "    # Separate files from directories\n",
        "    files_only = [f for f in all_files if f.is_file()]\n",
        "    directories_only = [f for f in all_files if f.is_dir()]\n",
        "\n",
        "    # Count files by extension\n",
        "    file_extensions = {}\n",
        "    for file in files_only:\n",
        "        ext = file.suffix.lower()\n",
        "        if ext == '':\n",
        "            ext = '(no extension)'\n",
        "        file_extensions[ext] = file_extensions.get(ext, 0) + 1\n",
        "\n",
        "    # Count .txt files specifically\n",
        "    txt_files = [f for f in files_only if f.suffix.lower() == '.txt']\n",
        "\n",
        "    # Analysis results\n",
        "    results = {\n",
        "        'total_items': len(all_files),\n",
        "        'total_files': len(files_only),\n",
        "        'total_directories': len(directories_only),\n",
        "        'txt_files': len(txt_files),\n",
        "        'file_extensions': file_extensions,\n",
        "        'txt_file_paths': txt_files\n",
        "    }\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"📁 Directory Analysis: {directory}\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Total items found (files + directories): {results['total_items']}\")\n",
        "        print(f\"Total files: {results['total_files']}\")\n",
        "        print(f\"Total directories: {results['total_directories']}\")\n",
        "        print(f\"Text files (.txt): {results['txt_files']}\")\n",
        "        print()\n",
        "\n",
        "        print(\"📊 File Extensions Summary:\")\n",
        "        print(\"-\" * 30)\n",
        "        for ext, count in sorted(file_extensions.items(), key=lambda x: x[1], reverse=True):\n",
        "            print(f\"  {ext}: {count} files\")\n",
        "        print()\n",
        "\n",
        "        if len(directories_only) > 0:\n",
        "            print(\"📂 Directory Structure:\")\n",
        "            print(\"-\" * 30)\n",
        "            for directory in sorted(directories_only):\n",
        "                # Show relative path from the base directory\n",
        "                relative_path = directory.relative_to(directory_path)\n",
        "                print(f\"  📁 {relative_path}\")\n",
        "            print()\n",
        "\n",
        "        if len(txt_files) > 0 and len(txt_files) <= 10:\n",
        "            print(\"📄 Sample .txt files:\")\n",
        "            print(\"-\" * 30)\n",
        "            for txt_file in sorted(txt_files)[:10]:\n",
        "                relative_path = txt_file.relative_to(directory_path)\n",
        "                file_size = txt_file.stat().st_size\n",
        "                print(f\"  📄 {relative_path} ({file_size:,} bytes)\")\n",
        "        elif len(txt_files) > 10:\n",
        "            print(f\"📄 First 10 .txt files (out of {len(txt_files)} total):\")\n",
        "            print(\"-\" * 30)\n",
        "            for txt_file in sorted(txt_files)[:10]:\n",
        "                relative_path = txt_file.relative_to(directory_path)\n",
        "                file_size = txt_file.stat().st_size\n",
        "                print(f\"  📄 {relative_path} ({file_size:,} bytes)\")\n",
        "            print(f\"  ... and {len(txt_files) - 10} more .txt files\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Now let's analyze our extracted directory\n",
        "print(\"Analyzing extracted data directory...\")\n",
        "analysis_results = analyze_directory_contents(extract_dir, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9e283717",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e283717",
        "outputId": "6a9bbeb6-a8b2-414a-9907-f3ab6d216f3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6th text file: lerq1936s01n01_014_plaintext_s06.txt (1,144 bytes)\n"
          ]
        }
      ],
      "source": [
        "# let's get 6th text file from analysis_results dictionary text_files_paths key\n",
        "# why 6th? because it seems a bit smaller\n",
        "text_files_list = sorted(analysis_results['txt_file_paths'])\n",
        "if len(text_files_list) >= 6:\n",
        "    sixth_text_file = text_files_list[5]  # 6th file, index starts from 0\n",
        "    print(f\"6th text file: {sixth_text_file.name} ({sixth_text_file.stat().st_size:,} bytes)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e18da885",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e18da885",
        "outputId": "6c10eeef-59aa-4a9d-f3c6-1b163e5424b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of lerq1936s01n01_014_plaintext_s06.txt:\n",
            "\n",
            "title: Gypsum\n",
            "author: \n",
            "uri: http://dom.lndb.lv/data/obj/159411\n",
            "\n",
            "\n",
            "\n",
            "There are numerous extensive layers of gypseous\n",
            "stone in Latvia, but only a few of them are being\n",
            "exploited, viz., the quarries at Kalnciems, Sloka\n",
            "(about 33 km. from Riga), Salaspils (about 20 km.\n",
            "from Riga) and Naves sala (about 25 km. from Riga).\n",
            "The gypseous stone is exported both in raw condition\n",
            "(gypsum), principally for the manufacture of\n",
            "cement, and in the form of Plaster of Paris. The export\n",
            "of gypsum totalled 69,000 tons\n",
            "\n",
            "... (truncated output)\n"
          ]
        }
      ],
      "source": [
        "# let's print out its contents\n",
        "with sixth_text_file.open('r', encoding='utf-8') as f:\n",
        "    content = f.read() # read whole file content into memory\n",
        "# file is closed here automatically due to the with statement\n",
        "print(f\"Contents of {sixth_text_file.name}:\\n\")\n",
        "print(content[:500])  # print first 500 characters to avoid too much output\n",
        "print(\"\\n... (truncated output)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "449270d3",
      "metadata": {
        "id": "449270d3"
      },
      "source": [
        "## Setting up our LLM API functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ee5d53aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee5d53aa",
        "outputId": "f05765e1-e8d7-47d1-a98f-9d24714a0a70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your OpenRouter API key: ··········\n",
            "Saving Open Router API key to .env file...\n",
            "Open Router API key saved to .env file.\n",
            "OpenRouter API key loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "#  we will prompt user to enter it manually if not found\n",
        "import getpass\n",
        "\n",
        "import os # we already imported this, but let's do it again for clarity - it is cached so no harm done\n",
        "\n",
        "\n",
        "# if still not found, prompt user to enter it manually\n",
        "if 'open_router_api_key' not in locals() or not open_router_api_key:\n",
        "    open_router_api_key = getpass.getpass(\"Please enter your OpenRouter API key: \")\n",
        "    # save it to .env file for future use\n",
        "    # note Google Colab will destroy .env file after session ends, so you will need to enter it again next time\n",
        "    # this can be useful if you re-run the notebook and want to avoid entering the key again\n",
        "    print(\"Saving Open Router API key to .env file...\")\n",
        "    with open('.env', 'a') as f:\n",
        "        f.write(f'OPENROUTER_API_KEY={open_router_api_key}\\n')\n",
        "    print(\"Open Router API key saved to .env file.\")\n",
        "\n",
        "# we now should have the OpenRouter API key available\n",
        "if open_router_api_key:\n",
        "    print(\"OpenRouter API key loaded successfully.\")\n",
        "else:\n",
        "    print(\"OpenRouter API key not found. Please make sure you have it set in your environment variables or .env file.\")\n",
        "    print(\"You can also enter it manually when prompted during API calls.\")\n",
        "\n",
        "# key point we do not print it publicly it is stored as a variable under the name open_router_api_key - of course you can change the name to something more descriptive\n",
        "# but do not print it to the console or logs, as it is sensitive information"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71574a1b",
      "metadata": {
        "id": "71574a1b"
      },
      "source": [
        "### About the model - Google: Gemini 2.5 Flash Lite\n",
        "\n",
        "Unlike previous session where we used OpenAI API, this session will focus on using [Google Gemini 2.5 Flash Lite model](https://openrouter.ai/google/gemini-2.5-flash-lite) via OpenRouter API. Again we could have used many different models, but we will use this one as it is lightweight and fast and **INEXPENSIVE**, which is ideal for our use case.\n",
        "\n",
        "**Model ID:** `google/gemini-2.5-flash-lite`  \n",
        "**Created:** July 22, 2025  \n",
        "**Context Window:** 1,048,576 tokens  \n",
        "**Pricing:**  \n",
        "- **Input tokens:** $0.10 per million  \n",
        "- **Output tokens:** $0.40 per million  \n",
        "\n",
        "**Tags:**  \n",
        "- Legal (#4)  \n",
        "- Marketing/SEO (#4)  \n",
        "- Translation (#9)  \n",
        "\n",
        "**Description:**  \n",
        "Gemini 2.5 Flash Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for **ultra-low latency** and **cost efficiency**. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models.\n",
        "\n",
        "By default, \"thinking\" (i.e., multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the **Reasoning API** parameter to selectively trade off cost for intelligence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "38b5ca4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38b5ca4d",
        "outputId": "183da7d6-1faf-4817-d4ea-e2d66e0fb49a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from OpenRouter API:\n",
            "* Kalnciems\n",
            "* Sloka\n",
            "* Riga\n",
            "* Salaspils\n",
            "* Naves sala\n",
            "* Norway\n",
            "* Sweden\n",
            "* Denmark\n",
            "* Finland\n",
            "* England\n"
          ]
        }
      ],
      "source": [
        "# let's define a generic function for OpenRouter API requests\n",
        "# it should have tshould define a new function get_openrouter_response it should have following parameters system_prompt, user_prompt,\n",
        "#  model defaulting to ChatGPT 3.5 and finally api_key which defaults to open_router_api_key .\n",
        "#  The function get_openrouter_response should function just like analyze_latvian_text_with_openrouter except with parameters.\n",
        "import requests  # we need to import requests to make API calls\n",
        "\n",
        "def get_openrouter_response(system_prompt, user_prompt,\n",
        "                            model=\"google/gemini-2.5-flash-lite\",\n",
        "                            api_key=open_router_api_key,\n",
        "                            max_tokens=4096,\n",
        "                            temperature=0.5,):\n",
        "    \"\"\"\n",
        "    Generic function to make requests to OpenRouter API with specified parameters.\n",
        "\n",
        "    :param system_prompt: The system prompt to guide the model's behavior.\n",
        "    :param user_prompt: The user query or text to analyze.\n",
        "    :param model: The model to use for the request (default is GPT-3.5).\n",
        "    :param api_key: The OpenRouter API key (default is loaded from environment).\n",
        "    :return: The response from the OpenRouter API.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set up the API endpoint and headers\n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": \"https://www.digitalhumanities.lv/bssdh/2025/\",  # Your project URL\n",
        "        \"X-Title\": \"BSSDH 2025 LLM Workshop - Generic OpenRouter Request\"\n",
        "    }\n",
        "\n",
        "    # Create the request payload\n",
        "    request_data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": 0.9\n",
        "    }\n",
        "\n",
        "    # Make the API request\n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, json=request_data, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        result = response.json()\n",
        "\n",
        "        if 'choices' in result and len(result['choices']) > 0:\n",
        "            return result['choices'][0]['message']['content']\n",
        "\n",
        "        else:\n",
        "            print(\"❌ Error: No response returned from the API\")\n",
        "            return None\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"❌ Request Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# let's test it on simple Meaning of Life question\n",
        "\n",
        "system_prompt = \"You are a digital humanities researcher specializing in named entity recognition and text analysis. You will analyze the text and provide all named entities as a list.\"\n",
        "user_prompt = content\n",
        "\n",
        "response = get_openrouter_response(system_prompt, user_prompt) # note we did not pass model or api_key, so it will use defaults of \"openai/gpt-3.5-turbo\" and open_router_api_key\n",
        "\n",
        "if response:\n",
        "    print(\"Response from OpenRouter API:\")\n",
        "    print(response)\n",
        "else:\n",
        "    print(\"Failed to get a response from OpenRouter API.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "489a99af",
      "metadata": {
        "id": "489a99af"
      },
      "source": [
        "## Adjusting system prompts\n",
        "\n",
        "We got our named entities but let's also have categories for them.\n",
        "We can do that by adjusting our system prompt to include categories for named entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "223485c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "223485c6",
        "outputId": "40bd4f1b-da2e-41fc-e758-2be7abfdcae2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusted system prompt: You are a digital humanities researcher specializing in named entity recognition and text analysis. You will analyze the text and provide all named entities as a list. Please categorize the named entities into PERSON, ORGANIZATION, LOCATION, and MISC.\n"
          ]
        }
      ],
      "source": [
        "# let's adjust our system prompt by adding extra instruction to add categories for named entities.\n",
        "extra_instruction = \"Please categorize the named entities into PERSON, ORGANIZATION, LOCATION, and MISC.\"\n",
        "system_prompt = f\"{system_prompt} {extra_instruction}\"\n",
        "print(f\"Adjusted system prompt: {system_prompt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cfdbae4b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfdbae4b",
        "outputId": "843ddc29-347e-4cc6-cb91-40050087337e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response with adjusted system prompt:\n",
            "Here are the named entities from the text, categorized as requested:\n",
            "\n",
            "**PERSON:**\n",
            "* None\n",
            "\n",
            "**ORGANIZATION:**\n",
            "* None\n",
            "\n",
            "**LOCATION:**\n",
            "* Latvia\n",
            "* Kalnciems\n",
            "* Sloka\n",
            "* Riga\n",
            "* Naves sala\n",
            "* Norway\n",
            "* Sweden\n",
            "* Denmark\n",
            "* Finland\n",
            "* England\n",
            "\n",
            "**MISC:**\n",
            "* Gypsum\n",
            "* Plaster of Paris\n"
          ]
        }
      ],
      "source": [
        "# let's see our response with adjusted system prompt\n",
        "response = get_openrouter_response(system_prompt, user_prompt)\n",
        "if response:\n",
        "    print(\"Response with adjusted system prompt:\")\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6369b81d",
      "metadata": {
        "id": "6369b81d"
      },
      "source": [
        "## Getting a summary of the text\n",
        "\n",
        "One of the basic tasks we can do with LLMs is to get a summary of the text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "24312220",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24312220",
        "outputId": "999766ed-c4d1-4716-ddeb-838067dc77cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of the text:\n",
            "Here's a summary of the provided text about gypsum:\n",
            "\n",
            "*   **Gypsum Deposits:** Latvia has extensive layers of gypseous stone, with active exploitation occurring in quarries at Kalnciems, Sloka, Salaspils, and Naves sala.\n",
            "*   **Exports:**\n",
            "    *   Raw gypsum is exported primarily for cement manufacturing.\n",
            "    *   In 1934, 69,000 tons of gypsum were exported, earning Ls 395,000, with exports increasing in the following year.\n",
            "    *   Gypsum is shipped to Norway, Sweden, Denmark, Finland, and England.\n",
            "    *   Plaster of Paris exports totaled approximately 6,500 tons in 1934, generating Ls 85,000, with a projected 100% increase expected.\n",
            "    *   The majority of Plaster of Paris is sent to England for use in manufacturing gypsum plates and other products.\n",
            "*   **Quality and Reputation:** Latvian gypsum has an average purity of 93%, with some layers reaching up to 99% purity. It is highly valued internationally due to its firm structure, which prevents machinery clogging during milling.\n"
          ]
        }
      ],
      "source": [
        "summary_system_prompt = \"\"\"You are a digital humanities researcher specializing in text summarization.\n",
        "You will summarize the text and provide a concise summary in structured bullet point format.\"\"\"\n",
        "response = get_openrouter_response(summary_system_prompt, content)\n",
        "if response:\n",
        "    print(\"Summary of the text:\")\n",
        "    print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c925855f",
      "metadata": {
        "id": "c925855f"
      },
      "source": [
        "## Concept mining\n",
        "\n",
        "For those are unfamiliar with the term, concept mining is a process of extracting key concepts from a text.\n",
        "\n",
        "Concept mining involves automatically identifying and extracting abstract ideas or key concepts from large amounts of unstructured text data. It goes beyond surface-level keyword extraction by attempting to detect the underlying themes, topics, or conceptual entities present in the text -- including those that might not be explicitly named but are implied or paraphrased.\n",
        "\n",
        "Example would be \"the rise of the internet\" instead of just \"internet\". Instead of \"potatoes\" we could extract \"agricultural products\" or \"food production\". Something like \"food anxiety\" instead of just \"food\".\n",
        "\n",
        "There are some older non LLM techniques such as LDA (Latent Dirichlet Allocation) - that can be used for concept mining, which the instructors have used in previous work. See - [Baklane, Anda., Saulespurens, Valdis “The Application of Latent Dirichlet Allocation for the Analysis of Latvian Historical Newspapers: Oskars Kalpaks’ Case Study.” Nauka, Tehnologìï, Ìnnovacìï, State Scientific Institution - Ukrainian Institute of Scientific and Technical Expertise and Info, 2022.](https://www.academia.edu/112360712/The_application_of_latent_Dirichlet_allocation_for_the_analysis_of_latvian_historical_newspapers_Oskars_Kalpaks_case_study)\n",
        "\n",
        "However LLMs can provide more nuanced and context-aware results compared to LDA, especially in understanding the subtleties of language and the relationships between concepts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "562cef48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "562cef48",
        "outputId": "1628feaf-fea7-4d0b-dc75-6896928e3508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Concepts extracted from the text:\n",
            "Here's a concept mining analysis of the provided text about Gypsum:\n",
            "\n",
            "**Theme: Gypsum Resources and Extraction**\n",
            "\n",
            "*   **Concept:** Gypseous stone layers\n",
            "    *   **Description:** Extensive deposits of gypsum found in Latvia.\n",
            "*   **Concept:** Exploited quarries\n",
            "    *   **Description:** Specific locations where gypsum is extracted.\n",
            "    *   **Examples:** Kalnciems, Sloka, Salaspils, Naves sala.\n",
            "*   **Concept:** Location of quarries\n",
            "    *   **Description:** Proximity of quarries to Riga (Sloka ~33 km, Salaspils ~20 km, Naves sala ~25 km).\n",
            "\n",
            "**Theme: Gypsum Products and Applications**\n",
            "\n",
            "*   **Concept:** Raw gypsum\n",
            "    *   **Description:** Unprocessed gypsum extracted from quarries.\n",
            "    *   **Primary Use:** Manufacture of cement.\n",
            "*   **Concept:** Plaster of Paris\n",
            "    *   **Description:** Processed form of gypsum.\n",
            "    *   **Primary Use:** Making gypsum plates and other articles.\n",
            "\n",
            "**Theme: Gypsum Trade and Economics**\n",
            "\n",
            "*   **Concept:** Export of gypsum (raw)\n",
            "    *   **Year:** 1934\n",
            "    *   **Quantity:** 69,000 tons\n",
            "    *   **Value:** Ls 395,000\n",
            "    *   **Trend:** Larger in the past year (prior to 1934).\n",
            "*   **Concept:** Export destinations (raw gypsum)\n",
            "    *   **Countries:** Norway, Sweden, Denmark, Finland, England.\n",
            "*   **Concept:** Export of Plaster of Paris\n",
            "    *   **Year:** 1934\n",
            "    *   **Quantity:** ~6,500 tons\n",
            "    *   **Value:** Ls 85,000\n",
            "    *   **Projected Trend:** 100% increase expected in the current year (after 1934).\n",
            "*   **Concept:** Primary export destination (Plaster of Paris)\n",
            "    *   **Country:** England.\n",
            "\n",
            "**Theme: Gypsum Quality and Characteristics**\n",
            "\n",
            "*   **Concept:** Average purity\n",
            "    *   **Percentage:** 93%\n",
            "*   **Concept:** High purity layers\n",
            "    *   **Percentage:** Up to 99%\n",
            "*   **Concept:** Foreign market perception\n",
            "    *   **Attribute:** Liked abroad.\n",
            "    *   **Reason:** Firm structure.\n",
            "    *   **Benefit of Firm Structure:** Facilitates milling process without clogging machinery.\n"
          ]
        }
      ],
      "source": [
        "# let's make a system prompt for concept mining\n",
        "system_prompt = \"\"\"You are a digital humanities researcher specializing in concept mining.\n",
        "You will analyze the text and extract key concepts, themes, and patterns as a structured list.\n",
        "Please provide the concepts in a clear and concise manner, categorizing them into relevant themes.\"\"\"\n",
        "response = get_openrouter_response(system_prompt, content)\n",
        "if response:\n",
        "    print(\"Concepts extracted from the text:\")\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f6b6f67",
      "metadata": {
        "id": "0f6b6f67"
      },
      "source": [
        "## 🧠 Concept Categories for 1930s Latvian Economic Reports\n",
        "\n",
        "Above LLM output gave us structured but still flexible list of concepts. Usually we want to focus on a specific set of concepts that are relevant to our research. Below is a list of categories that we can use to organize the concepts extracted from the 1930s Latvian Economic Reports.\n",
        "\n",
        "### 🏦 Finance & Banking\n",
        "- Financial stability  \n",
        "- Monetary policy  \n",
        "- Currency regulation  \n",
        "- Exchange rates  \n",
        "- Foreign investment  \n",
        "- Credit availability  \n",
        "- Central banking  \n",
        "- Gold reserves  \n",
        "\n",
        "### 📈 Trade & Commerce\n",
        "- Trade liberalization  \n",
        "- Export promotion  \n",
        "- Import substitution  \n",
        "- Customs tariffs  \n",
        "- Balance of trade  \n",
        "- Foreign trade agreements  \n",
        "- Trade protectionism  \n",
        "- Trade surplus/deficit  \n",
        "- Transit trade  \n",
        "- Port activity (e.g., Riga, Liepāja)  \n",
        "\n",
        "### 🚜 Agriculture & Rural Economy\n",
        "- Agricultural modernization  \n",
        "- Land reform  \n",
        "- Crop yields  \n",
        "- Agricultural exports  \n",
        "- Collective farming (if present)  \n",
        "- Peasant cooperatives  \n",
        "- Grain storage and reserves  \n",
        "- Rural credit  \n",
        "- Mechanization of agriculture  \n",
        "- Dairy and livestock production  \n",
        "\n",
        "### 🏭 Industry & Infrastructure\n",
        "- Industrialization  \n",
        "- Manufacturing output  \n",
        "- Raw material imports  \n",
        "- Industrial policy  \n",
        "- State-owned enterprises  \n",
        "- Energy production (electricity, fuel)  \n",
        "- Infrastructure investment  \n",
        "- Transportation development  \n",
        "- Railway modernization  \n",
        "- Telecommunications expansion  \n",
        "\n",
        "### 👷 Labor & Employment\n",
        "- Unemployment  \n",
        "- Labor migration  \n",
        "- Wages and cost of living  \n",
        "- Labor policy  \n",
        "- Social insurance  \n",
        "- Vocational training  \n",
        "- Workforce productivity  \n",
        "- State employment programs  \n",
        "\n",
        "### 💰 Prices & Markets\n",
        "- Price stability  \n",
        "- Inflation control  \n",
        "- Consumer goods availability  \n",
        "- Market regulation  \n",
        "- Food pricing  \n",
        "- Speculation control  \n",
        "\n",
        "### 🏛️ Economic Policy & Governance\n",
        "- Five-year plans (if applicable)  \n",
        "- Corporatism  \n",
        "- Authoritarian economic planning  \n",
        "- State intervention  \n",
        "- Public-private partnerships  \n",
        "- Fiscal policy  \n",
        "- Budget deficit  \n",
        "- Taxation policy  \n",
        "- Economic nationalism  \n",
        "\n",
        "### 🌍 International Relations & Geopolitics\n",
        "- Trade with Germany, USSR, UK, Sweden, etc.  \n",
        "- Regional trade blocs (e.g., Baltic Entente)  \n",
        "- Neutrality in foreign policy  \n",
        "- Geopolitical pressures on trade  \n",
        "- Sanctions or economic treaties  \n",
        "\n",
        "### 🧑‍🏫 Education & Human Capital\n",
        "- Economic education  \n",
        "- Technical schools  \n",
        "- Business training  \n",
        "- Scientific research for industry  \n",
        "- Demographic skills gap  \n",
        "\n",
        "### 🧱 Development & Urbanization\n",
        "- Urban planning  \n",
        "- Rural-urban migration  \n",
        "- Public works  \n",
        "- Housing policy  \n",
        "- Regional economic disparity  \n",
        "- Municipal finance  \n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 High-Level & Cross-Cutting Concepts\n",
        "- Economic self-sufficiency  \n",
        "- National economic strategy  \n",
        "- Resilience to global crisis  \n",
        "- Preparation for war economy  \n",
        "- Currency sovereignty  \n",
        "- Export dependency  \n",
        "- Shadow economy (if mentioned)  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a4c806e",
      "metadata": {
        "id": "0a4c806e"
      },
      "source": [
        "## Focusing on specific concepts - agriculture\n",
        "\n",
        "For this workshop let's focus on the agriculture category. We will extract concepts related to agriculture from the 1930s Latvian Economic Reports and analyze them in more detail.\n",
        "\n",
        "### Making a larger system prompt\n",
        "\n",
        "Our system prompt will be a bit more complex as we want to mention the specific category we are interested in. Also we want to provide example of format that we want the LLM to return so our analysis will be more structured and easier to work with.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7abe9ea5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7abe9ea5",
        "outputId": "1f67c3f4-e27c-4443-907b-776f96579d62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agriculture-related concepts extracted from the text:\n",
            "[\n",
            "  {\n",
            "    \"phrase\": \"The gypseous stone is exported both in raw condition (gypsum), principally for the manufacture of cement, and in the form of Plaster of Paris.\",\n",
            "    \"explanation\": \"This describes the export of raw gypsum and processed gypsum (Plaster of Paris) for industrial use, indicating a component of Latvia's export economy.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"The export of gypsum totalled 69,000 tons in 1934 and rendered a sum of Ls 395,000.\",\n",
            "    \"explanation\": \"This quantifies the volume and value of gypsum exports in a specific year, highlighting its significance in trade.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"Latvian gypsum is very much liked abroad because of its firm structure which facilitates the milling process without clogging the machinery.\",\n",
            "    \"explanation\": \"This points to the quality of Latvian gypsum and its favorable characteristics for processing, which contributes to its export marketability.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"The export of Plaster of Paris amounted to about 6,500 tons in 1934, which rendered Ls 85,000.\",\n",
            "    \"explanation\": \"This provides data on the export of a processed gypsum product, indicating another facet of Latvia's trade in mineral resources.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "agriculture_system_prompt = \"\"\"You are an expert assistant trained to analyze historical economic texts from 1930s Latvia.\n",
        "Your task is to read the input document or paragraph and extract any **concepts** that relate specifically to the domain of **Agriculture & Rural Economy**.\n",
        "Focus only on concepts connected to the following subcategories:\n",
        "- Agricultural modernization\n",
        "- Land reform\n",
        "- Crop yields\n",
        "- Agricultural exports\n",
        "- Collective farming (if present)\n",
        "- Peasant cooperatives\n",
        "- Grain storage and reserves\n",
        "- Rural credit\n",
        "- Mechanization of agriculture\n",
        "- Dairy and livestock production\n",
        "- Other agricultural practices\n",
        "\n",
        "For each concept you extract, return:\n",
        "- The **exact phrase** or **paraphrased expression** found in the text\n",
        "- A **brief explanation** (1–2 sentences) summarizing the concept’s meaning or context\n",
        "- The most relevant **subcategory** from the list above that it matches\n",
        "\n",
        "Only return relevant agricultural or rural economy concepts. If no relevant concept is found, return an empty list.\n",
        "\n",
        "Use the following output format (in JSON):\n",
        "\n",
        "[\n",
        "  {\n",
        "    \"phrase\": \"introduction of American tractors\",\n",
        "    \"explanation\": \"Refers to adoption of mechanized equipment to improve farming productivity.\",\n",
        "    \"subcategory\": \"Mechanization of agriculture\"\n",
        "  },\n",
        "  {\n",
        "    \"phrase\": \"state-supported grain warehouses\",\n",
        "    \"explanation\": \"Refers to government involvement in securing grain storage for future needs or trade.\",\n",
        "    \"subcategory\": \"Grain storage and reserves\"\n",
        "  }\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "response = get_openrouter_response(agriculture_system_prompt, content)\n",
        "if response:\n",
        "    print(\"Agriculture-related concepts extracted from the text:\")\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f9fc6f",
      "metadata": {
        "id": "f6f9fc6f"
      },
      "source": [
        "## Fine-tuning the system prompt and adjusting model\n",
        "\n",
        "The LLM output is well structured and concepts are extracted with one MAJOR issue - semantically gypsum and plaster of Paris are not agricultural products!\n",
        "\n",
        " We can fix this by adjusting our system prompt to be more specific about the concepts we want to extract.\n",
        "\n",
        " Also we can adjust the model or its parameters to be more focused on the specific task we are trying to achieve. For example, we can increase the maximum number of tokens to allow for more detailed responses or adjust the temperature to make the model more conservative in its responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "45748e85",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45748e85",
        "outputId": "fb6c40c7-6767-4db8-a5bf-f6d8eb655e83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agriculture-related concepts extracted from the text using Gemini 2.5 Pro:\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "### let's try a different model first - let's try the best (and most expensive) current Gemini model as of mid 2025 - 2.5 Pro\n",
        "\n",
        "response = get_openrouter_response(agriculture_system_prompt, content, model=\"google/gemini-2.5-pro\")\n",
        "if response:\n",
        "    print(\"Agriculture-related concepts extracted from the text using Gemini 2.5 Pro:\")\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd88c0da",
      "metadata": {
        "id": "fd88c0da"
      },
      "source": [
        "### Trying other models - finding middle ground between speed and accuracy\n",
        "\n",
        "Pro model provided accurate answer namely that there are no agricultural products in the text. However, it is not very fast and it is not very cheap to use. The above query cost us about 0.01 USD - that is 1 cent which might not seem like much, but if we want to analyze a large corpus of documents, the costs can add up quickly.\n",
        "\n",
        "So let's see if we can find a model that is faster and cheaper but still provides accurate results. We will try different models and see how they perform on our task.\n",
        "\n",
        "Let's try the `golden mean` as of today [Google Gemini 2.5 Flash](https://openrouter.ai/google/gemini-2.5-flash)\n",
        "\n",
        "To do so we simply provide the model name in the `get_openrouter_response` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c793322d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c793322d",
        "outputId": "5b118a0c-8bd8-4cd0-8a27-27634d2f063b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agriculture-related concepts extracted from the text using Gemini 2.5 Flash:\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "# let's try with google/gemini-2.5-flash model\n",
        "response = get_openrouter_response(agriculture_system_prompt, content, model=\"google/gemini-2.5-flash\")\n",
        "if response:\n",
        "    print(\"Agriculture-related concepts extracted from the text using Gemini 2.5 Flash:\")\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bc6539a",
      "metadata": {
        "id": "2bc6539a"
      },
      "source": [
        "## Running model across multiple documents - our corpus\n",
        "\n",
        "The response looks good (and 10x cheaper that Gemini 2.5 Pro due mostly to limiting reasoning outputs which we do not need for concept mining task), however the question remains, what will happen when we run the model across multiple documents? Will have have many false positives or will the model be able to filter out the noise and provide us with relevant concepts?\n",
        "\n",
        "Now that we have gotten our results from a single document we can run the model across multiple documents in our corpus. We will iterate over the files in the directory where we extracted the LERQ corpus and apply the same system prompt to each file.\n",
        "\n",
        "Usually I like to run my prompts across a smaller sample than full corpus. So let's pick 30 documents at random from the corpus and see how the model performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f395f621",
      "metadata": {
        "id": "f395f621"
      },
      "outputs": [],
      "source": [
        "# let's pick random 30 files from the corpus and run the model across them\n",
        "def run_model_across_corpus(directory_path, system_prompt,\n",
        "                            model=\"google/gemini-2.5-flash\",\n",
        "                            sample_size=30,\n",
        "                            seed=2025,\n",
        "                            verbose=True,\n",
        "                            delay=0.1):\n",
        "    \"\"\"\n",
        "    Run the model across a sample of files in the specified directory.\n",
        "\n",
        "    :param directory_path: Path to the directory containing text files.\n",
        "    :param system_prompt: The system prompt to guide the model's behavior.\n",
        "    :param model: The model to use for the request (default is Gemini 2.5 Flash).\n",
        "    :param sample_size: Number of files to sample from the directory.\n",
        "    :return: List of responses from the model for each file.\n",
        "    \"\"\"\n",
        "    from pathlib import Path\n",
        "    from datetime import datetime\n",
        "    import time\n",
        "    from tqdm import tqdm  # for progress bar\n",
        "    import random  # for random sampling\n",
        "\n",
        "    directory = Path(directory_path)\n",
        "    txt_files = list(directory.glob(\"**/*.txt\"))  # Get all .txt files recursively\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Found {len(txt_files)} text files in the directory {directory}.\")\n",
        "\n",
        "    if len(txt_files) < sample_size:\n",
        "        print(f\"Not enough files in the directory. Found {len(txt_files)} files, but requested {sample_size}.\")\n",
        "        return []\n",
        "\n",
        "    if seed is not None:\n",
        "        random.seed(seed) # this ensures reproducibility of the random sample\n",
        "        # most random operations in computer science are not truly random, but rather pseudo-random\n",
        "\n",
        "    sampled_files = random.sample(txt_files, sample_size)  # Randomly sample files\n",
        "    responses = {} # we will use a dictionary to store responses with file names as keys\n",
        "\n",
        "    for file in tqdm(sampled_files):\n",
        "        with file.open('r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "        if verbose:\n",
        "            now = datetime.now()\n",
        "            # let's print the time in a human-readable format\n",
        "            print(f\"{now.strftime('%Y-%m-%d %H:%M:%S')}  Processing file: {file.name} ({file.stat().st_size:,} bytes)\")\n",
        "\n",
        "        response = get_openrouter_response(system_prompt, content, model=model)\n",
        "        if response:\n",
        "            responses[file.name] = response\n",
        "        else:\n",
        "            print(f\"Failed to get a response for {file.name}\")\n",
        "\n",
        "        time.sleep(delay)  # Delay to avoid hitting API rate limits if necessary\n",
        "\n",
        "    return responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "411f04ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "411f04ad",
        "outputId": "72d3076c-5568-4793-93ed-9e8c91593cd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:48<00:00,  1.63s/it]\n"
          ]
        }
      ],
      "source": [
        "# what was our extract_dir again?\n",
        "# we actually want the subdirectory with Latvian_Economic_Review folder\n",
        "extract_dir = Path(\"data/Latvian_Economic_Review\")  # Adjusted to point to the correct subdirectory\n",
        "responses = run_model_across_corpus(extract_dir, agriculture_system_prompt,\n",
        "                                    model=\"google/gemini-2.5-flash\", # actually the default but let's be explicit\n",
        "                                    sample_size=30, # again default is 30, but we can change it\n",
        "                                    seed=2025, # specific seed for reproducibility\n",
        "                                    verbose=False # set to True to see progress and processing information\n",
        "                                    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c44c83a",
      "metadata": {
        "id": "4c44c83a"
      },
      "source": [
        "## Analyzing responses\n",
        "\n",
        "Now that we got 30 responses let's see if they are relevant and if the model was able to filter out the noise and provide us with relevant concepts.\n",
        "\n",
        "Main thing is to remember the format of our responses. Each response is a string in JSON format that contains the concepts extracted from the text. We have stored all responses in a dictionary with file name as key and response as value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "353b5ef4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "353b5ef4",
        "outputId": "5124e4b3-8199-422e-f19a-fa01058d1cd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 30 responses from the model across the sampled files.\n"
          ]
        }
      ],
      "source": [
        "# how many responses do we have?\n",
        "print(f\"Got {len(responses)} responses from the model across the sampled files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "7b343782",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b343782",
        "outputId": "51dd4ad8-6789-4a57-f2bd-688a7982377b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: lerq1938s01n03_023_plaintext_s10.txt\n",
            "Response: []\n",
            "\n",
            "File: lerq1938s01n01_039_plaintext_s30.txt\n",
            "Response: []\n",
            "\n",
            "File: lerq1936s01n04_032_plaintext_s13.txt\n",
            "Response: []\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# let's print the first 3 responses from our responses dictionary\n",
        "for i, (file_name, response) in enumerate(responses.items()):\n",
        "    if i < 3:  # Print only the first 3 responses\n",
        "        print(f\"File: {file_name}\")\n",
        "        print(f\"Response: {response}\\n\")\n",
        "    else:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "dec7722e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dec7722e",
        "outputId": "cbc82a3e-e027-48b6-edf1-f133d9793464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: lerq1938s01n03_023_plaintext_s10.txt has no valid response.\n",
            "File: lerq1938s01n01_039_plaintext_s30.txt has no valid response.\n",
            "File: lerq1936s01n04_032_plaintext_s13.txt has no valid response.\n",
            "File: lerq1939s01n03_005_plaintext_s02.txt has no valid response.\n",
            "File: lerq1938s01n04_014_plaintext_s10.txt has a valid response.\n",
            "File: lerq1939s01n03_042_plaintext_s20.txt has no valid response.\n",
            "File: lerq1939s01n04_020_plaintext_s08.txt has a valid response.\n",
            "File: lerq1939s01n01_046_plaintext_s20.txt has no valid response.\n",
            "File: lerq1936s01n02_018_plaintext_s08.txt has no valid response.\n",
            "File: lerq1937s01n07_020_plaintext_s08.txt has no valid response.\n",
            "File: lerq1940s01n01_007_plaintext_s03.txt has a valid response.\n",
            "File: lerq1937s01n08_017_plaintext_s05.txt has a valid response.\n",
            "File: lerq1936s01n02_033_plaintext_s21.txt has a valid response.\n",
            "File: lerq1939s01n01_035_plaintext_s13.txt has a valid response.\n",
            "File: lerq1937s01n06_040_plaintext_s27.txt has no valid response.\n",
            "File: lerq1937s01n05_009_plaintext_s05.txt has no valid response.\n",
            "File: lerq1936s01n03_036_plaintext_s21.txt has no valid response.\n",
            "File: lerq1936s01n01_009_plaintext_s04.txt has no valid response.\n",
            "File: lerq1936s01n03_018_plaintext_s09.txt has a valid response.\n",
            "File: lerq1939s01n01_039_plaintext_s16.txt has a valid response.\n",
            "File: lerq1938s01n04_023_plaintext_s28.txt has no valid response.\n",
            "File: lerq1937s01n07_008_plaintext_s04.txt has a valid response.\n",
            "File: lerq1937s01n06_014_plaintext_s13.txt has a valid response.\n",
            "File: lerq1939s01n04_003_plaintext_s01.txt has a valid response.\n",
            "File: lerq1940s01n01_026_plaintext_s10.txt has a valid response.\n",
            "File: lerq1936s01n02_033_plaintext_s20.txt has no valid response.\n",
            "File: lerq1937s01n08_009_plaintext_s02.txt has a valid response.\n",
            "File: lerq1938s01n01_033_plaintext_s12.txt has a valid response.\n",
            "File: lerq1937s01n07_038_plaintext_s19.txt has no valid response.\n",
            "File: lerq1937s01n08_038_plaintext_s16.txt has a valid response.\n"
          ]
        }
      ],
      "source": [
        "# let's see if we got any responses at all meaning responses that are not [] or None\n",
        "for file_name, response in responses.items():\n",
        "    if response and response != \"[]\":\n",
        "        print(f\"File: {file_name} has a valid response.\")\n",
        "    else:\n",
        "        print(f\"File: {file_name} has no valid response.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "00cc9947",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00cc9947",
        "outputId": "a64c5ea5-550f-457e-e255-a10307ac4578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 15 valid responses from the model across the sampled files.\n"
          ]
        }
      ],
      "source": [
        "# Let's get valid responses only and print them\n",
        "valid_response = {}\n",
        "for file_name, response in responses.items():\n",
        "    if response and response != \"[]\":\n",
        "        valid_response[file_name] = response\n",
        "# how many valid responses do we have?\n",
        "print(f\"Got {len(valid_response)} valid responses from the model across the sampled files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "b03372ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b03372ca",
        "outputId": "e1871ab8-a534-4132-a9bc-d944a3ada79f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: lerq1938s01n04_014_plaintext_s10.txt\n",
            "Response: [\n",
            "  {\n",
            "    \"phrase\": \"Latvia is primarily an agricultural country\",\n",
            "    \"explanation\": \"This statement highlights the foundational economic structure of Latvia, indicating that agriculture is its main economic activity.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  }\n",
            "]\n",
            "\n",
            "File: lerq1939s01n04_020_plaintext_s08.txt\n",
            "Response: ```json\n",
            "[\n",
            "  {\n",
            "    \"phrase\": \"large fields of oats\",\n",
            "    \"explanation\": \"Refers to the cultivation of oats, likely for animal feed, indicating a traditional agricultural practice.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"local yield of potatoes is designated for admixture to the petrol imported from abroad\",\n",
            "    \"explanation\": \"Suggests an innovative agricultural practice where a portion of potato yield is used for industrial purposes, potentially as an additive to fuel, rather than solely for food.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"tractors\",\n",
            "    \"explanation\": \"Refers to mechanized agricultural equipment used for farming tasks.\",\n",
            "    \"subcategory\": \"Mechanization of agriculture\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"horse transport is still more or less dominant\",\n",
            "    \"explanation\": \"Indicates the continued reliance on traditional, non-mechanized forms of transport in rural areas, highlighting the slow pace of agricultural modernization.\",\n",
            "    \"subcategory\": \"Agricultural modernization\"\n",
            "  }\n",
            "]\n",
            "```\n",
            "\n",
            "File: lerq1940s01n01_007_plaintext_s03.txt\n",
            "Response: [\n",
            "  {\n",
            "    \"phrase\": \"Chamber of Agriculture\",\n",
            "    \"explanation\": \"This refers to an official body representing agricultural interests, suggesting an organized approach to agricultural policy or advocacy.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"supply of the principal foodstuffs\",\n",
            "    \"explanation\": \"This indicates the nation's ability to produce or secure essential food items, crucial for national self-sufficiency and resilience during difficult times.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  }\n",
            "]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# let's print first 3 valid responses\n",
        "for i, (file_name, response) in enumerate(valid_response.items()):\n",
        "    if i < 3:  # Print only the first 3 valid responses\n",
        "        print(f\"File: {file_name}\")\n",
        "        print(f\"Response: {response}\\n\")\n",
        "    else:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1966916d",
      "metadata": {
        "id": "1966916d"
      },
      "source": [
        "## Converting responses from JSON to Python dictionary\n",
        "\n",
        "Our LLM responses are formated as JSON strings, so we need to convert them to Python dictionaries and/or lists for easier manipulation and analysis. We can use the `json` module in Python to do this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "dda854a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dda854a6",
        "outputId": "7035dab0-39cb-4be7-be96-8e932602037a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error decoding JSON for file lerq1939s01n04_020_plaintext_s08.txt: Expecting value: line 1 column 1 (char 0)\n",
            "Error decoding JSON for file lerq1937s01n08_017_plaintext_s05.txt: Expecting value: line 1 column 1 (char 0)\n",
            "Error decoding JSON for file lerq1939s01n01_035_plaintext_s13.txt: Expecting value: line 1 column 1 (char 0)\n",
            "Error decoding JSON for file lerq1936s01n03_018_plaintext_s09.txt: Expecting value: line 1 column 1 (char 0)\n",
            "Error decoding JSON for file lerq1937s01n07_008_plaintext_s04.txt: Expecting value: line 1 column 1 (char 0)\n",
            "Error decoding JSON for file lerq1940s01n01_026_plaintext_s10.txt: Expecting value: line 1 column 1 (char 0)\n",
            "File: lerq1938s01n04_014_plaintext_s10.txt\n",
            "Response: [{'phrase': 'Latvia is primarily an agricultural country', 'explanation': 'This statement highlights the foundational economic structure of Latvia, indicating that agriculture is its main economic activity.', 'subcategory': 'Other agricultural practices'}]\n",
            "\n",
            "File: lerq1940s01n01_007_plaintext_s03.txt\n",
            "Response: [{'phrase': 'Chamber of Agriculture', 'explanation': 'This refers to an official body representing agricultural interests, suggesting an organized approach to agricultural policy or advocacy.', 'subcategory': 'Other agricultural practices'}, {'phrase': 'supply of the principal foodstuffs', 'explanation': \"This indicates the nation's ability to produce or secure essential food items, crucial for national self-sufficiency and resilience during difficult times.\", 'subcategory': 'Other agricultural practices'}]\n",
            "\n",
            "File: lerq1936s01n02_033_plaintext_s21.txt\n",
            "Response: [{'phrase': 'flax, grain, hides, skins, butter and timber were exported through the ports of Riga, Ventspils and Liepaja', 'explanation': 'This indicates that agricultural and animal products, along with timber, were significant export commodities for Latvia, facilitated by its port infrastructure.', 'subcategory': 'Agricultural exports'}]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# let's go through all responses and convert them from JSON strings to Python data structures\n",
        "# this will also be a good test on LLM output stability\n",
        "import json\n",
        "# let's convert all responses from JSON strings to Python data structures\n",
        "converted_responses = {}\n",
        "bad_responses = {}\n",
        "for file_name, response in valid_response.items():\n",
        "    try:\n",
        "        converted_responses[file_name] = json.loads(response)  # Convert JSON string to Python data structure\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON for file {file_name}: {e}\")\n",
        "        bad_responses[file_name] = response\n",
        "\n",
        "# let's print the first 3 converted responses\n",
        "for i, (file_name, response) in enumerate(converted_responses.items()):\n",
        "    if i < 3:  # Print only the first 3 converted responses\n",
        "        print(f\"File: {file_name}\")\n",
        "        print(f\"Response: {response}\\n\")\n",
        "    else:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "70b6338f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70b6338f",
        "outputId": "9049e13e-1dfb-463f-b8f2-ed7ec0a2b65c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: lerq1939s01n04_020_plaintext_s08.txt has a bad response: ```json\n",
            "[\n",
            "  {\n",
            "    \"phrase\": \"large fields of oats\",\n",
            "    \"explanation\": \"Refers to the cultivation of oats, likely for animal feed, indicating a traditional agricultural practice.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"local yield of potatoes is designated for admixture to the petrol imported from abroad\",\n",
            "    \"explanation\": \"Suggests an innovative agricultural practice where a portion of potato yield is used for industrial purposes, potentially as an additive to fuel, rather than solely for food.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"tractors\",\n",
            "    \"explanation\": \"Refers to mechanized agricultural equipment used for farming tasks.\",\n",
            "    \"subcategory\": \"Mechanization of agriculture\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"horse transport is still more or less dominant\",\n",
            "    \"explanation\": \"Indicates the continued reliance on traditional, non-mechanized forms of transport in rural areas, highlighting the slow pace of agricultural modernization.\",\n",
            "    \"subcategory\": \"Agricultural modernization\"\n",
            "  }\n",
            "]\n",
            "```\n",
            "File: lerq1937s01n08_017_plaintext_s05.txt has a bad response: ```json\n",
            "[\n",
            "  {\n",
            "    \"phrase\": \"radical Agrarian Reform\",\n",
            "    \"explanation\": \"A fundamental change in land ownership, redistributing land from large landowners to create new, smaller farms.\",\n",
            "    \"subcategory\": \"Land reform\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"48% of the entire agricultural area belonged to big landowners\",\n",
            "    \"explanation\": \"Describes the pre-reform distribution of agricultural land, indicating a high concentration of ownership.\",\n",
            "    \"subcategory\": \"Land reform\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"overwhelming majority of the rural population was landless\",\n",
            "    \"explanation\": \"Highlights the social and economic problem of landlessness among the rural populace before the reform.\",\n",
            "    \"subcategory\": \"Land reform\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"established a new system of cultivating the soil\",\n",
            "    \"explanation\": \"Refers to changes in agricultural practices and organization resulting from the land reform.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"About 72,000 new farms have been created from 1,500 large estates\",\n",
            "    \"explanation\": \"Quantifies the outcome of the agrarian reform, showing the significant subdivision of large landholdings into many smaller farms.\",\n",
            "    \"subcategory\": \"Land reform\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"linking the former landless inhabitants to the soil\",\n",
            "    \"explanation\": \"Explains a social objective of the land reform: providing land to previously landless rural residents.\",\n",
            "    \"subcategory\": \"Land reform\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"problem of covering the domestic demand for breadstuffs with the country's own yield of grain\",\n",
            "    \"explanation\": \"Refers to the national goal of achieving self-sufficiency in grain production.\",\n",
            "    \"subcategory\": \"Grain storage and reserves\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"increasing the yield of breadstuffs\",\n",
            "    \"explanation\": \"Indicates efforts to improve the productivity of grain farming.\",\n",
            "    \"subcategory\": \"Crop yields\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"export of agricultural produce\",\n",
            "    \"explanation\": \"Describes the country's move towards exporting farm goods after achieving domestic self-sufficiency.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"replenishing and improving the country's supply of livestock\",\n",
            "    \"explanation\": \"Refers to efforts to increase and enhance the quality of farm animals.\",\n",
            "    \"subcategory\": \"Dairy and livestock production\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"creating a firm basis for the dairy industry\",\n",
            "    \"explanation\": \"Indicates the development of the dairy sector as a significant part of the agricultural economy.\",\n",
            "    \"subcategory\": \"Dairy and livestock production\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"dairy industry, which has been expanding perceptibly in recent years\",\n",
            "    \"explanation\": \"Highlights the growth and increasing importance of milk and dairy product production.\",\n",
            "    \"subcategory\": \"Dairy and livestock production\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"butter, which figures prominently in the total value of Latvian exports\",\n",
            "    \"explanation\": \"Identifies butter as a key agricultural export commodity.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"development of pig farming\",\n",
            "    \"explanation\": \"Refers to the expansion of pig breeding as another important agricultural sector.\",\n",
            "    \"subcategory\": \"Dairy and livestock production\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"timber materials and flax\",\n",
            "    \"explanation\": \"Mentions other agricultural products (or products derived from agriculture) that are important for export.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"Latvia is an agricultural country\",\n",
            "    \"explanation\": \"States the fundamental economic character of Latvia, emphasizing its reliance on agriculture.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"producing sufficient breadstuffs for our own requirements\",\n",
            "    \"explanation\": \"Refers to the achievement of self-sufficiency in grain production.\",\n",
            "    \"subcategory\": \"Grain storage and reserves\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"agricultural machines\",\n",
            "    \"explanation\": \"Mentions the manufacture of machinery specifically for agricultural use, indicating a push towards mechanization.\",\n",
            "    \"subcategory\": \"Mechanization of agriculture\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"central union 'Turiba', which embraces all the co-operative societies and similar economic organisations and enterprises in the whole country\",\n",
            "    \"explanation\": \"Describes a central organization that unifies various cooperative societies, likely including peasant cooperatives, to coordinate economic activities.\",\n",
            "    \"subcategory\": \"Peasant cooperatives\"\n",
            "  }\n",
            "]\n",
            "```\n",
            "File: lerq1939s01n01_035_plaintext_s13.txt has a bad response: ```json\n",
            "[\n",
            "  {\n",
            "    \"phrase\": \"Latvian Chamber of Agriculture\",\n",
            "    \"explanation\": \"An institution employing agronomists and cultural-technicists, indicating a structured approach to agricultural development and support.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"Land culture (agronomists, foresters, cultural-technicists)\",\n",
            "    \"explanation\": \"One of the six sections of the Chamber of Professions, highlighting the importance of land management, agricultural expertise, and technical skills in rural development.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  }\n",
            "]\n",
            "```\n",
            "File: lerq1936s01n03_018_plaintext_s09.txt has a bad response: ```json\n",
            "[]\n",
            "```\n",
            "File: lerq1937s01n07_008_plaintext_s04.txt has a bad response: ```json\n",
            "[\n",
            "  {\n",
            "    \"phrase\": \"good arable land, which constitutes the basis of our national economy\",\n",
            "    \"explanation\": \"Highlights the fundamental importance of fertile agricultural land as the foundation of Latvia's national economy.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"requirements of Latvian agriculture and the elaborating of agricultural produce\",\n",
            "    \"explanation\": \"Indicates that new industries are developing to support and process agricultural products, suggesting a focus on value-added agricultural output.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"exploiting the less valuable layers of gypseous stone for agricultural purposes, in order to augment fertility\",\n",
            "    \"explanation\": \"Refers to the potential use of gypsum as a fertilizer to improve soil fertility and crop yields.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"large quantity of milled gypsum has been distributed at 40 different places in Latvia in order to test the efficacy of this material as a fertiliser\",\n",
            "    \"explanation\": \"Describes experimental trials to assess the effectiveness of gypsum as a fertilizer, indicating a scientific approach to improving agricultural practices.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"This loose lime is of great value as a fertilising material and the possibility of its wider application for agricultural purposes is claiming due attention at present\",\n",
            "    \"explanation\": \"Emphasizes the importance of loose lime as a fertilizer and the ongoing consideration of its expanded use in agriculture.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  }\n",
            "]\n",
            "```\n",
            "File: lerq1940s01n01_026_plaintext_s10.txt has a bad response: ```json\n",
            "[\n",
            "  {\n",
            "    \"phrase\": \"Ministry of Agriculture\",\n",
            "    \"explanation\": \"Refers to a government department responsible for agricultural policy and oversight.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  }\n",
            "]\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# let's print the bad responses\n",
        "for file_name, response in bad_responses.items():\n",
        "    print(f\"File: {file_name} has a bad response: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c78ec18a",
      "metadata": {
        "id": "c78ec18a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "6cbd75a6",
      "metadata": {
        "id": "6cbd75a6"
      },
      "source": [
        "### Fixing bad responses\n",
        "\n",
        "We see that bad responses are actually are not truly bad they just start with ```json and end with ```. We can fix this by removing the ```json and ``` from the start and end of the response string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "f0f32894",
      "metadata": {
        "id": "f0f32894"
      },
      "outputs": [],
      "source": [
        "# let's go through all responses and convert them from JSON strings to Python dictionaries\n",
        "# we will also strip the response from ```json and ``` at the start and end of the response string\n",
        "good_responses = {}\n",
        "bad_responses = {}\n",
        "for file_name, response in responses.items():\n",
        "    response = response.lstrip(\"```json\")  # Remove ```json from the start\n",
        "    response = response.rstrip(\"```\")  # Remove ``` from the end\n",
        "    try:\n",
        "        good_responses[file_name] = json.loads(response)  # Convert JSON string to Python data structure\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON for file {file_name}: {e}\")\n",
        "        bad_responses[file_name] = response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "c91c3514",
      "metadata": {
        "id": "c91c3514"
      },
      "outputs": [],
      "source": [
        "# let's print first bad responses again\n",
        "for file_name, response in bad_responses.items():\n",
        "    print(f\"File: {file_name} has a bad response: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d0746dd",
      "metadata": {
        "id": "3d0746dd"
      },
      "source": [
        "### Fixing bad JSON\n",
        "\n",
        "We have gotten the easy cases fixed but the last malformed JSON is due to double quotes not being escaped properly in all places. We can fix this by replacing the double quotes with escaped double quotes.\n",
        "\n",
        "Alternatively we could apply stricter instructions to the model to return valid JSON. However, this would require us to adjust our system prompt and possibly the model parameters as well.\n",
        "\n",
        "We have a third option which is to supply the response to LLM again with different prompt and have LLM fix the JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "b57db797",
      "metadata": {
        "id": "b57db797"
      },
      "outputs": [],
      "source": [
        "# let's fix the malformed JSON responses\n",
        "fix_json_prompt = \"\"\"Please fix the following JSON response which should be an array of objects where double quotes are not escaped properly.\n",
        "The response should be proper JSON string without ```json start and without ``` end with all other content intact.\"\"\"\n",
        "fixed_responses = {}\n",
        "for file_name, response in bad_responses.items():\n",
        "    # we will use the same get_openrouter_response function to fix the JSON\n",
        "    fixed_response = get_openrouter_response(fix_json_prompt, response, model=\"google/gemini-2.5-flash\")\n",
        "    print(f\"Fixed response for {file_name}: {fixed_response}\")\n",
        "    # try parsing response to ensure it is valid JSON\n",
        "    try:\n",
        "        json.loads(fixed_response)  # Validate the fixed response\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding fixed JSON for file {file_name}: {e}\")\n",
        "        fixed_response = None\n",
        "    if fixed_response:\n",
        "        fixed_responses[file_name] = fixed_response\n",
        "    else:\n",
        "        print(f\"Failed to fix JSON for {file_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running custom system prompt on specific folder\n",
        "\n",
        "Now that we have experimented with a small sample let's create a new function that takes the following parameters:\n",
        "\n",
        "\n",
        "- system_prompt - our custom instructions\n",
        "- file_folder where the files to be analyzed is needed\n",
        "- file_extension where we want .txt to be default extenstion\n",
        "- max_files where we want None to be default meaning unlimited files\n",
        "- seed where we want None to be default this would be used when max_files is not None\n",
        "- model where default would be \"google/gemini-2.5-flash\"\n",
        "\n",
        "We could add a few more parameters such as temperature and max tokens but this is enough to get started\n"
      ],
      "metadata": {
        "id": "34s6AuyekV4n"
      },
      "id": "34s6AuyekV4n"
    },
    {
      "cell_type": "code",
      "source": [
        "# we want a function that will take our system prompt and file folder and will save in save_folder all responses using original file name and custom suffix\n",
        "# - system_prompt - our custom instructions\n",
        "# - file_folder where the files to be analyzed is needed\n",
        "# - file_extension where we want .txt to be default extenstion\n",
        "# - max_files where we want None to be default meaning unlimited files\n",
        "# - seed where we want None to be default this would be used when max_files is not None\n",
        "# - model where default would be \"google/gemini-2.5-flash\"\n",
        "# - save_folder where default would be \"responses\"\n",
        "# - we could specific save suffix but instead let's have a filename friendly version of model meaning we will remove / and - and replace those with _\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "def save_responses_from_folder(system_prompt,\n",
        "                               file_folder,\n",
        "                               file_extension=\".txt\",\n",
        "                               max_files=None,\n",
        "                               seed=None,\n",
        "                               model=\"google/gemini-2.5-flash\",\n",
        "                               save_folder=\"responses\",\n",
        "                               delay=0.1,\n",
        "                               verbose=True):\n",
        "    \"\"\"\n",
        "    Processes files in a folder using an LLM and saves the responses.\n",
        "\n",
        "    :param system_prompt: The system prompt to guide the model's behavior.\n",
        "    :param file_folder: Path to the directory containing files to process.\n",
        "    :param file_extension: The extension of the files to process (default is .txt).\n",
        "    :param max_files: Maximum number of files to process (default is None for all files).\n",
        "    :param seed: Seed for random sampling if max_files is not None.\n",
        "    :param model: The model to use for the request (default is google/gemini-2.5-flash).\n",
        "    :param save_folder: The folder to save the responses (default is \"responses\").\n",
        "    :param delay: Delay in seconds between API calls to avoid rate limits.\n",
        "    :param verbose: If True, print detailed information during processing.\n",
        "    \"\"\"\n",
        "    file_folder_path = Path(file_folder)\n",
        "    save_folder_path = Path(save_folder)\n",
        "\n",
        "    if not file_folder_path.exists():\n",
        "        print(f\"Error: File folder '{file_folder}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    # Create save folder if it doesn't exist\n",
        "    save_folder_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Create a filename-friendly version of the model name\n",
        "    model_suffix = model.replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
        "\n",
        "    # Find files with the specified extension\n",
        "    all_files = list(file_folder_path.rglob(f\"*{file_extension}\"))\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Found {len(all_files)} '{file_extension}' files in '{file_folder}'.\")\n",
        "\n",
        "    files_to_process = all_files\n",
        "    if max_files is not None and max_files < len(all_files):\n",
        "        if seed is not None:\n",
        "            random.seed(seed)\n",
        "        files_to_process = random.sample(all_files, max_files)\n",
        "        if verbose:\n",
        "            print(f\"Processing a random sample of {len(files_to_process)} files.\")\n",
        "\n",
        "    if not files_to_process:\n",
        "        print(\"No files to process.\")\n",
        "        return\n",
        "\n",
        "    for file in tqdm(files_to_process, desc=\"Processing files\"):\n",
        "        if verbose:\n",
        "            now = datetime.now()\n",
        "            print(f\"\\n{now.strftime('%Y-%m-%d %H:%M:%S')}  Processing file: {file.relative_to(file_folder_path)} ({file.stat().st_size:,} bytes)\")\n",
        "\n",
        "        # Construct the output filename\n",
        "        output_filename = save_folder_path / f\"{file.stem}_{model_suffix}.{file.suffix}\"\n",
        "\n",
        "\n",
        "        # Create parent directories for the output file if they don't exist\n",
        "        output_filename.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Check if response already exists\n",
        "        if output_filename.exists():\n",
        "            if verbose:\n",
        "                print(f\"  Response already exists for {file.name}, skipping.\")\n",
        "            continue\n",
        "\n",
        "\n",
        "        with file.open('r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        response = get_openrouter_response(system_prompt, content, model=model)\n",
        "\n",
        "        # Save the response to a file\n",
        "        with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
        "            outfile.write(response)\n",
        "            if verbose:\n",
        "                print(f\"  Response saved to {output_filename}\")\n",
        "\n",
        "        time.sleep(delay)  # Pause between requests\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\nFinished processing files.\")"
      ],
      "metadata": {
        "id": "nshDX3a-lYfZ"
      },
      "id": "nshDX3a-lYfZ",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's get a folder that we want to process\n",
        "# it would be data/Latvian_Economic_Review folder here\n",
        "# for now let's only get 10 responses\n",
        "source_folder = \"data/Latvian_Economic_Review\"\n",
        "# now we can call the function using same old agriculture prompt\n",
        "save_responses_from_folder(agriculture_system_prompt,\n",
        "                           source_folder,\n",
        "                           max_files = 10)"
      ],
      "metadata": {
        "id": "6tpcRUPcoJm-",
        "outputId": "4530551a-7afc-42e8-fa36-007750f45b97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6tpcRUPcoJm-",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 419 '.txt' files in 'data/Latvian_Economic_Review'.\n",
            "Processing a random sample of 10 files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2025-08-06 18:27:49  Processing file: lerq1936s01n02_018_plaintext_s07.txt (1,208 bytes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  10%|█         | 1/10 [00:01<00:13,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Response saved to responses/lerq1936s01n02_018_plaintext_s07_google_gemini_2.5_flash..txt\n",
            "\n",
            "2025-08-06 18:27:51  Processing file: lerq1937s01n08_033_plaintext_s12.txt (13,662 bytes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  20%|██        | 2/10 [00:02<00:10,  1.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Response saved to responses/lerq1937s01n08_033_plaintext_s12_google_gemini_2.5_flash..txt\n",
            "\n",
            "2025-08-06 18:27:52  Processing file: lerq1940s01n01_040_plaintext_s17.txt (6,022 bytes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  30%|███       | 3/10 [00:03<00:07,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Response saved to responses/lerq1940s01n01_040_plaintext_s17_google_gemini_2.5_flash..txt\n",
            "\n",
            "2025-08-06 18:27:53  Processing file: lerq1937s01n07_033_plaintext_s13.txt (6,771 bytes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  40%|████      | 4/10 [00:05<00:08,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Response saved to responses/lerq1937s01n07_033_plaintext_s13_google_gemini_2.5_flash..txt\n",
            "\n",
            "2025-08-06 18:27:55  Processing file: lerq1938s01n02_009_plaintext_s04.txt (16,623 bytes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  50%|█████     | 5/10 [00:08<00:10,  2.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Response saved to responses/lerq1938s01n02_009_plaintext_s04_google_gemini_2.5_flash..txt\n",
            "\n",
            "2025-08-06 18:27:58  Processing file: lerq1936s01n01_023_plaintext_s15.txt (370 bytes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  60%|██████    | 6/10 [00:09<00:07,  1.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Response saved to responses/lerq1936s01n01_023_plaintext_s15_google_gemini_2.5_flash..txt\n",
            "\n",
            "2025-08-06 18:27:59  Processing file: lerq1937s01n06_009_plaintext_s09.txt (6,257 bytes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  70%|███████   | 7/10 [00:11<00:04,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Response saved to responses/lerq1937s01n06_009_plaintext_s09_google_gemini_2.5_flash..txt\n",
            "\n",
            "2025-08-06 18:28:00  Processing file: lerq1938s01n03_021_plaintext_s08.txt (7,975 bytes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  80%|████████  | 8/10 [00:12<00:02,  1.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Response saved to responses/lerq1938s01n03_021_plaintext_s08_google_gemini_2.5_flash..txt\n",
            "\n",
            "2025-08-06 18:28:01  Processing file: lerq1936s01n03_022_plaintext_s12.txt (1,851 bytes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing files:  90%|█████████ | 9/10 [00:12<00:01,  1.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Response saved to responses/lerq1936s01n03_022_plaintext_s12_google_gemini_2.5_flash..txt\n",
            "\n",
            "2025-08-06 18:28:02  Processing file: lerq1937s01n06_006_plaintext_s03.txt (785 bytes)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 10/10 [00:13<00:00,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Response saved to responses/lerq1937s01n06_006_plaintext_s03_google_gemini_2.5_flash..txt\n",
            "\n",
            "Finished processing files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adjusting system prompt for simpler output\n",
        "\n",
        "Now we see that sometimes instead of JSON we get nonstandard output.\n",
        "\n",
        "This presents a problem for automatic processing, so again we would need to write more code to catch these non standard situations or make our prompt even stronger or change our model.\n",
        "\n",
        "For now we will change prompt to be simpler and to produce simple CSV - comma separated values output"
      ],
      "metadata": {
        "id": "74Ds0R4tpsUZ"
      },
      "id": "74Ds0R4tpsUZ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}