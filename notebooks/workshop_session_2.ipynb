{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61c6cb6a",
   "metadata": {},
   "source": [
    "# Using LLMs in Humanities Research via API\n",
    "\n",
    "## Session 2 14.00-15.30 - Working with LLMs via API\n",
    "\n",
    "Through practical examples, we will explore prompt engineering techniques for tasks such as concept mining and named entity recognition in textual data.\n",
    "\n",
    "## Session Outline\n",
    "\n",
    "- **Prompt Engineering**: Techniques for crafting effective prompts to guide LLMs in generating relevant and accurate responses.\n",
    "- **Concept Mining**: Using LLMs to extract key concepts from text, enabling researchers to identify important themes and ideas.\n",
    "- **Named Entity Recognition (NER)**: Implementing NER to identify and classify entities in text, such as people, organizations, and locations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3328f6",
   "metadata": {},
   "source": [
    "## BSSDH 2025 Workshop Data\n",
    "\n",
    "Before we start exploring the API, let's take a look at the corpus of documents we will be working. \n",
    "Data for workshops in [Baltic Summer School of Digital Humanities 2025](https://www.digitalhumanities.lv/bssdh/2025/about/)\n",
    "\n",
    "**Repository:** https://github.com/LNB-DH/BSSDH_2025_workshop_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## CORPUS OVERVIEW\n",
    "\n",
    "\n",
    "1. SOURCE MATERIAL\n",
    "------------------\n",
    "\n",
    "| Periodical | Details |\n",
    "|------------|---------|\n",
    "| \"Rigasche Zeitung\" (RZei) (1918–1919) | - **Data file:** `Rigasche_Zeitung_1918_1919.zip`<br>- **Download Rigasche Zeitung:** https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Rigasche_Zeitung_1918_1919.zip<br>- Morning newspaper, intermittently published from 1778 to 1919 in Riga.<br>- Language: German (Fraktur script)<br>- Once the most popular morning paper in the Baltic provinces of the Russian Empire.<br>- Covered general political and economic news in Riga, the Baltics, the Russian Empire, and internationally.<br>- Historical context: World War I, Latvian War of Independence.<br>- Link: https://periodika.lv/#periodicalMeta:234;-1<br>- More info: https://enciklopedija.lv/skirklis/163962 |\n",
    "| \"Latvian Economic Review\" (LERQ) (1936–1940) | - **Data file:** `Latvian_Economic_Review_1936_1940.zip`<br>- **Download Latvian Economic Review:** https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Latvian_Economic_Review_1936_1940.zip<br>- Full title: \"Latvian Economic Review: A quarterly review of trade, industry and agriculture\".<br>- Language: English (modern)<br>- Published by the Latvian Chamber of Commerce and Industry (established 1934).<br>- Focused on cross-border representation of Latvian economy during the Great Depression, increasing state control, push for autarky, and start of WWII.<br>- Link: https://periodika.lv/#periodicalItem:620 |\n",
    "\n",
    "2. CORPUS INFORMATION\n",
    "----------------------\n",
    "\n",
    "| Metric | RZei | LERQ |\n",
    "|--------|------|------|\n",
    "| Token Count (words) | 5.37 million | 0.5 million |\n",
    "| Issue Count | 359 issues | 18 issues |\n",
    "| Segment (Article <=> File) Count | 4,597 | 419 |\n",
    "| Language | German | English |\n",
    "| Script | Fraktur | Modern |\n",
    "\n",
    "Filename Structure:\n",
    "-------------------\n",
    "Format: [periodical][year][volume#*][issue#]_[page#]_[[plaintext]]_[segment#]\n",
    "\n",
    "Example: `lerq1936s01n02_031_plaintext_s17.txt`\n",
    "         → 17th segment from LERQ, Issue 2, 1936, page 31.\n",
    "\n",
    "*Volume value in corpus is one in all cases.\n",
    "\n",
    "3. METHODOLOGY\n",
    "---------------\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| 3.1. Source Access | Digitised issues obtained from the National Library of Latvia (https://periodika.lv/) |\n",
    "| 3.2. Processing & OCR | CCS docWORKS & ABBYY FineReader 9.0<br>- LERQ has better OCR quality than RZei<br>- No further data cleaning/normalization |\n",
    "| 3.3. Metadata Added | Fields: title, author, uri<br>- Author info available in:<br>&nbsp;&nbsp;&nbsp;&nbsp;LERQ: 4 cases (0.95%)<br>&nbsp;&nbsp;&nbsp;&nbsp;RZei: 325 cases (7.05%)<br>- Title availability:<br>&nbsp;&nbsp;&nbsp;&nbsp;LERQ: 95.7%, RZei: 99.15%<br>- URI coverage: 100% for both<br>- URIs point to LNB DOM system |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a419d247",
   "metadata": {},
   "source": [
    "## Extracting documents\n",
    "\n",
    "We could extract documents manually by downloading the appropriate zip file and extracting files by *hand* using file extracting capabilities built into your Operating System(Windows has built in extractor) or using external program such as 7-zip, WinRAR, etc. However, it is more replicable and convenient to use a script that will do this for us. We would supply a url or file name and the script would download the file, extract it to approparite location, and return a list of files that were extracted.\n",
    "\n",
    "### Additional considerations when extracting documents\n",
    "\n",
    "* Where will be extracted files be stored? - Ideally we would have a same relative structure when extracting files locally and on remote server such as Google Colab.\n",
    "* How will we handle file names? - Usually we would like to keep the original file names, but we might want to add some additional information such as source or date of extraction.\n",
    "* How will we handle errors? - We should consider what to do if the file cannot be downloaded or extracted. Should we skip it or raise an error?\n",
    "\n",
    "### Extracting Latvian Economic Review\n",
    "\n",
    "For this session we will extract Latvian Economic Review (LERQ) corpus. We will use a script that will download the file, extract it to appropriate location, and return a list of files that were extracted.\n",
    "\n",
    "We will write a function in Python that will do this for us. The function will take a URL or file name as an argument and will download file from url and then extract it. We will have a default location where the files will be extracted, but we can also specify a different location if needed.\n",
    "\n",
    "```python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2b4ecf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will extract data from https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Latvian_Economic_Review_1936_1940.zip\n",
      "Starting download at 2025-07-31 11:40:24.272770\n",
      "Starting download from https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Latvian_Economic_Review_1936_1940.zip\n",
      "Download finished at 2025-07-31 11:40:25.271504 taking 0:00:00.998734 seconds\n",
      "Starting extraction to data at 2025-07-31 11:40:25.271504\n",
      "Extraction finished at 2025-07-31 11:40:27.656727 taking 0:00:02.385223 seconds\n",
      "Total time taken: 0:00:03.383957 seconds\n"
     ]
    }
   ],
   "source": [
    "url = \"https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Latvian_Economic_Review_1936_1940.zip\"\n",
    "print(\"Will extract data from\", url)\n",
    "# next we define a function that will download and extract the zip file, this way we can reuse it later if needed\n",
    "# we can also set some default values for the arguments, so we do not have to specify\n",
    "# default values always come after the mandatory arguments in Python functions\n",
    "def extract_zip(url, output_dir=\"data\", verbose=False):\n",
    "    # we could have imported these at the top, but we want to keep the script self-contained\n",
    "    import requests  # this should be cached by notebooks, so it **should** not require importing it every time\n",
    "    from zipfile import ZipFile\n",
    "    from io import BytesIO\n",
    "\n",
    "    # In verbose mode let's some extra information about the download and extraction process\n",
    "    # This is useful for debugging and understanding the flow of the script\n",
    "    from datetime import datetime\n",
    "    if verbose:\n",
    "        download_start = datetime.now()\n",
    "        # we print start time including milliseconds\n",
    "        print(f\"Starting download at {download_start.strftime('%Y-%m-%d %H:%M:%S.%f')}\")\n",
    "        print(\"Starting download from\", url)\n",
    "    response = requests.get(url)\n",
    "    if verbose:\n",
    "        download_finish = datetime.now()\n",
    "        print(f\"Download finished at {download_finish.strftime('%Y-%m-%d %H:%M:%S.%f')} taking {download_finish - download_start} seconds\")\n",
    "    if response.status_code == 200: # it is possible a request fails, e.g. if the URL is incorrect\n",
    "        if verbose:\n",
    "            extract_start = datetime.now()\n",
    "            print(f\"Starting extraction to {output_dir} at {extract_start.strftime('%Y-%m-%d %H:%M:%S.%f')}\")\n",
    "        with ZipFile(BytesIO(response.content)) as zf:\n",
    "            zf.extractall(output_dir)\n",
    "        if verbose:\n",
    "            extract_end = datetime.now()\n",
    "            print(f\"Extraction finished at {extract_end.strftime('%Y-%m-%d %H:%M:%S.%f')} taking {extract_end - extract_start} seconds\")\n",
    "            print(f\"Total time taken: {extract_end - download_start} seconds\")\n",
    "    else:\n",
    "        print(\"Failed to download data:\", response.status_code)\n",
    "\n",
    "# now that we have our function defined, we can call it immediately\n",
    "# note we do not supply all arguments, first one is mandatory, the rest are optional\n",
    "# so we skip over output_dir in this case\n",
    "extract_zip(url, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bc187d",
   "metadata": {},
   "source": [
    "### Getting information about extracted files\n",
    "\n",
    "It is a good practice to double check what files were extracted and where they are located. We can do this by listing the files in the directory where we extracted them. We can use Python pathlib to do this. \n",
    "The goal is to double check that what we extacted matches what we expected. We can also check the file names and their structure to make sure they are correct.\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd0d77bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1 files to data\n",
      "Latvian_Economic_Review\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "extract_dir = Path(\"data\") # note this is a a relative path, relative to the current working directory for the notebook\n",
    "# let's check if the directory exists and how many files it contains\n",
    "if extract_dir.exists():\n",
    "    files = list(extract_dir.glob(\"*\"))  # this will list all files in the directory\n",
    "    print(f\"Extracted {len(files)} files to {extract_dir}\")\n",
    "    for file in files:\n",
    "        print(file.name)  # print the name of each file \n",
    "else:\n",
    "    print(f\"Directory {extract_dir} does not exist. Please check the extraction process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7ade9a",
   "metadata": {},
   "source": [
    "### Getting information about subfolders in the extracted directory\n",
    "Looks like we only have a single file but it is actually not a file but a directory. This is because we extracted a zip file that contains files under a single directory. \n",
    "\n",
    "Next we want to check how many total files we have and also how many files we have with *.txt extension. This will help us to understand how many files we can work with and if there are any files that we might want to exclude from our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d05596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check how many files total we have and how many files with *.txt extension counting all subfolders \n",
    "# this means we will perform a recursive search for all files in the directory\n",
    "\n",
    "def analyze_directory_contents(directory_path, verbose=True):\n",
    "    \"\"\"\n",
    "    Analyze the contents of a directory recursively and provide detailed information.\n",
    "    \n",
    "    Args:\n",
    "        directory_path: Path object or string path to the directory\n",
    "        verbose: If True, print detailed information about file types and structure\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing analysis results\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    directory = Path(directory_path)\n",
    "    \n",
    "    if not directory.exists():\n",
    "        print(f\"Directory {directory} does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    # Get all files recursively\n",
    "    all_files = list(directory.rglob(\"*\"))\n",
    "    \n",
    "    # Separate files from directories\n",
    "    files_only = [f for f in all_files if f.is_file()]\n",
    "    directories_only = [f for f in all_files if f.is_dir()]\n",
    "    \n",
    "    # Count files by extension\n",
    "    file_extensions = {}\n",
    "    for file in files_only:\n",
    "        ext = file.suffix.lower()\n",
    "        if ext == '':\n",
    "            ext = '(no extension)'\n",
    "        file_extensions[ext] = file_extensions.get(ext, 0) + 1\n",
    "    \n",
    "    # Count .txt files specifically\n",
    "    txt_files = [f for f in files_only if f.suffix.lower() == '.txt']\n",
    "    \n",
    "    # Analysis results\n",
    "    results = {\n",
    "        'total_items': len(all_files),\n",
    "        'total_files': len(files_only),\n",
    "        'total_directories': len(directories_only),\n",
    "        'txt_files': len(txt_files),\n",
    "        'file_extensions': file_extensions,\n",
    "        'txt_file_paths': txt_files\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"📁 Directory Analysis: {directory}\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total items found (files + directories): {results['total_items']}\")\n",
    "        print(f\"Total files: {results['total_files']}\")\n",
    "        print(f\"Total directories: {results['total_directories']}\")\n",
    "        print(f\"Text files (.txt): {results['txt_files']}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"📊 File Extensions Summary:\")\n",
    "        print(\"-\" * 30)\n",
    "        for ext, count in sorted(file_extensions.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {ext}: {count} files\")\n",
    "        print()\n",
    "        \n",
    "        if len(directories_only) > 0:\n",
    "            print(\"📂 Directory Structure:\")\n",
    "            print(\"-\" * 30)\n",
    "            for directory in sorted(directories_only):\n",
    "                # Show relative path from the base directory\n",
    "                relative_path = directory.relative_to(directory_path)\n",
    "                print(f\"  📁 {relative_path}\")\n",
    "            print()\n",
    "        \n",
    "        if len(txt_files) > 0 and len(txt_files) <= 10:\n",
    "            print(\"📄 Sample .txt files:\")\n",
    "            print(\"-\" * 30)\n",
    "            for txt_file in sorted(txt_files)[:10]:\n",
    "                relative_path = txt_file.relative_to(directory_path)\n",
    "                file_size = txt_file.stat().st_size\n",
    "                print(f\"  📄 {relative_path} ({file_size:,} bytes)\")\n",
    "        elif len(txt_files) > 10:\n",
    "            print(f\"📄 First 10 .txt files (out of {len(txt_files)} total):\")\n",
    "            print(\"-\" * 30)\n",
    "            for txt_file in sorted(txt_files)[:10]:\n",
    "                relative_path = txt_file.relative_to(directory_path)\n",
    "                file_size = txt_file.stat().st_size\n",
    "                print(f\"  📄 {relative_path} ({file_size:,} bytes)\")\n",
    "            print(f\"  ... and {len(txt_files) - 10} more .txt files\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Now let's analyze our extracted directory\n",
    "print(\"Analyzing extracted data directory...\")\n",
    "analysis_results = analyze_directory_contents(extract_dir, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
