{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "61c6cb6a",
      "metadata": {
        "id": "61c6cb6a"
      },
      "source": [
        "# Using LLMs in Humanities Research via API\n",
        "\n",
        "## Session 2 14.00-15.30 - Working with LLMs via API\n",
        "\n",
        "Through practical examples, we will explore prompt engineering techniques for tasks such as concept mining and named entity recognition in textual data.\n",
        "\n",
        "## Session Outline\n",
        "\n",
        "- **Prompt Engineering**: Techniques for crafting effective prompts to guide LLMs in generating relevant and accurate responses.\n",
        "- **Concept Mining**: Using LLMs to extract key concepts from text, enabling researchers to identify important themes and ideas.\n",
        "- **Named Entity Recognition (NER)**: Implementing NER to identify and classify entities in text, such as people, organizations, and locations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb3328f6",
      "metadata": {
        "id": "fb3328f6"
      },
      "source": [
        "## BSSDH 2025 Workshop Data\n",
        "\n",
        "Before we start exploring the API, let's take a look at the corpus of documents we will be working.\n",
        "Data for workshops in [Baltic Summer School of Digital Humanities 2025](https://www.digitalhumanities.lv/bssdh/2025/about/)\n",
        "\n",
        "**Repository:** https://github.com/LNB-DH/BSSDH_2025_workshop_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## CORPUS OVERVIEW\n",
        "\n",
        "\n",
        "1. SOURCE MATERIAL\n",
        "------------------\n",
        "\n",
        "| Periodical | Details |\n",
        "|------------|---------|\n",
        "| \"Rigasche Zeitung\" (RZei) (1918–1919) | - **Data file:** `Rigasche_Zeitung_1918_1919.zip`<br>- **Download Rigasche Zeitung:** https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Rigasche_Zeitung_1918_1919.zip<br>- Morning newspaper, intermittently published from 1778 to 1919 in Riga.<br>- Language: German (Fraktur script)<br>- Once the most popular morning paper in the Baltic provinces of the Russian Empire.<br>- Covered general political and economic news in Riga, the Baltics, the Russian Empire, and internationally.<br>- Historical context: World War I, Latvian War of Independence.<br>- Link: https://periodika.lv/#periodicalMeta:234;-1<br>- More info: https://enciklopedija.lv/skirklis/163962 |\n",
        "| \"Latvian Economic Review\" (LERQ) (1936–1940) | - **Data file:** `Latvian_Economic_Review_1936_1940.zip`<br>- **Download Latvian Economic Review:** https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Latvian_Economic_Review_1936_1940.zip<br>- Full title: \"Latvian Economic Review: A quarterly review of trade, industry and agriculture\".<br>- Language: English (modern)<br>- Published by the Latvian Chamber of Commerce and Industry (established 1934).<br>- Focused on cross-border representation of Latvian economy during the Great Depression, increasing state control, push for autarky, and start of WWII.<br>- Link: https://periodika.lv/#periodicalItem:620 |\n",
        "\n",
        "2. CORPUS INFORMATION\n",
        "----------------------\n",
        "\n",
        "| Metric | RZei | LERQ |\n",
        "|--------|------|------|\n",
        "| Token Count (words) | 5.37 million | 0.5 million |\n",
        "| Issue Count | 359 issues | 18 issues |\n",
        "| Segment (Article <=> File) Count | 4,597 | 419 |\n",
        "| Language | German | English |\n",
        "| Script | Fraktur | Modern |\n",
        "\n",
        "Filename Structure:\n",
        "-------------------\n",
        "Format: [periodical][year][volume#*][issue#]_[page#]_[[plaintext]]_[segment#]\n",
        "\n",
        "Example: `lerq1936s01n02_031_plaintext_s17.txt`\n",
        "         → 17th segment from LERQ, Issue 2, 1936, page 31.\n",
        "\n",
        "*Volume value in corpus is one in all cases.\n",
        "\n",
        "3. METHODOLOGY\n",
        "---------------\n",
        "\n",
        "| Step | Description |\n",
        "|------|-------------|\n",
        "| 3.1. Source Access | Digitised issues obtained from the National Library of Latvia (https://periodika.lv/) |\n",
        "| 3.2. Processing & OCR | CCS docWORKS & ABBYY FineReader 9.0<br>- LERQ has better OCR quality than RZei<br>- No further data cleaning/normalization |\n",
        "| 3.3. Metadata Added | Fields: title, author, uri<br>- Author info available in:<br>&nbsp;&nbsp;&nbsp;&nbsp;LERQ: 4 cases (0.95%)<br>&nbsp;&nbsp;&nbsp;&nbsp;RZei: 325 cases (7.05%)<br>- Title availability:<br>&nbsp;&nbsp;&nbsp;&nbsp;LERQ: 95.7%, RZei: 99.15%<br>- URI coverage: 100% for both<br>- URIs point to LNB DOM system |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a419d247",
      "metadata": {
        "id": "a419d247"
      },
      "source": [
        "## Extracting documents\n",
        "\n",
        "We could extract documents manually by downloading the appropriate zip file and extracting files by *hand* using file extracting capabilities built into your Operating System(Windows has built in extractor) or using external program such as 7-zip, WinRAR, etc. However, it is more replicable and convenient to use a script that will do this for us. We would supply a url or file name and the script would download the file, extract it to approparite location, and return a list of files that were extracted.\n",
        "\n",
        "### Additional considerations when extracting documents\n",
        "\n",
        "* Where will be extracted files be stored? - Ideally we would have a same relative structure when extracting files locally and on remote server such as Google Colab.\n",
        "* How will we handle file names? - Usually we would like to keep the original file names, but we might want to add some additional information such as source or date of extraction.\n",
        "* How will we handle errors? - We should consider what to do if the file cannot be downloaded or extracted. Should we skip it or raise an error?\n",
        "\n",
        "### Extracting Latvian Economic Review\n",
        "\n",
        "For this session we will extract Latvian Economic Review (LERQ) corpus. We will use a script that will download the file, extract it to appropriate location, and return a list of files that were extracted.\n",
        "\n",
        "We will write a function in Python that will do this for us. The function will take a URL or file name as an argument and will download file from url and then extract it. We will have a default location where the files will be extracted, but we can also specify a different location if needed.\n",
        "\n",
        "```python\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f2b4ecf6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2b4ecf6",
        "outputId": "01b6ddb5-0701-4570-8d37-a9e42ae8febe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Will extract data from https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Latvian_Economic_Review_1936_1940.zip\n",
            "Starting download at 2025-08-06 22:03:34.029763\n",
            "Starting download from https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Latvian_Economic_Review_1936_1940.zip\n",
            "Download finished at 2025-08-06 22:03:35.119427 taking 0:00:01.089664 seconds\n",
            "Starting extraction to data at 2025-08-06 22:03:35.119427\n",
            "Extraction finished at 2025-08-06 22:03:35.282463 taking 0:00:00.163036 seconds\n",
            "Total time taken: 0:00:01.252700 seconds\n"
          ]
        }
      ],
      "source": [
        "url = \"https://github.com/LNB-DH/BSSDH_2025_workshop_data/raw/main/data/Latvian_Economic_Review_1936_1940.zip\"\n",
        "print(\"Will extract data from\", url)\n",
        "# next we define a function that will download and extract the zip file, this way we can reuse it later if needed\n",
        "# we can also set some default values for the arguments, so we do not have to specify\n",
        "# default values always come after the mandatory arguments in Python functions\n",
        "def extract_zip(url, output_dir=\"data\", verbose=False):\n",
        "    # we could have imported these at the top, but we want to keep the script self-contained\n",
        "    import requests  # this should be cached by notebooks, so it **should** not require importing it every time\n",
        "    from zipfile import ZipFile\n",
        "    from io import BytesIO\n",
        "\n",
        "    # In verbose mode let's some extra information about the download and extraction process\n",
        "    # This is useful for debugging and understanding the flow of the script\n",
        "    from datetime import datetime\n",
        "    if verbose:\n",
        "        download_start = datetime.now()\n",
        "        # we print start time including milliseconds\n",
        "        print(f\"Starting download at {download_start.strftime('%Y-%m-%d %H:%M:%S.%f')}\")\n",
        "        print(\"Starting download from\", url)\n",
        "    response = requests.get(url)\n",
        "    if verbose:\n",
        "        download_finish = datetime.now()\n",
        "        print(f\"Download finished at {download_finish.strftime('%Y-%m-%d %H:%M:%S.%f')} taking {download_finish - download_start} seconds\")\n",
        "    if response.status_code == 200: # it is possible a request fails, e.g. if the URL is incorrect\n",
        "        if verbose:\n",
        "            extract_start = datetime.now()\n",
        "            print(f\"Starting extraction to {output_dir} at {extract_start.strftime('%Y-%m-%d %H:%M:%S.%f')}\")\n",
        "        with ZipFile(BytesIO(response.content)) as zf:\n",
        "            zf.extractall(output_dir)\n",
        "        if verbose:\n",
        "            extract_end = datetime.now()\n",
        "            print(f\"Extraction finished at {extract_end.strftime('%Y-%m-%d %H:%M:%S.%f')} taking {extract_end - extract_start} seconds\")\n",
        "            print(f\"Total time taken: {extract_end - download_start} seconds\")\n",
        "    else:\n",
        "        print(\"Failed to download data:\", response.status_code)\n",
        "\n",
        "# now that we have our function defined, we can call it immediately\n",
        "# note we do not supply all arguments, first one is mandatory, the rest are optional\n",
        "# so we skip over output_dir in this case\n",
        "extract_zip(url, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82bc187d",
      "metadata": {
        "id": "82bc187d"
      },
      "source": [
        "### Getting information about extracted files\n",
        "\n",
        "It is a good practice to double check what files were extracted and where they are located. We can do this by listing the files in the directory where we extracted them. We can use Python pathlib to do this.\n",
        "The goal is to double check that what we extacted matches what we expected. We can also check the file names and their structure to make sure they are correct.\n",
        "\n",
        "```python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dd0d77bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd0d77bf",
        "outputId": "4d9fa301-0e1e-487a-c746-79c72a5d23bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted 3 files to data\n",
            "Latvian_Economic_Review\n",
            "RigascheZeitung_samples\n",
            "Rigasche_Zeitung_1918_1919\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "extract_dir = Path(\"data\") # note this is a a relative path, relative to the current working directory for the notebook\n",
        "# let's check if the directory exists and how many files it contains\n",
        "if extract_dir.exists():\n",
        "    files = list(extract_dir.glob(\"*\"))  # this will list all files in the directory\n",
        "    print(f\"Extracted {len(files)} files to {extract_dir}\")\n",
        "    for file in files:\n",
        "        print(file.name)  # print the name of each file\n",
        "else:\n",
        "    print(f\"Directory {extract_dir} does not exist. Please check the extraction process.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae7ade9a",
      "metadata": {
        "id": "ae7ade9a"
      },
      "source": [
        "### Getting information about subfolders in the extracted directory\n",
        "Looks like we only have a single file but it is actually not a file but a directory. This is because we extracted a zip file that contains files under a single directory.\n",
        "\n",
        "Next we want to check how many total files we have and also how many files we have with *.txt extension. This will help us to understand how many files we can work with and if there are any files that we might want to exclude from our analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "55d05596",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55d05596",
        "outputId": "492fbede-4666-4a44-b3a6-16e9676cccd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing extracted data directory...\n",
            "📁 Directory Analysis: data\n",
            "==================================================\n",
            "Total items found (files + directories): 5087\n",
            "Total files: 5078\n",
            "Total directories: 9\n",
            "Text files (.txt): 5054\n",
            "\n",
            "📊 File Extensions Summary:\n",
            "------------------------------\n",
            "  .txt: 5054 files\n",
            "  .jpg: 21 files\n",
            "  .db: 3 files\n",
            "\n",
            "📂 Directory Structure:\n",
            "------------------------------\n",
            "  📁 Latvian_Economic_Review\n",
            "  📁 Rigasche_Zeitung_1918_1919\n",
            "  📁 RigascheZeitung_samples\n",
            "  📁 RigascheZeitung_samples\\1918\n",
            "  📁 RigascheZeitung_samples\\1918\\rzei1918s01n062\n",
            "  📁 RigascheZeitung_samples\\1918\\rzei1918s01n248\n",
            "  📁 RigascheZeitung_samples\\1919\n",
            "  📁 RigascheZeitung_samples\\1919\\rzei1919s01n01\n",
            "  📁 RigascheZeitung_samples\\1919\\rzei1919s01n52\n",
            "\n",
            "📄 First 10 .txt files (out of 5054 total):\n",
            "------------------------------\n",
            "  📄 Latvian_Economic_Review\\lerq1936s01n01_003_plaintext_s01.txt (8,662 bytes)\n",
            "  📄 Latvian_Economic_Review\\lerq1936s01n01_006_plaintext_s02.txt (5,190 bytes)\n",
            "  📄 Latvian_Economic_Review\\lerq1936s01n01_008_plaintext_s03.txt (6,066 bytes)\n",
            "  📄 Latvian_Economic_Review\\lerq1936s01n01_009_plaintext_s04.txt (5,523 bytes)\n",
            "  📄 Latvian_Economic_Review\\lerq1936s01n01_013_plaintext_s05.txt (3,866 bytes)\n",
            "  📄 Latvian_Economic_Review\\lerq1936s01n01_014_plaintext_s06.txt (1,144 bytes)\n",
            "  📄 Latvian_Economic_Review\\lerq1936s01n01_014_plaintext_s07.txt (629 bytes)\n",
            "  📄 Latvian_Economic_Review\\lerq1936s01n01_015_plaintext_s08.txt (7,520 bytes)\n",
            "  📄 Latvian_Economic_Review\\lerq1936s01n01_017_plaintext_s09.txt (951 bytes)\n",
            "  📄 Latvian_Economic_Review\\lerq1936s01n01_018_plaintext_s10.txt (7,625 bytes)\n",
            "  ... and 5044 more .txt files\n"
          ]
        }
      ],
      "source": [
        "# let's check how many files total we have and how many files with *.txt extension counting all subfolders\n",
        "# this means we will perform a recursive search for all files in the directory\n",
        "\n",
        "def analyze_directory_contents(directory_path, verbose=True):\n",
        "    \"\"\"\n",
        "    Analyze the contents of a directory recursively and provide detailed information.\n",
        "\n",
        "    Args:\n",
        "        directory_path: Path object or string path to the directory\n",
        "        verbose: If True, print detailed information about file types and structure\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing analysis results\n",
        "    \"\"\"\n",
        "    from pathlib import Path\n",
        "\n",
        "    directory = Path(directory_path)\n",
        "\n",
        "    if not directory.exists():\n",
        "        print(f\"Directory {directory} does not exist.\")\n",
        "        return None\n",
        "\n",
        "    # Get all files recursively\n",
        "    all_files = list(directory.rglob(\"*\"))\n",
        "\n",
        "    # Separate files from directories\n",
        "    files_only = [f for f in all_files if f.is_file()]\n",
        "    directories_only = [f for f in all_files if f.is_dir()]\n",
        "\n",
        "    # Count files by extension\n",
        "    file_extensions = {}\n",
        "    for file in files_only:\n",
        "        ext = file.suffix.lower()\n",
        "        if ext == '':\n",
        "            ext = '(no extension)'\n",
        "        file_extensions[ext] = file_extensions.get(ext, 0) + 1\n",
        "\n",
        "    # Count .txt files specifically\n",
        "    txt_files = [f for f in files_only if f.suffix.lower() == '.txt']\n",
        "\n",
        "    # Analysis results\n",
        "    results = {\n",
        "        'total_items': len(all_files),\n",
        "        'total_files': len(files_only),\n",
        "        'total_directories': len(directories_only),\n",
        "        'txt_files': len(txt_files),\n",
        "        'file_extensions': file_extensions,\n",
        "        'txt_file_paths': txt_files\n",
        "    }\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"📁 Directory Analysis: {directory}\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Total items found (files + directories): {results['total_items']}\")\n",
        "        print(f\"Total files: {results['total_files']}\")\n",
        "        print(f\"Total directories: {results['total_directories']}\")\n",
        "        print(f\"Text files (.txt): {results['txt_files']}\")\n",
        "        print()\n",
        "\n",
        "        print(\"📊 File Extensions Summary:\")\n",
        "        print(\"-\" * 30)\n",
        "        for ext, count in sorted(file_extensions.items(), key=lambda x: x[1], reverse=True):\n",
        "            print(f\"  {ext}: {count} files\")\n",
        "        print()\n",
        "\n",
        "        if len(directories_only) > 0:\n",
        "            print(\"📂 Directory Structure:\")\n",
        "            print(\"-\" * 30)\n",
        "            for directory in sorted(directories_only):\n",
        "                # Show relative path from the base directory\n",
        "                relative_path = directory.relative_to(directory_path)\n",
        "                print(f\"  📁 {relative_path}\")\n",
        "            print()\n",
        "\n",
        "        if len(txt_files) > 0 and len(txt_files) <= 10:\n",
        "            print(\"📄 Sample .txt files:\")\n",
        "            print(\"-\" * 30)\n",
        "            for txt_file in sorted(txt_files)[:10]:\n",
        "                relative_path = txt_file.relative_to(directory_path)\n",
        "                file_size = txt_file.stat().st_size\n",
        "                print(f\"  📄 {relative_path} ({file_size:,} bytes)\")\n",
        "        elif len(txt_files) > 10:\n",
        "            print(f\"📄 First 10 .txt files (out of {len(txt_files)} total):\")\n",
        "            print(\"-\" * 30)\n",
        "            for txt_file in sorted(txt_files)[:10]:\n",
        "                relative_path = txt_file.relative_to(directory_path)\n",
        "                file_size = txt_file.stat().st_size\n",
        "                print(f\"  📄 {relative_path} ({file_size:,} bytes)\")\n",
        "            print(f\"  ... and {len(txt_files) - 10} more .txt files\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Now let's analyze our extracted directory\n",
        "print(\"Analyzing extracted data directory...\")\n",
        "analysis_results = analyze_directory_contents(extract_dir, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9e283717",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e283717",
        "outputId": "6a9bbeb6-a8b2-414a-9907-f3ab6d216f3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6th text file: lerq1936s01n01_014_plaintext_s06.txt (1,144 bytes)\n"
          ]
        }
      ],
      "source": [
        "# let's get 6th text file from analysis_results dictionary text_files_paths key\n",
        "# why 6th? because it seems a bit smaller\n",
        "text_files_list = sorted(analysis_results['txt_file_paths'])\n",
        "if len(text_files_list) >= 6:\n",
        "    sixth_text_file = text_files_list[5]  # 6th file, index starts from 0\n",
        "    print(f\"6th text file: {sixth_text_file.name} ({sixth_text_file.stat().st_size:,} bytes)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e18da885",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e18da885",
        "outputId": "6c10eeef-59aa-4a9d-f3c6-1b163e5424b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents of lerq1936s01n01_014_plaintext_s06.txt:\n",
            "\n",
            "title: Gypsum\n",
            "author: \n",
            "uri: http://dom.lndb.lv/data/obj/159411\n",
            "\n",
            "\n",
            "\n",
            "There are numerous extensive layers of gypseous\n",
            "stone in Latvia, but only a few of them are being\n",
            "exploited, viz., the quarries at Kalnciems, Sloka\n",
            "(about 33 km. from Riga), Salaspils (about 20 km.\n",
            "from Riga) and Naves sala (about 25 km. from Riga).\n",
            "The gypseous stone is exported both in raw condition\n",
            "(gypsum), principally for the manufacture of\n",
            "cement, and in the form of Plaster of Paris. The export\n",
            "of gypsum totalled 69,000 tons\n",
            "\n",
            "... (truncated output)\n"
          ]
        }
      ],
      "source": [
        "# let's print out its contents\n",
        "with sixth_text_file.open('r', encoding='utf-8') as f:\n",
        "    content = f.read() # read whole file content into memory\n",
        "# file is closed here automatically due to the with statement\n",
        "print(f\"Contents of {sixth_text_file.name}:\\n\")\n",
        "print(content[:500])  # print first 500 characters to avoid too much output\n",
        "print(\"\\n... (truncated output)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "449270d3",
      "metadata": {
        "id": "449270d3"
      },
      "source": [
        "## Setting up our LLM API functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ee5d53aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee5d53aa",
        "outputId": "f05765e1-e8d7-47d1-a98f-9d24714a0a70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Open Router API key to .env file...\n",
            "Open Router API key saved to .env file.\n",
            "OpenRouter API key loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "#  we will prompt user to enter it manually if not found\n",
        "import getpass\n",
        "\n",
        "import os # we already imported this, but let's do it again for clarity - it is cached so no harm done\n",
        "\n",
        "\n",
        "# if still not found, prompt user to enter it manually\n",
        "if 'open_router_api_key' not in locals() or not open_router_api_key:\n",
        "    open_router_api_key = getpass.getpass(\"Please enter your OpenRouter API key: \")\n",
        "    # save it to .env file for future use\n",
        "    # note Google Colab will destroy .env file after session ends, so you will need to enter it again next time\n",
        "    # this can be useful if you re-run the notebook and want to avoid entering the key again\n",
        "    print(\"Saving Open Router API key to .env file...\")\n",
        "    with open('.env', 'a') as f:\n",
        "        f.write(f'OPENROUTER_API_KEY={open_router_api_key}\\n')\n",
        "    print(\"Open Router API key saved to .env file.\")\n",
        "\n",
        "# we now should have the OpenRouter API key available\n",
        "if open_router_api_key:\n",
        "    print(\"OpenRouter API key loaded successfully.\")\n",
        "else:\n",
        "    print(\"OpenRouter API key not found. Please make sure you have it set in your environment variables or .env file.\")\n",
        "    print(\"You can also enter it manually when prompted during API calls.\")\n",
        "\n",
        "# key point we do not print it publicly it is stored as a variable under the name open_router_api_key - of course you can change the name to something more descriptive\n",
        "# but do not print it to the console or logs, as it is sensitive information"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71574a1b",
      "metadata": {
        "id": "71574a1b"
      },
      "source": [
        "### About the model - Google: Gemini 2.5 Flash Lite\n",
        "\n",
        "Unlike previous session where we used OpenAI API, this session will focus on using [Google Gemini 2.5 Flash Lite model](https://openrouter.ai/google/gemini-2.5-flash-lite) via OpenRouter API. Again we could have used many different models, but we will use this one as it is lightweight and fast and **INEXPENSIVE**, which is ideal for our use case.\n",
        "\n",
        "**Model ID:** `google/gemini-2.5-flash-lite`  \n",
        "**Created:** July 22, 2025  \n",
        "**Context Window:** 1,048,576 tokens  \n",
        "**Pricing:**  \n",
        "- **Input tokens:** $0.10 per million  \n",
        "- **Output tokens:** $0.40 per million  \n",
        "\n",
        "**Tags:**  \n",
        "- Legal (#4)  \n",
        "- Marketing/SEO (#4)  \n",
        "- Translation (#9)  \n",
        "\n",
        "**Description:**  \n",
        "Gemini 2.5 Flash Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for **ultra-low latency** and **cost efficiency**. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models.\n",
        "\n",
        "By default, \"thinking\" (i.e., multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the **Reasoning API** parameter to selectively trade off cost for intelligence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "38b5ca4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38b5ca4d",
        "outputId": "183da7d6-1faf-4817-d4ea-e2d66e0fb49a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response from OpenRouter API:\n",
            "Here are the named entities identified in the text:\n",
            "\n",
            "*   Kalnciems\n",
            "*   Sloka\n",
            "*   Riga\n",
            "*   Salaspils\n",
            "*   Naves sala\n",
            "*   Plaster of Paris\n",
            "*   Norway\n",
            "*   Sweden\n",
            "*   Denmark\n",
            "*   Finland\n",
            "*   England\n"
          ]
        }
      ],
      "source": [
        "# let's define a generic function for OpenRouter API requests\n",
        "# it should have tshould define a new function get_openrouter_response it should have following parameters system_prompt, user_prompt,\n",
        "#  model defaulting to ChatGPT 3.5 and finally api_key which defaults to open_router_api_key .\n",
        "#  The function get_openrouter_response should function just like analyze_latvian_text_with_openrouter except with parameters.\n",
        "import requests  # we need to import requests to make API calls\n",
        "\n",
        "def get_openrouter_response(system_prompt, user_prompt,\n",
        "                            model=\"google/gemini-2.5-flash-lite\",\n",
        "                            api_key=open_router_api_key,\n",
        "                            max_tokens=4096,\n",
        "                            temperature=0.5,):\n",
        "    \"\"\"\n",
        "    Generic function to make requests to OpenRouter API with specified parameters.\n",
        "\n",
        "    :param system_prompt: The system prompt to guide the model's behavior.\n",
        "    :param user_prompt: The user query or text to analyze.\n",
        "    :param model: The model to use for the request (default is GPT-3.5).\n",
        "    :param api_key: The OpenRouter API key (default is loaded from environment).\n",
        "    :return: The response from the OpenRouter API.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set up the API endpoint and headers\n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": \"https://www.digitalhumanities.lv/bssdh/2025/\",  # Your project URL\n",
        "        \"X-Title\": \"BSSDH 2025 LLM Workshop - Generic OpenRouter Request\"\n",
        "    }\n",
        "\n",
        "    # Create the request payload\n",
        "    request_data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        \"max_tokens\": max_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": 0.9\n",
        "    }\n",
        "\n",
        "    # Make the API request\n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, json=request_data, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        result = response.json()\n",
        "\n",
        "        if 'choices' in result and len(result['choices']) > 0:\n",
        "            return result['choices'][0]['message']['content']\n",
        "\n",
        "        else:\n",
        "            print(\"❌ Error: No response returned from the API\")\n",
        "            return None\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"❌ Request Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# let's test it on simple Meaning of Life question\n",
        "\n",
        "system_prompt = \"You are a digital humanities researcher specializing in named entity recognition and text analysis. You will analyze the text and provide all named entities as a list.\"\n",
        "user_prompt = content\n",
        "\n",
        "response = get_openrouter_response(system_prompt, user_prompt) # note we did not pass model or api_key, so it will use defaults of \"openai/gpt-3.5-turbo\" and open_router_api_key\n",
        "\n",
        "if response:\n",
        "    print(\"Response from OpenRouter API:\")\n",
        "    print(response)\n",
        "else:\n",
        "    print(\"Failed to get a response from OpenRouter API.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "489a99af",
      "metadata": {
        "id": "489a99af"
      },
      "source": [
        "## Adjusting system prompts\n",
        "\n",
        "We got our named entities but let's also have categories for them.\n",
        "We can do that by adjusting our system prompt to include categories for named entities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "223485c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "223485c6",
        "outputId": "40bd4f1b-da2e-41fc-e758-2be7abfdcae2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjusted system prompt: You are a digital humanities researcher specializing in named entity recognition and text analysis. You will analyze the text and provide all named entities as a list. Please categorize the named entities into PERSON, ORGANIZATION, LOCATION, and MISC.\n"
          ]
        }
      ],
      "source": [
        "# let's adjust our system prompt by adding extra instruction to add categories for named entities.\n",
        "extra_instruction = \"Please categorize the named entities into PERSON, ORGANIZATION, LOCATION, and MISC.\"\n",
        "system_prompt = f\"{system_prompt} {extra_instruction}\"\n",
        "print(f\"Adjusted system prompt: {system_prompt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cfdbae4b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfdbae4b",
        "outputId": "843ddc29-347e-4cc6-cb91-40050087337e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response with adjusted system prompt:\n",
            "Here are the named entities from the text, categorized as requested:\n",
            "\n",
            "**PERSON:**\n",
            "* None\n",
            "\n",
            "**ORGANIZATION:**\n",
            "* None\n",
            "\n",
            "**LOCATION:**\n",
            "* Latvia\n",
            "* Kalnciems\n",
            "* Sloka\n",
            "* Riga\n",
            "* Naves sala\n",
            "* Norway\n",
            "* Sweden\n",
            "* Denmark\n",
            "* Finland\n",
            "* England\n",
            "\n",
            "**MISC:**\n",
            "* Gypsum\n",
            "* Plaster of Paris\n"
          ]
        }
      ],
      "source": [
        "# let's see our response with adjusted system prompt\n",
        "response = get_openrouter_response(system_prompt, user_prompt)\n",
        "if response:\n",
        "    print(\"Response with adjusted system prompt:\")\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6369b81d",
      "metadata": {
        "id": "6369b81d"
      },
      "source": [
        "## Getting a summary of the text\n",
        "\n",
        "One of the basic tasks we can do with LLMs is to get a summary of the text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "24312220",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24312220",
        "outputId": "999766ed-c4d1-4716-ddeb-838067dc77cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary of the text:\n",
            "Here's a summary of the provided text about gypsum:\n",
            "\n",
            "*   **Gypsum Deposits:** Latvia possesses extensive layers of gypseous stone, with active exploitation occurring in quarries at Kalnciems, Sloka, Salaspils, and Naves sala.\n",
            "*   **Export of Raw Gypsum:**\n",
            "    *   Raw gypsum is primarily exported for cement manufacturing.\n",
            "    *   In 1934, 69,000 tons were exported, generating Ls 395,000.\n",
            "    *   Exports were even higher in the subsequent year.\n",
            "    *   Destinations include Norway, Sweden, Denmark, Finland, and England.\n",
            "    *   Latvian gypsum is valued abroad for its firm structure, which prevents machinery clogging during milling.\n",
            "    *   Purity averages 93%, with some layers yielding up to 99% pure gypsum.\n",
            "*   **Export of Plaster of Paris:**\n",
            "    *   In 1934, approximately 6,500 tons of Plaster of Paris were exported, earning Ls 85,000.\n",
            "    *   A 100% increase in Plaster of Paris exports was anticipated for the following year.\n",
            "    *   The majority of Plaster of Paris is sent to England for use in manufacturing gypsum plates and other products.\n"
          ]
        }
      ],
      "source": [
        "summary_system_prompt = \"\"\"You are a digital humanities researcher specializing in text summarization.\n",
        "You will summarize the text and provide a concise summary in structured bullet point format.\"\"\"\n",
        "response = get_openrouter_response(summary_system_prompt, content)\n",
        "if response:\n",
        "    print(\"Summary of the text:\")\n",
        "    print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c925855f",
      "metadata": {
        "id": "c925855f"
      },
      "source": [
        "## Concept mining\n",
        "\n",
        "For those are unfamiliar with the term, concept mining is a process of extracting key concepts from a text.\n",
        "\n",
        "Concept mining involves automatically identifying and extracting abstract ideas or key concepts from large amounts of unstructured text data. It goes beyond surface-level keyword extraction by attempting to detect the underlying themes, topics, or conceptual entities present in the text -- including those that might not be explicitly named but are implied or paraphrased.\n",
        "\n",
        "Example would be \"the rise of the internet\" instead of just \"internet\". Instead of \"potatoes\" we could extract \"agricultural products\" or \"food production\". Something like \"food anxiety\" instead of just \"food\".\n",
        "\n",
        "There are some older non LLM techniques such as LDA (Latent Dirichlet Allocation) - that can be used for concept mining, which the instructors have used in previous work. See - [Baklane, Anda., Saulespurens, Valdis “The Application of Latent Dirichlet Allocation for the Analysis of Latvian Historical Newspapers: Oskars Kalpaks’ Case Study.” Nauka, Tehnologìï, Ìnnovacìï, State Scientific Institution - Ukrainian Institute of Scientific and Technical Expertise and Info, 2022.](https://www.academia.edu/112360712/The_application_of_latent_Dirichlet_allocation_for_the_analysis_of_latvian_historical_newspapers_Oskars_Kalpaks_case_study)\n",
        "\n",
        "However LLMs can provide more nuanced and context-aware results compared to LDA, especially in understanding the subtleties of language and the relationships between concepts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "562cef48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "562cef48",
        "outputId": "1628feaf-fea7-4d0b-dc75-6896928e3508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Concepts extracted from the text:\n",
            "Here's a concept mining analysis of the provided text about Gypsum:\n",
            "\n",
            "**Theme: Gypsum Resources and Exploitation**\n",
            "\n",
            "*   **Resource Presence:**\n",
            "    *   Extensive layers of gypseous stone in Latvia\n",
            "*   **Exploited Quarries:**\n",
            "    *   Kalnciems\n",
            "    *   Sloka (location: ~33 km from Riga)\n",
            "    *   Salaspils (location: ~20 km from Riga)\n",
            "    *   Naves sala (location: ~25 km from Riga)\n",
            "\n",
            "**Theme: Gypsum Products and Applications**\n",
            "\n",
            "*   **Raw Gypsum:**\n",
            "    *   Use: Manufacture of cement\n",
            "*   **Processed Gypsum:**\n",
            "    *   Product: Plaster of Paris\n",
            "    *   Use: Making gypsum plates and other articles\n",
            "\n",
            "**Theme: Gypsum Trade and Economics**\n",
            "\n",
            "*   **Export Destinations:**\n",
            "    *   Norway\n",
            "    *   Sweden\n",
            "    *   Denmark\n",
            "    *   Finland\n",
            "    *   England (primary destination for Plaster of Paris)\n",
            "*   **Export Volumes and Value (1934):**\n",
            "    *   Raw Gypsum: 69,000 tons, Ls 395,000\n",
            "    *   Plaster of Paris: ~6,500 tons, Ls 85,000\n",
            "*   **Export Trends:**\n",
            "    *   Raw gypsum export larger in the past year (prior to 1934)\n",
            "    *   100% increase expected for Plaster of Paris export in the current year (after 1934)\n",
            "\n",
            "**Theme: Gypsum Quality and Characteristics**\n",
            "\n",
            "*   **Purity:**\n",
            "    *   Average purity: 93%\n",
            "    *   Maximum purity: up to 99%\n",
            "*   **Physical Properties:**\n",
            "    *   Firm structure\n",
            "    *   Facilitates milling process\n",
            "    *   Does not clog machinery\n",
            "*   **Market Perception:**\n",
            "    *   \"Very much liked abroad\" due to its structure\n"
          ]
        }
      ],
      "source": [
        "# let's make a system prompt for concept mining\n",
        "system_prompt = \"\"\"You are a digital humanities researcher specializing in concept mining.\n",
        "You will analyze the text and extract key concepts, themes, and patterns as a structured list.\n",
        "Please provide the concepts in a clear and concise manner, categorizing them into relevant themes.\"\"\"\n",
        "response = get_openrouter_response(system_prompt, content)\n",
        "if response:\n",
        "    print(\"Concepts extracted from the text:\")\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f6b6f67",
      "metadata": {
        "id": "0f6b6f67"
      },
      "source": [
        "## 🧠 Concept Categories for 1930s Latvian Economic Reports\n",
        "\n",
        "Above LLM output gave us structured but still flexible list of concepts. Usually we want to focus on a specific set of concepts that are relevant to our research. Below is a list of categories that we can use to organize the concepts extracted from the 1930s Latvian Economic Reports.\n",
        "\n",
        "### 🏦 Finance & Banking\n",
        "- Financial stability  \n",
        "- Monetary policy  \n",
        "- Currency regulation  \n",
        "- Exchange rates  \n",
        "- Foreign investment  \n",
        "- Credit availability  \n",
        "- Central banking  \n",
        "- Gold reserves  \n",
        "\n",
        "### 📈 Trade & Commerce\n",
        "- Trade liberalization  \n",
        "- Export promotion  \n",
        "- Import substitution  \n",
        "- Customs tariffs  \n",
        "- Balance of trade  \n",
        "- Foreign trade agreements  \n",
        "- Trade protectionism  \n",
        "- Trade surplus/deficit  \n",
        "- Transit trade  \n",
        "- Port activity (e.g., Riga, Liepāja)  \n",
        "\n",
        "### 🚜 Agriculture & Rural Economy\n",
        "- Agricultural modernization  \n",
        "- Land reform  \n",
        "- Crop yields  \n",
        "- Agricultural exports  \n",
        "- Collective farming (if present)  \n",
        "- Peasant cooperatives  \n",
        "- Grain storage and reserves  \n",
        "- Rural credit  \n",
        "- Mechanization of agriculture  \n",
        "- Dairy and livestock production  \n",
        "\n",
        "### 🏭 Industry & Infrastructure\n",
        "- Industrialization  \n",
        "- Manufacturing output  \n",
        "- Raw material imports  \n",
        "- Industrial policy  \n",
        "- State-owned enterprises  \n",
        "- Energy production (electricity, fuel)  \n",
        "- Infrastructure investment  \n",
        "- Transportation development  \n",
        "- Railway modernization  \n",
        "- Telecommunications expansion  \n",
        "\n",
        "### 👷 Labor & Employment\n",
        "- Unemployment  \n",
        "- Labor migration  \n",
        "- Wages and cost of living  \n",
        "- Labor policy  \n",
        "- Social insurance  \n",
        "- Vocational training  \n",
        "- Workforce productivity  \n",
        "- State employment programs  \n",
        "\n",
        "### 💰 Prices & Markets\n",
        "- Price stability  \n",
        "- Inflation control  \n",
        "- Consumer goods availability  \n",
        "- Market regulation  \n",
        "- Food pricing  \n",
        "- Speculation control  \n",
        "\n",
        "### 🏛️ Economic Policy & Governance\n",
        "- Five-year plans (if applicable)  \n",
        "- Corporatism  \n",
        "- Authoritarian economic planning  \n",
        "- State intervention  \n",
        "- Public-private partnerships  \n",
        "- Fiscal policy  \n",
        "- Budget deficit  \n",
        "- Taxation policy  \n",
        "- Economic nationalism  \n",
        "\n",
        "### 🌍 International Relations & Geopolitics\n",
        "- Trade with Germany, USSR, UK, Sweden, etc.  \n",
        "- Regional trade blocs (e.g., Baltic Entente)  \n",
        "- Neutrality in foreign policy  \n",
        "- Geopolitical pressures on trade  \n",
        "- Sanctions or economic treaties  \n",
        "\n",
        "### 🧑‍🏫 Education & Human Capital\n",
        "- Economic education  \n",
        "- Technical schools  \n",
        "- Business training  \n",
        "- Scientific research for industry  \n",
        "- Demographic skills gap  \n",
        "\n",
        "### 🧱 Development & Urbanization\n",
        "- Urban planning  \n",
        "- Rural-urban migration  \n",
        "- Public works  \n",
        "- Housing policy  \n",
        "- Regional economic disparity  \n",
        "- Municipal finance  \n",
        "\n",
        "---\n",
        "\n",
        "### 🎯 High-Level & Cross-Cutting Concepts\n",
        "- Economic self-sufficiency  \n",
        "- National economic strategy  \n",
        "- Resilience to global crisis  \n",
        "- Preparation for war economy  \n",
        "- Currency sovereignty  \n",
        "- Export dependency  \n",
        "- Shadow economy (if mentioned)  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a4c806e",
      "metadata": {
        "id": "0a4c806e"
      },
      "source": [
        "## Focusing on specific concepts - agriculture\n",
        "\n",
        "For this workshop let's focus on the agriculture category. We will extract concepts related to agriculture from the 1930s Latvian Economic Reports and analyze them in more detail.\n",
        "\n",
        "### Making a larger system prompt\n",
        "\n",
        "Our system prompt will be a bit more complex as we want to mention the specific category we are interested in. Also we want to provide example of format that we want the LLM to return so our analysis will be more structured and easier to work with.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7abe9ea5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7abe9ea5",
        "outputId": "1f67c3f4-e27c-4443-907b-776f96579d62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agriculture-related concepts extracted from the text:\n",
            "```json\n",
            "[\n",
            "  {\n",
            "    \"phrase\": \"gypseous stone is exported both in raw condition (gypsum), principally for the manufacture of cement, and in the form of Plaster of Paris\",\n",
            "    \"explanation\": \"This describes the export of raw gypsum and processed gypsum (Plaster of Paris) for industrial use, indicating a component of Latvia's trade in mineral resources.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"The export of gypsum totalled 69,000 tons in 1934 and rendered a sum of Ls 395,000\",\n",
            "    \"explanation\": \"This quantifies the volume and value of gypsum exports in a specific year, highlighting its significance in Latvia's trade balance.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"Latvian gypsum is very much liked abroad because of its firm structure which facilitates the milling process without clogging the machinery\",\n",
            "    \"explanation\": \"This highlights a qualitative advantage of Latvian gypsum that makes it desirable for export, suggesting a focus on product quality for international markets.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"The export of Plaster of Paris amounted to about 6,500 tons in 1934, which rendered Ls 85,000\",\n",
            "    \"explanation\": \"This provides data on the export of a processed gypsum product, indicating diversification in export commodities.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  }\n",
            "]\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "agriculture_system_prompt = \"\"\"You are an expert assistant trained to analyze historical economic texts from 1930s Latvia.\n",
        "Your task is to read the input document or paragraph and extract any **concepts** that relate specifically to the domain of **Agriculture & Rural Economy**.\n",
        "Focus only on concepts connected to the following subcategories:\n",
        "- Agricultural modernization\n",
        "- Land reform\n",
        "- Crop yields\n",
        "- Agricultural exports\n",
        "- Collective farming (if present)\n",
        "- Peasant cooperatives\n",
        "- Grain storage and reserves\n",
        "- Rural credit\n",
        "- Mechanization of agriculture\n",
        "- Dairy and livestock production\n",
        "- Other agricultural practices\n",
        "\n",
        "For each concept you extract, return:\n",
        "- The **exact phrase** or **paraphrased expression** found in the text\n",
        "- A **brief explanation** (1–2 sentences) summarizing the concept’s meaning or context\n",
        "- The most relevant **subcategory** from the list above that it matches\n",
        "\n",
        "Only return relevant agricultural or rural economy concepts. If no relevant concept is found, return an empty list.\n",
        "\n",
        "Use the following output format (in JSON):\n",
        "\n",
        "[\n",
        "  {\n",
        "    \"phrase\": \"introduction of American tractors\",\n",
        "    \"explanation\": \"Refers to adoption of mechanized equipment to improve farming productivity.\",\n",
        "    \"subcategory\": \"Mechanization of agriculture\"\n",
        "  },\n",
        "  {\n",
        "    \"phrase\": \"state-supported grain warehouses\",\n",
        "    \"explanation\": \"Refers to government involvement in securing grain storage for future needs or trade.\",\n",
        "    \"subcategory\": \"Grain storage and reserves\"\n",
        "  }\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "response = get_openrouter_response(agriculture_system_prompt, content)\n",
        "if response:\n",
        "    print(\"Agriculture-related concepts extracted from the text:\")\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f9fc6f",
      "metadata": {
        "id": "f6f9fc6f"
      },
      "source": [
        "## Fine-tuning the system prompt and adjusting model\n",
        "\n",
        "The LLM output is well structured and concepts are extracted with one MAJOR issue - semantically gypsum and plaster of Paris are not agricultural products!\n",
        "\n",
        " We can fix this by adjusting our system prompt to be more specific about the concepts we want to extract.\n",
        "\n",
        " Also we can adjust the model or its parameters to be more focused on the specific task we are trying to achieve. For example, we can increase the maximum number of tokens to allow for more detailed responses or adjust the temperature to make the model more conservative in its responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "45748e85",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45748e85",
        "outputId": "fb6c40c7-6767-4db8-a5bf-f6d8eb655e83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agriculture-related concepts extracted from the text using Gemini 2.5 Pro:\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "### let's try a different model first - let's try the best (and most expensive) current Gemini model as of mid 2025 - 2.5 Pro\n",
        "\n",
        "response = get_openrouter_response(agriculture_system_prompt, content, model=\"google/gemini-2.5-pro\")\n",
        "if response:\n",
        "    print(\"Agriculture-related concepts extracted from the text using Gemini 2.5 Pro:\")\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd88c0da",
      "metadata": {
        "id": "fd88c0da"
      },
      "source": [
        "### Trying other models - finding middle ground between speed and accuracy\n",
        "\n",
        "Pro model provided accurate answer namely that there are no agricultural products in the text. However, it is not very fast and it is not very cheap to use. The above query cost us about 0.01 USD - that is 1 cent which might not seem like much, but if we want to analyze a large corpus of documents, the costs can add up quickly.\n",
        "\n",
        "So let's see if we can find a model that is faster and cheaper but still provides accurate results. We will try different models and see how they perform on our task.\n",
        "\n",
        "Let's try the `golden mean` as of today [Google Gemini 2.5 Flash](https://openrouter.ai/google/gemini-2.5-flash)\n",
        "\n",
        "To do so we simply provide the model name in the `get_openrouter_response` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c793322d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c793322d",
        "outputId": "5b118a0c-8bd8-4cd0-8a27-27634d2f063b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agriculture-related concepts extracted from the text using Gemini 2.5 Flash:\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "# let's try with google/gemini-2.5-flash model\n",
        "response = get_openrouter_response(agriculture_system_prompt, content, model=\"google/gemini-2.5-flash\")\n",
        "if response:\n",
        "    print(\"Agriculture-related concepts extracted from the text using Gemini 2.5 Flash:\")\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bc6539a",
      "metadata": {
        "id": "2bc6539a"
      },
      "source": [
        "## Running model across multiple documents - our corpus\n",
        "\n",
        "The response looks good (and 10x cheaper that Gemini 2.5 Pro due mostly to limiting reasoning outputs which we do not need for concept mining task), however the question remains, what will happen when we run the model across multiple documents? Will have have many false positives or will the model be able to filter out the noise and provide us with relevant concepts?\n",
        "\n",
        "Now that we have gotten our results from a single document we can run the model across multiple documents in our corpus. We will iterate over the files in the directory where we extracted the LERQ corpus and apply the same system prompt to each file.\n",
        "\n",
        "Usually I like to run my prompts across a smaller sample than full corpus. So let's pick 30 documents at random from the corpus and see how the model performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f395f621",
      "metadata": {
        "id": "f395f621"
      },
      "outputs": [],
      "source": [
        "# let's pick random 30 files from the corpus and run the model across them\n",
        "def run_model_across_corpus(directory_path, system_prompt,\n",
        "                            model=\"google/gemini-2.5-flash\",\n",
        "                            sample_size=30,\n",
        "                            seed=2025,\n",
        "                            verbose=True,\n",
        "                            delay=0.1):\n",
        "    \"\"\"\n",
        "    Run the model across a sample of files in the specified directory.\n",
        "\n",
        "    :param directory_path: Path to the directory containing text files.\n",
        "    :param system_prompt: The system prompt to guide the model's behavior.\n",
        "    :param model: The model to use for the request (default is Gemini 2.5 Flash).\n",
        "    :param sample_size: Number of files to sample from the directory.\n",
        "    :return: List of responses from the model for each file.\n",
        "    \"\"\"\n",
        "    from pathlib import Path\n",
        "    from datetime import datetime\n",
        "    import time\n",
        "    from tqdm import tqdm  # for progress bar\n",
        "    import random  # for random sampling\n",
        "\n",
        "    directory = Path(directory_path)\n",
        "    txt_files = list(directory.glob(\"**/*.txt\"))  # Get all .txt files recursively\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Found {len(txt_files)} text files in the directory {directory}.\")\n",
        "\n",
        "    if len(txt_files) < sample_size:\n",
        "        print(f\"Not enough files in the directory. Found {len(txt_files)} files, but requested {sample_size}.\")\n",
        "        return []\n",
        "\n",
        "    if seed is not None:\n",
        "        random.seed(seed) # this ensures reproducibility of the random sample\n",
        "        # most random operations in computer science are not truly random, but rather pseudo-random\n",
        "\n",
        "    sampled_files = random.sample(txt_files, sample_size)  # Randomly sample files\n",
        "    responses = {} # we will use a dictionary to store responses with file names as keys\n",
        "\n",
        "    for file in tqdm(sampled_files):\n",
        "        with file.open('r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "        if verbose:\n",
        "            now = datetime.now()\n",
        "            # let's print the time in a human-readable format\n",
        "            print(f\"{now.strftime('%Y-%m-%d %H:%M:%S')}  Processing file: {file.name} ({file.stat().st_size:,} bytes)\")\n",
        "\n",
        "        response = get_openrouter_response(system_prompt, content, model=model)\n",
        "        if response:\n",
        "            responses[file.name] = response\n",
        "        else:\n",
        "            print(f\"Failed to get a response for {file.name}\")\n",
        "\n",
        "        time.sleep(delay)  # Delay to avoid hitting API rate limits if necessary\n",
        "\n",
        "    return responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "411f04ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "411f04ad",
        "outputId": "72d3076c-5568-4793-93ed-9e8c91593cd3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [01:12<00:00,  2.41s/it]\n"
          ]
        }
      ],
      "source": [
        "# what was our extract_dir again?\n",
        "# we actually want the subdirectory with Latvian_Economic_Review folder\n",
        "extract_dir = Path(\"data/Latvian_Economic_Review\")  # Adjusted to point to the correct subdirectory\n",
        "responses = run_model_across_corpus(extract_dir, agriculture_system_prompt,\n",
        "                                    model=\"google/gemini-2.5-flash\", # actually the default but let's be explicit\n",
        "                                    sample_size=30, # again default is 30, but we can change it\n",
        "                                    seed=2025, # specific seed for reproducibility\n",
        "                                    verbose=False # set to True to see progress and processing information\n",
        "                                    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c44c83a",
      "metadata": {
        "id": "4c44c83a"
      },
      "source": [
        "## Analyzing responses\n",
        "\n",
        "Now that we got 30 responses let's see if they are relevant and if the model was able to filter out the noise and provide us with relevant concepts.\n",
        "\n",
        "Main thing is to remember the format of our responses. Each response is a string in JSON format that contains the concepts extracted from the text. We have stored all responses in a dictionary with file name as key and response as value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "353b5ef4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "353b5ef4",
        "outputId": "5124e4b3-8199-422e-f19a-fa01058d1cd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Got 30 responses from the model across the sampled files.\n"
          ]
        }
      ],
      "source": [
        "# how many responses do we have?\n",
        "print(f\"Got {len(responses)} responses from the model across the sampled files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "7b343782",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b343782",
        "outputId": "51dd4ad8-6789-4a57-f2bd-688a7982377b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File: lerq1938s01n04_029_plaintext_s32.txt\n",
            "Response: []\n",
            "\n",
            "File: lerq1936s01n02_030_plaintext_s16.txt\n",
            "Response: I'm sorry, but I cannot find any concepts related to Agriculture & Rural Economy in the provided text. The document focuses on the technical specifications and quality standards for the export of sawn coniferous timber from Latvia.\n",
            "\n",
            "File: lerq1939s01n02_021_plaintext_s09.txt\n",
            "Response: [\n",
            "  {\n",
            "    \"phrase\": \"building of dwelling houses for agricultural labourers\",\n",
            "    \"explanation\": \"This refers to an allocation of funds for constructing housing for workers in the agricultural sector, indicating a focus on rural infrastructure and the welfare of agricultural workers.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"assisting farmers, who happen to be in difficult circumstances\",\n",
            "    \"explanation\": \"This indicates a government initiative to provide financial or other aid to farmers facing economic hardship, suggesting a form of rural support or credit.\",\n",
            "    \"subcategory\": \"Rural credit\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"rationalisation fund\",\n",
            "    \"explanation\": \"While not exclusively agricultural, a 'rationalisation fund' in the context of assisting farmers and other economic activities likely implies efforts to improve efficiency and productivity, which would include agricultural modernization.\",\n",
            "    \"subcategory\": \"Agricultural modernization\"\n",
            "  }\n",
            "]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# let's print the first 3 responses from our responses dictionary\n",
        "for i, (file_name, response) in enumerate(responses.items()):\n",
        "    if i < 3:  # Print only the first 3 responses\n",
        "        print(f\"File: {file_name}\")\n",
        "        print(f\"Response: {response}\\n\")\n",
        "    else:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "dec7722e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dec7722e",
        "outputId": "cbc82a3e-e027-48b6-edf1-f133d9793464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File: lerq1938s01n04_029_plaintext_s32.txt has no valid response.\n",
            "File: lerq1936s01n02_030_plaintext_s16.txt has a valid response.\n",
            "File: lerq1939s01n02_021_plaintext_s09.txt has a valid response.\n",
            "File: lerq1938s01n03_021_plaintext_s08.txt has no valid response.\n",
            "File: lerq1936s01n04_038_plaintext_s17.txt has no valid response.\n",
            "File: lerq1938s01n04_019_plaintext_s17.txt has a valid response.\n",
            "File: lerq1936s01n01_003_plaintext_s01.txt has a valid response.\n",
            "File: lerq1938s01n01_007_plaintext_s03.txt has no valid response.\n",
            "File: lerq1938s01n01_013_plaintext_s05.txt has a valid response.\n",
            "File: lerq1938s01n04_035_plaintext_s38.txt has no valid response.\n",
            "File: lerq1937s01n06_007_plaintext_s05.txt has no valid response.\n",
            "File: lerq1936s01n02_018_plaintext_s07.txt has a valid response.\n",
            "File: lerq1938s01n01_021_plaintext_s08.txt has a valid response.\n",
            "File: lerq1938s01n01_036_plaintext_s18.txt has a valid response.\n",
            "File: lerq1936s01n03_005_plaintext_s01.txt has a valid response.\n",
            "File: lerq1936s01n01_032_plaintext_s23.txt has no valid response.\n",
            "File: lerq1936s01n03_021_plaintext_s11.txt has no valid response.\n",
            "File: lerq1940s01n01_032_plaintext_s12.txt has a valid response.\n",
            "File: lerq1937s01n05_031_plaintext_s15.txt has a valid response.\n",
            "File: lerq1938s01n01_037_plaintext_s23.txt has no valid response.\n",
            "File: lerq1936s01n01_036_plaintext_s26.txt has a valid response.\n",
            "File: lerq1939s01n01_037_plaintext_s14.txt has no valid response.\n",
            "File: lerq1936s01n01_023_plaintext_s15.txt has no valid response.\n",
            "File: lerq1938s01n04_024_plaintext_s30.txt has a valid response.\n",
            "File: lerq1938s01n04_013_plaintext_s08.txt has a valid response.\n",
            "File: lerq1936s01n02_018_plaintext_s08.txt has no valid response.\n",
            "File: lerq1940s01n01_042_plaintext_s18.txt has a valid response.\n",
            "File: lerq1937s01n05_010_plaintext_s06.txt has no valid response.\n",
            "File: lerq1940s01n01_035_plaintext_s14.txt has a valid response.\n",
            "File: lerq1937s01n05_006_plaintext_s03.txt has no valid response.\n"
          ]
        }
      ],
      "source": [
        "# let's see if we got any responses at all meaning responses that are not [] or None\n",
        "for file_name, response in responses.items():\n",
        "    if response and response != \"[]\":\n",
        "        print(f\"File: {file_name} has a valid response.\")\n",
        "    else:\n",
        "        print(f\"File: {file_name} has no valid response.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "00cc9947",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00cc9947",
        "outputId": "a64c5ea5-550f-457e-e255-a10307ac4578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Got 16 valid responses from the model across the sampled files.\n"
          ]
        }
      ],
      "source": [
        "# Let's get valid responses only and print them\n",
        "valid_response = {}\n",
        "for file_name, response in responses.items():\n",
        "    if response and response != \"[]\":\n",
        "        valid_response[file_name] = response\n",
        "# how many valid responses do we have?\n",
        "print(f\"Got {len(valid_response)} valid responses from the model across the sampled files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b03372ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b03372ca",
        "outputId": "e1871ab8-a534-4132-a9bc-d944a3ada79f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File: lerq1936s01n02_030_plaintext_s16.txt\n",
            "Response: I'm sorry, but I cannot find any concepts related to Agriculture & Rural Economy in the provided text. The document focuses on the technical specifications and quality standards for the export of sawn coniferous timber from Latvia.\n",
            "\n",
            "File: lerq1939s01n02_021_plaintext_s09.txt\n",
            "Response: [\n",
            "  {\n",
            "    \"phrase\": \"building of dwelling houses for agricultural labourers\",\n",
            "    \"explanation\": \"This refers to an allocation of funds for constructing housing for workers in the agricultural sector, indicating a focus on rural infrastructure and the welfare of agricultural workers.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"assisting farmers, who happen to be in difficult circumstances\",\n",
            "    \"explanation\": \"This indicates a government initiative to provide financial or other aid to farmers facing economic hardship, suggesting a form of rural support or credit.\",\n",
            "    \"subcategory\": \"Rural credit\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"rationalisation fund\",\n",
            "    \"explanation\": \"While not exclusively agricultural, a 'rationalisation fund' in the context of assisting farmers and other economic activities likely implies efforts to improve efficiency and productivity, which would include agricultural modernization.\",\n",
            "    \"subcategory\": \"Agricultural modernization\"\n",
            "  }\n",
            "]\n",
            "\n",
            "File: lerq1938s01n04_019_plaintext_s17.txt\n",
            "Response: [\n",
            "  {\n",
            "    \"phrase\": \"savings and loan societies\",\n",
            "    \"explanation\": \"These are financial institutions that attract savings from the population and provide petty credit, likely supporting rural communities and agricultural endeavors.\",\n",
            "    \"subcategory\": \"Rural credit\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"co-operative societies and their unions\",\n",
            "    \"explanation\": \"Refers to organizations where members pool resources for mutual benefit, which could include agricultural cooperatives for production, marketing, or credit.\",\n",
            "    \"subcategory\": \"Peasant cooperatives\"\n",
            "  }\n",
            "]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# let's print first 3 valid responses\n",
        "for i, (file_name, response) in enumerate(valid_response.items()):\n",
        "    if i < 3:  # Print only the first 3 valid responses\n",
        "        print(f\"File: {file_name}\")\n",
        "        print(f\"Response: {response}\\n\")\n",
        "    else:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1966916d",
      "metadata": {
        "id": "1966916d"
      },
      "source": [
        "## Converting responses from JSON to Python dictionary\n",
        "\n",
        "Our LLM responses are formated as JSON strings, so we need to convert them to Python dictionaries and/or lists for easier manipulation and analysis. We can use the `json` module in Python to do this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "dda854a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dda854a6",
        "outputId": "7035dab0-39cb-4be7-be96-8e932602037a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error decoding JSON for file lerq1936s01n02_030_plaintext_s16.txt: Expecting value: line 1 column 1 (char 0)\n",
            "Error decoding JSON for file lerq1936s01n01_003_plaintext_s01.txt: Expecting value: line 1 column 1 (char 0)\n",
            "Error decoding JSON for file lerq1938s01n01_013_plaintext_s05.txt: Expecting value: line 1 column 1 (char 0)\n",
            "Error decoding JSON for file lerq1938s01n01_021_plaintext_s08.txt: Expecting value: line 1 column 1 (char 0)\n",
            "Error decoding JSON for file lerq1940s01n01_032_plaintext_s12.txt: Expecting value: line 1 column 1 (char 0)\n",
            "Error decoding JSON for file lerq1936s01n01_036_plaintext_s26.txt: Expecting value: line 1 column 1 (char 0)\n",
            "Error decoding JSON for file lerq1938s01n04_024_plaintext_s30.txt: Expecting value: line 1 column 1 (char 0)\n",
            "Error decoding JSON for file lerq1940s01n01_042_plaintext_s18.txt: Expecting value: line 1 column 1 (char 0)\n",
            "File: lerq1939s01n02_021_plaintext_s09.txt\n",
            "Response: [{'phrase': 'building of dwelling houses for agricultural labourers', 'explanation': 'This refers to an allocation of funds for constructing housing for workers in the agricultural sector, indicating a focus on rural infrastructure and the welfare of agricultural workers.', 'subcategory': 'Other agricultural practices'}, {'phrase': 'assisting farmers, who happen to be in difficult circumstances', 'explanation': 'This indicates a government initiative to provide financial or other aid to farmers facing economic hardship, suggesting a form of rural support or credit.', 'subcategory': 'Rural credit'}, {'phrase': 'rationalisation fund', 'explanation': \"While not exclusively agricultural, a 'rationalisation fund' in the context of assisting farmers and other economic activities likely implies efforts to improve efficiency and productivity, which would include agricultural modernization.\", 'subcategory': 'Agricultural modernization'}]\n",
            "\n",
            "File: lerq1938s01n04_019_plaintext_s17.txt\n",
            "Response: [{'phrase': 'savings and loan societies', 'explanation': 'These are financial institutions that attract savings from the population and provide petty credit, likely supporting rural communities and agricultural endeavors.', 'subcategory': 'Rural credit'}, {'phrase': 'co-operative societies and their unions', 'explanation': 'Refers to organizations where members pool resources for mutual benefit, which could include agricultural cooperatives for production, marketing, or credit.', 'subcategory': 'Peasant cooperatives'}]\n",
            "\n",
            "File: lerq1936s01n02_018_plaintext_s07.txt\n",
            "Response: [{'phrase': 'import of fruit, foodstuffs, animal products and cattle fodder is subject to sanitary control', 'explanation': 'This indicates regulations concerning the import of agricultural and animal products to ensure their safety and quality, which impacts agricultural trade.', 'subcategory': 'Agricultural exports'}]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# let's go through all responses and convert them from JSON strings to Python data structures\n",
        "# this will also be a good test on LLM output stability\n",
        "import json\n",
        "# let's convert all responses from JSON strings to Python data structures\n",
        "converted_responses = {}\n",
        "bad_responses = {}\n",
        "for file_name, response in valid_response.items():\n",
        "    try:\n",
        "        converted_responses[file_name] = json.loads(response)  # Convert JSON string to Python data structure\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON for file {file_name}: {e}\")\n",
        "        bad_responses[file_name] = response\n",
        "\n",
        "# let's print the first 3 converted responses\n",
        "for i, (file_name, response) in enumerate(converted_responses.items()):\n",
        "    if i < 3:  # Print only the first 3 converted responses\n",
        "        print(f\"File: {file_name}\")\n",
        "        print(f\"Response: {response}\\n\")\n",
        "    else:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "70b6338f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70b6338f",
        "outputId": "9049e13e-1dfb-463f-b8f2-ed7ec0a2b65c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File: lerq1936s01n02_030_plaintext_s16.txt has a bad response: I'm sorry, but I cannot find any concepts related to Agriculture & Rural Economy in the provided text. The document focuses on the technical specifications and quality standards for the export of sawn coniferous timber from Latvia.\n",
            "File: lerq1936s01n01_003_plaintext_s01.txt has a bad response: ```json\n",
            "[\n",
            "  {\n",
            "    \"phrase\": \"Agricultural products of superior quality, such as butter, bacon, seeds etc., as well as timber materials rank foremost among our exports.\",\n",
            "    \"explanation\": \"Highlights the primary role of high-quality agricultural products and timber in Latvia's export economy.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"export of Latvian butter, which receded from 18,739 tons in 1931 to 15,701 tons in 1934.\",\n",
            "    \"explanation\": \"Indicates a decline in the volume of butter exported, reflecting challenges in the dairy export sector.\",\n",
            "    \"subcategory\": \"Dairy and livestock production\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"reorganisation of the butter export trade\",\n",
            "    \"explanation\": \"Refers to efforts to restructure and improve the system for exporting butter to maintain market share and value.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"export of bacon has developed in recent years.\",\n",
            "    \"explanation\": \"Indicates growth and increasing importance of bacon as an agricultural export product.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"export of clover seed increased steadily from 1,212 tons in 1930 to 3,328 tons in 1934.\",\n",
            "    \"explanation\": \"Shows a positive trend in the export volume of a specific agricultural crop, clover seed.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"seed export trade has been thoroughly reorganised lately, with a view to maintaining the high quality of Latvian seeds.\",\n",
            "    \"explanation\": \"Describes initiatives to restructure and improve the seed export sector, focusing on quality control.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"fall of prices of agricultural products in the world market, which has placed agrarian countries in a very awkward position.\",\n",
            "    \"explanation\": \"Highlights the negative impact of declining global prices for agricultural goods on countries heavily reliant on agrarian economies.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"remedial measures in agricultural countries\",\n",
            "    \"explanation\": \"Refers to actions taken by governments in agrarian nations to mitigate the effects of the economic depression on their agricultural sectors.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"diminished purchasing power of the farmers, who are themselves the principal buyers\",\n",
            "    \"explanation\": \"Indicates that farmers, due to reduced income from agricultural product sales, have less money to spend on imported goods, impacting the overall economy.\",\n",
            "    \"subcategory\": \"Rural credit\"\n",
            "  }\n",
            "]\n",
            "```\n",
            "File: lerq1938s01n01_013_plaintext_s05.txt has a bad response: I am sorry, but the provided text does not contain any concepts related to \"Agriculture & Rural Economy\". The document focuses on foreign policy, international relations, and geopolitical issues in the 1930s.\n",
            "File: lerq1938s01n01_021_plaintext_s08.txt has a bad response: ```json\n",
            "[\n",
            "  {\n",
            "    \"phrase\": \"first agricultural society in 1886\",\n",
            "    \"explanation\": \"Establishment of an organization focused on agricultural development and cooperation.\",\n",
            "    \"subcategory\": \"Peasant cooperatives\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"first threshing machine society in 1888\",\n",
            "    \"explanation\": \"Formation of a cooperative for the shared use of agricultural machinery, indicating early mechanization efforts.\",\n",
            "    \"subcategory\": \"Mechanization of agriculture\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"first cattle control society in 1901\",\n",
            "    \"explanation\": \"Establishment of a society aimed at managing and improving livestock, likely related to breeding or health.\",\n",
            "    \"subcategory\": \"Dairy and livestock production\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"first co-operative dairy in 1909\",\n",
            "    \"explanation\": \"Formation of a cooperative for processing and marketing dairy products, crucial for the rural economy.\",\n",
            "    \"subcategory\": \"Dairy and livestock production\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"Riga Agricultural Central Society\",\n",
            "    \"explanation\": \"A central organization formed to spread agricultural knowledge, promoting modernization and best practices.\",\n",
            "    \"subcategory\": \"Agricultural modernization\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"Agric. machine collective utilisation societies\",\n",
            "    \"explanation\": \"Societies formed for the shared use of agricultural machinery, indicating cooperative mechanization.\",\n",
            "    \"subcategory\": \"Mechanization of agriculture\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"dairy societies ... practically won for themselves the monopoly in the producing of butter, cheese and other milk products, which are of such great importance in Latvia's agriculture and export trade\",\n",
            "    \"explanation\": \"Cooperative organizations dominating the production of key agricultural exports, highlighting their success and economic impact.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"amalgamating the weaker societies, whose equipment was less up-to-date, with the larger societies\",\n",
            "    \"explanation\": \"Consolidation of dairy cooperatives to improve efficiency and rationalize production, indicating a move towards modernization.\",\n",
            "    \"subcategory\": \"Agricultural modernization\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"mutual fire insurance societies operate principally in the countryside, where they undertake fire risks in respect of farm-houses, furniture, fittings, stock and other objects\",\n",
            "    \"explanation\": \"Insurance services specifically for rural areas, covering agricultural assets and property, which supports rural stability.\",\n",
            "    \"subcategory\": \"Rural credit\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"agricultural societies\",\n",
            "    \"explanation\": \"General term for societies focused on agricultural activities and interests, likely promoting collective action among farmers.\",\n",
            "    \"subcategory\": \"Peasant cooperatives\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"societies for the mutual use of machines\",\n",
            "    \"explanation\": \"Organizations facilitating shared access to agricultural machinery, reducing individual costs and promoting efficiency.\",\n",
            "    \"subcategory\": \"Mechanization of agriculture\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"Latvian Chamber of Agriculture\",\n",
            "    \"explanation\": \"A government-established body aimed at organizing and supervising agricultural activities and cooperatives.\",\n",
            "    \"subcategory\": \"Agricultural modernization\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"non-profit-bearing agricultural societies which operate under the supervision of the Chamber\",\n",
            "    \"explanation\": \"New type of agricultural societies focused on non-commercial objectives, guided by the Chamber of Agriculture to systematize rural organization.\",\n",
            "    \"subcategory\": \"Peasant cooperatives\"\n",
            "  }\n",
            "]\n",
            "```\n",
            "File: lerq1940s01n01_032_plaintext_s12.txt has a bad response: ```json\n",
            "[\n",
            "  {\n",
            "    \"phrase\": \"agricultural produce exporting country\",\n",
            "    \"explanation\": \"Describes Latvia's economic character as primarily focused on exporting agricultural goods.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"agricultural machinery\",\n",
            "    \"explanation\": \"Refers to equipment used in farming, imported by Latvia from the Soviet Union.\",\n",
            "    \"subcategory\": \"Mechanization of agriculture\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"bran for feeding livestock\",\n",
            "    \"explanation\": \"Indicates the import of feed for animals, suggesting a component of livestock production.\",\n",
            "    \"subcategory\": \"Dairy and livestock production\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"various items of Latvian agricultural produce\",\n",
            "    \"explanation\": \"Refers to a range of agricultural products from Latvia that the Soviet Union is interested in purchasing.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"live pigs\",\n",
            "    \"explanation\": \"Specific mention of an animal product exported by Latvia, indicating livestock production.\",\n",
            "    \"subcategory\": \"Dairy and livestock production\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"raw callhides, sole leather and chrome leather\",\n",
            "    \"explanation\": \"Products derived from animal hides, indicating a connection to livestock and related industries.\",\n",
            "    \"subcategory\": \"Dairy and livestock production\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"phosphorites, which are required for the production of superphosphate\",\n",
            "    \"explanation\": \"Phosphorites are raw materials for superphosphate, a type of fertilizer, indicating an agricultural input.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  }\n",
            "]\n",
            "```\n",
            "File: lerq1936s01n01_036_plaintext_s26.txt has a bad response: ```json\n",
            "[\n",
            "  {\n",
            "    \"phrase\": \"SEEDS EXPORT AND IMPORT\",\n",
            "    \"explanation\": \"Refers to the international trade of seeds, indicating a market for agricultural inputs and outputs.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"FLAX EXPORT\",\n",
            "    \"explanation\": \"Refers to the export of flax, an agricultural product used for textiles and other purposes, indicating its importance in foreign trade.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"BUTTER AND EGGS EXPORT\",\n",
            "    \"explanation\": \"Refers to the export of dairy and poultry products, highlighting their role in the country's agricultural trade.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"BACON EXPORT\",\n",
            "    \"explanation\": \"Refers to the export of processed pork products, indicating a focus on livestock production and its contribution to foreign trade.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"Valsts zemes banka\",\n",
            "    \"explanation\": \"A state land bank, likely involved in providing credit or managing land-related financial transactions, potentially supporting land reform or agricultural development.\",\n",
            "    \"subcategory\": \"Rural credit\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"Latvijas zemnieku kreditbanka\",\n",
            "    \"explanation\": \"A credit bank specifically for Latvian farmers, indicating a dedicated financial institution providing loans or other financial services to the agricultural sector.\",\n",
            "    \"subcategory\": \"Rural credit\"\n",
            "  }\n",
            "]\n",
            "```\n",
            "File: lerq1938s01n04_024_plaintext_s30.txt has a bad response: ```json\n",
            "[\n",
            "  {\n",
            "    \"phrase\": \"general agricultural census of 1939\",\n",
            "    \"explanation\": \"A planned comprehensive survey of agricultural activities and resources to gather economic and land ownership data.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"agricultural census in all the Baltic countries in 1939\",\n",
            "    \"explanation\": \"A coordinated effort among Baltic states to conduct a comprehensive survey of their agricultural sectors in 1939.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"fishery census\",\n",
            "    \"explanation\": \"A planned survey to collect data on fishing activities, specifically targeting farms with fishing boats or tackle.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"rural building statistics\",\n",
            "    \"explanation\": \"The organization of statistical data to track the increase and purpose of new buildings in rural areas.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  }\n",
            "]\n",
            "```\n",
            "File: lerq1940s01n01_042_plaintext_s18.txt has a bad response: ```json\n",
            "[\n",
            "  {\n",
            "    \"phrase\": \"agronomist advocating rational agriculture\",\n",
            "    \"explanation\": \"Refers to the role of an agronomist promoting efficient and scientific farming methods.\",\n",
            "    \"subcategory\": \"Agricultural modernization\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"cultivating of grain left a very small margin of profit\",\n",
            "    \"explanation\": \"Indicates that grain farming was not very profitable, suggesting low yields or market prices.\",\n",
            "    \"subcategory\": \"Crop yields\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"stock-farming and dairying were yet in a primitive state\",\n",
            "    \"explanation\": \"Describes the underdeveloped condition of livestock and dairy production, implying a need for improvement.\",\n",
            "    \"subcategory\": \"Dairy and livestock production\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"adopt the modern dairy technic of Denmark and other countries\",\n",
            "    \"explanation\": \"Advocates for the adoption of advanced dairy farming methods from other nations to improve local practices.\",\n",
            "    \"subcategory\": \"Agricultural modernization\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"acquire new machines\",\n",
            "    \"explanation\": \"Refers to the acquisition of modern equipment to improve agricultural efficiency.\",\n",
            "    \"subcategory\": \"Mechanization of agriculture\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"establish co-operative dairy societies\",\n",
            "    \"explanation\": \"Promotes the formation of farmer-led cooperative organizations specifically for dairy production.\",\n",
            "    \"subcategory\": \"Peasant cooperatives\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"full use of the achievements of agronomical science\",\n",
            "    \"explanation\": \"Encourages farmers to apply scientific advancements in agriculture to improve their practices.\",\n",
            "    \"subcategory\": \"Agricultural modernization\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"Latvian grain farming suffered from foreign competition\",\n",
            "    \"explanation\": \"Indicates that domestic grain production was negatively impacted by cheaper or more abundant foreign grain.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"protection of the existing agricultural branches from foreign competition\",\n",
            "    \"explanation\": \"Refers to policies aimed at safeguarding domestic agricultural sectors from the adverse effects of international competition.\",\n",
            "    \"subcategory\": \"Agricultural exports\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"variety of domestic agricultural products\",\n",
            "    \"explanation\": \"Emphasizes the importance of diversifying agricultural output within the country.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"reduce considerably the costprice of agricultural produce\",\n",
            "    \"explanation\": \"Aims to lower the production costs of agricultural goods to make them more competitive or profitable.\",\n",
            "    \"subcategory\": \"Other agricultural practices\"\n",
            "  },\n",
            "  {\n",
            "    \"phrase\": \"farmers should be guaranteed reasonable and firm prices for their products for several years in advance\",\n",
            "    \"explanation\": \"Advocates for price stability and security for agricultural products to encourage farmers to invest in improvements.\",\n",
            "    \"subcategory\": \"Rural credit\"\n",
            "  }\n",
            "]\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# let's print the bad responses\n",
        "for file_name, response in bad_responses.items():\n",
        "    print(f\"File: {file_name} has a bad response: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c78ec18a",
      "metadata": {
        "id": "c78ec18a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "6cbd75a6",
      "metadata": {
        "id": "6cbd75a6"
      },
      "source": [
        "### Fixing bad responses\n",
        "\n",
        "We see that bad responses are actually are not truly bad they just start with ```json and end with ```. We can fix this by removing the ```json and ``` from the start and end of the response string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f0f32894",
      "metadata": {
        "id": "f0f32894"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error decoding JSON for file lerq1936s01n02_030_plaintext_s16.txt: Expecting value: line 1 column 1 (char 0)\n",
            "Error decoding JSON for file lerq1938s01n01_013_plaintext_s05.txt: Expecting value: line 1 column 1 (char 0)\n"
          ]
        }
      ],
      "source": [
        "# let's go through all responses and convert them from JSON strings to Python dictionaries\n",
        "# we will also strip the response from ```json and ``` at the start and end of the response string\n",
        "good_responses = {}\n",
        "bad_responses = {}\n",
        "for file_name, response in responses.items():\n",
        "    response = response.lstrip(\"```json\")  # Remove ```json from the start\n",
        "    response = response.rstrip(\"```\")  # Remove ``` from the end\n",
        "    try:\n",
        "        good_responses[file_name] = json.loads(response)  # Convert JSON string to Python data structure\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON for file {file_name}: {e}\")\n",
        "        bad_responses[file_name] = response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "c91c3514",
      "metadata": {
        "id": "c91c3514"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File: lerq1936s01n02_030_plaintext_s16.txt has a bad response: I'm sorry, but I cannot find any concepts related to Agriculture & Rural Economy in the provided text. The document focuses on the technical specifications and quality standards for the export of sawn coniferous timber from Latvia.\n",
            "File: lerq1938s01n01_013_plaintext_s05.txt has a bad response: I am sorry, but the provided text does not contain any concepts related to \"Agriculture & Rural Economy\". The document focuses on foreign policy, international relations, and geopolitical issues in the 1930s.\n"
          ]
        }
      ],
      "source": [
        "# let's print first bad responses again\n",
        "for file_name, response in bad_responses.items():\n",
        "    print(f\"File: {file_name} has a bad response: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d0746dd",
      "metadata": {
        "id": "3d0746dd"
      },
      "source": [
        "### Fixing bad JSON\n",
        "\n",
        "We have gotten the easy cases fixed but the last malformed JSON is due to double quotes not being escaped properly in all places. We can fix this by replacing the double quotes with escaped double quotes.\n",
        "\n",
        "Alternatively we could apply stricter instructions to the model to return valid JSON. However, this would require us to adjust our system prompt and possibly the model parameters as well.\n",
        "\n",
        "We have a third option which is to supply the response to LLM again with different prompt and have LLM fix the JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "b57db797",
      "metadata": {
        "id": "b57db797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fixed response for lerq1936s01n02_030_plaintext_s16.txt: ```json\n",
            "[\n",
            "  {\n",
            "    \"concept\": \"Forestry\",\n",
            "    \"relevance\": \"High\",\n",
            "    \"reason\": \"The document is entirely focused on 'sawn coniferous timber' and its export, which is a core aspect of forestry and timber production.\"\n",
            "  },\n",
            "  {\n",
            "    \"concept\": \"International Trade\",\n",
            "    \"relevance\": \"High\",\n",
            "    \"reason\": \"The document specifies 'export' of timber from Latvia, detailing quality standards and technical specifications for international buyers, which is a direct aspect of international trade.\"\n",
            "  },\n",
            "  {\n",
            "    \"concept\": \"Quality Control\",\n",
            "    \"relevance\": \"High\",\n",
            "    \"reason\": \"The document explicitly discusses 'quality standards' and 'technical specifications' for timber, indicating a strong focus on ensuring the quality of the product for export.\"\n",
            "  },\n",
            "  {\n",
            "    \"concept\": \"Wood Industry\",\n",
            "    \"relevance\": \"High\",\n",
            "    \"reason\": \"The subject matter is 'sawn coniferous timber', which is a primary product of the wood industry. The document details its characteristics and standards.\"\n",
            "  },\n",
            "  {\n",
            "    \"concept\": \"Logistics & Supply Chain\",\n",
            "    \"relevance\": \"Medium\",\n",
            "    \"reason\": \"While not explicitly detailed, the 'export' of timber inherently involves logistics and supply chain considerations (transport, storage, delivery to international markets) to move the product from Latvia to its destination.\"\n",
            "  },\n",
            "  {\n",
            "    \"concept\": \"Environmental Regulations\",\n",
            "    \"relevance\": \"Low\",\n",
            "    \"reason\": \"The document focuses on product specifications and trade, not environmental regulations related to timber harvesting or sustainable forestry. While relevant to the broader timber industry, it's not a direct focus of this specific text.\"\n",
            "  },\n",
            "  {\n",
            "    \"concept\": \"Economic Development\",\n",
            "    \"relevance\": \"Low\",\n",
            "    \"reason\": \"Export of timber contributes to economic development, but the document itself is a technical specification, not an analysis of economic impact or policy.\"\n",
            "  }\n",
            "]\n",
            "```\n",
            "Error decoding fixed JSON for file lerq1936s01n02_030_plaintext_s16.txt: Expecting value: line 1 column 1 (char 0)\n",
            "Failed to fix JSON for lerq1936s01n02_030_plaintext_s16.txt\n",
            "Fixed response for lerq1938s01n01_013_plaintext_s05.txt: ```json\n",
            "[\n",
            "  {\n",
            "    \"concept\": \"Agriculture & Rural Economy\",\n",
            "    \"relevance\": \"none\",\n",
            "    \"reason\": \"The document focuses on foreign policy, international relations, and geopolitical issues in the 1930s. It does not contain any content related to agriculture or rural economy.\"\n",
            "  }\n",
            "]\n",
            "```\n",
            "Error decoding fixed JSON for file lerq1938s01n01_013_plaintext_s05.txt: Expecting value: line 1 column 1 (char 0)\n",
            "Failed to fix JSON for lerq1938s01n01_013_plaintext_s05.txt\n"
          ]
        }
      ],
      "source": [
        "# let's fix the malformed JSON responses\n",
        "fix_json_prompt = \"\"\"Please fix the following JSON response which should be an array of objects where double quotes are not escaped properly.\n",
        "The response should be proper JSON string without ```json start and without ``` end with all other content intact.\"\"\"\n",
        "fixed_responses = {}\n",
        "for file_name, response in bad_responses.items():\n",
        "    # we will use the same get_openrouter_response function to fix the JSON\n",
        "    fixed_response = get_openrouter_response(fix_json_prompt, response, model=\"google/gemini-2.5-flash\")\n",
        "    print(f\"Fixed response for {file_name}: {fixed_response}\")\n",
        "    # try parsing response to ensure it is valid JSON\n",
        "    try:\n",
        "        json.loads(fixed_response)  # Validate the fixed response\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding fixed JSON for file {file_name}: {e}\")\n",
        "        fixed_response = None\n",
        "    if fixed_response:\n",
        "        fixed_responses[file_name] = fixed_response\n",
        "    else:\n",
        "        print(f\"Failed to fix JSON for {file_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34s6AuyekV4n",
      "metadata": {
        "id": "34s6AuyekV4n"
      },
      "source": [
        "## Running custom system prompt on specific folder\n",
        "\n",
        "Now that we have experimented with a small sample let's create a new function that takes the following parameters:\n",
        "\n",
        "\n",
        "- system_prompt - our custom instructions\n",
        "- file_folder where the files to be analyzed is needed\n",
        "- file_extension where we want .txt to be default extenstion\n",
        "- max_files where we want None to be default meaning unlimited files\n",
        "- seed where we want None to be default this would be used when max_files is not None\n",
        "- model where default would be \"google/gemini-2.5-flash\"\n",
        "\n",
        "We could add a few more parameters such as temperature and max tokens but this is enough to get started\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "nshDX3a-lYfZ",
      "metadata": {
        "id": "nshDX3a-lYfZ"
      },
      "outputs": [],
      "source": [
        "# we want a function that will take our system prompt and file folder and will save in save_folder all responses using original file name and custom suffix\n",
        "# - system_prompt - our custom instructions\n",
        "# - file_folder where the files to be analyzed is needed\n",
        "# - file_extension where we want .txt to be default extenstion\n",
        "# - max_files where we want None to be default meaning unlimited files\n",
        "# - seed where we want None to be default this would be used when max_files is not None\n",
        "# - model where default would be \"google/gemini-2.5-flash\"\n",
        "# - save_folder where default would be \"responses\"\n",
        "# - we could specific save suffix but instead let's have a filename friendly version of model meaning we will remove / and - and replace those with _\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "def save_responses_from_folder(system_prompt,\n",
        "                               file_folder,\n",
        "                               file_extension=\".txt\",\n",
        "                               max_files=None,\n",
        "                               seed=None,\n",
        "                               model=\"google/gemini-2.5-flash\",\n",
        "                               save_folder=\"responses\",\n",
        "                               delay=0.1,\n",
        "                               verbose=True):\n",
        "    \"\"\"\n",
        "    Processes files in a folder using an LLM and saves the responses.\n",
        "\n",
        "    :param system_prompt: The system prompt to guide the model's behavior.\n",
        "    :param file_folder: Path to the directory containing files to process.\n",
        "    :param file_extension: The extension of the files to process (default is .txt).\n",
        "    :param max_files: Maximum number of files to process (default is None for all files).\n",
        "    :param seed: Seed for random sampling if max_files is not None.\n",
        "    :param model: The model to use for the request (default is google/gemini-2.5-flash).\n",
        "    :param save_folder: The folder to save the responses (default is \"responses\").\n",
        "    :param delay: Delay in seconds between API calls to avoid rate limits.\n",
        "    :param verbose: If True, print detailed information during processing.\n",
        "    \"\"\"\n",
        "    file_folder_path = Path(file_folder)\n",
        "    save_folder_path = Path(save_folder)\n",
        "\n",
        "    if not file_folder_path.exists():\n",
        "        print(f\"Error: File folder '{file_folder}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    # Create save folder if it doesn't exist\n",
        "    save_folder_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Create a filename-friendly version of the model name\n",
        "    model_suffix = model.replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
        "\n",
        "    # Find files with the specified extension\n",
        "    all_files = sorted(file_folder_path.rglob(f\"*{file_extension}\"))\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Found {len(all_files)} '{file_extension}' files in '{file_folder}'.\")\n",
        "\n",
        "    files_to_process = all_files\n",
        "    if max_files is not None and max_files < len(all_files):\n",
        "        if seed is not None:\n",
        "            random.seed(seed)\n",
        "        files_to_process = random.sample(all_files, max_files)\n",
        "        if verbose:\n",
        "            print(f\"Processing a random sample of {len(files_to_process)} files.\")\n",
        "\n",
        "    if not files_to_process:\n",
        "        print(\"No files to process.\")\n",
        "        return\n",
        "\n",
        "    for file in tqdm(files_to_process, desc=\"Processing files\"):\n",
        "        if verbose:\n",
        "            now = datetime.now()\n",
        "            print(f\"\\n{now.strftime('%Y-%m-%d %H:%M:%S')}  Processing file: {file.relative_to(file_folder_path)} ({file.stat().st_size:,} bytes)\")\n",
        "\n",
        "        # Construct the output filename\n",
        "        output_filename = save_folder_path / f\"{file.stem}_{model_suffix}{file.suffix}\"\n",
        "\n",
        "\n",
        "        # Create parent directories for the output file if they don't exist\n",
        "        output_filename.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Check if response already exists\n",
        "        if output_filename.exists():\n",
        "            if verbose:\n",
        "                print(f\"  Response already exists for {file.name}, skipping.\")\n",
        "            continue\n",
        "\n",
        "\n",
        "        with file.open('r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        response = get_openrouter_response(system_prompt, content, model=model)\n",
        "\n",
        "        # Save the response to a file\n",
        "        with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
        "            outfile.write(response)\n",
        "            if verbose:\n",
        "                print(f\"  Response saved to {output_filename}\")\n",
        "\n",
        "        time.sleep(delay)  # Pause between requests\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\nFinished processing files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "6tpcRUPcoJm-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tpcRUPcoJm-",
        "outputId": "4530551a-7afc-42e8-fa36-007750f45b97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 419 '.txt' files in 'data/Latvian_Economic_Review'.\n",
            "Processing a random sample of 10 files.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "2025-08-06 22:05:28  Processing file: lerq1936s01n01_023_plaintext_s14.txt (1,211 bytes)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files:  10%|█         | 1/10 [00:01<00:10,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Response saved to responses\\lerq1936s01n01_023_plaintext_s14_google_gemini_2.5_flash..txt\n",
            "\n",
            "2025-08-06 22:05:29  Processing file: lerq1939s01n04_005_plaintext_s02.txt (449 bytes)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files:  20%|██        | 2/10 [00:02<00:08,  1.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Response saved to responses\\lerq1939s01n04_005_plaintext_s02_google_gemini_2.5_flash..txt\n",
            "\n",
            "2025-08-06 22:05:30  Processing file: lerq1937s01n06_006_plaintext_s03.txt (785 bytes)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files:  30%|███       | 3/10 [00:03<00:07,  1.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Response saved to responses\\lerq1937s01n06_006_plaintext_s03_google_gemini_2.5_flash..txt\n",
            "\n",
            "2025-08-06 22:05:31  Processing file: lerq1940s01n02_008_plaintext_s03.txt (14,211 bytes)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files:  40%|████      | 4/10 [00:05<00:10,  1.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Response saved to responses\\lerq1940s01n02_008_plaintext_s03_google_gemini_2.5_flash..txt\n",
            "\n",
            "2025-08-06 22:05:34  Processing file: lerq1940s01n02_003_plaintext_s01.txt (688 bytes)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files:  50%|█████     | 5/10 [00:06<00:07,  1.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Response saved to responses\\lerq1940s01n02_003_plaintext_s01_google_gemini_2.5_flash..txt\n",
            "\n",
            "2025-08-06 22:05:35  Processing file: lerq1939s01n02_014_plaintext_s06.txt (8,203 bytes)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files:  60%|██████    | 6/10 [00:09<00:07,  1.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Response saved to responses\\lerq1939s01n02_014_plaintext_s06_google_gemini_2.5_flash..txt\n",
            "\n",
            "2025-08-06 22:05:37  Processing file: lerq1938s01n01_036_plaintext_s16.txt (3,470 bytes)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files:  70%|███████   | 7/10 [00:10<00:04,  1.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Response saved to responses\\lerq1938s01n01_036_plaintext_s16_google_gemini_2.5_flash..txt\n",
            "\n",
            "2025-08-06 22:05:39  Processing file: lerq1938s01n02_038_plaintext_s15.txt (10,039 bytes)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files:  80%|████████  | 8/10 [00:53<00:29, 14.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Response saved to responses\\lerq1938s01n02_038_plaintext_s15_google_gemini_2.5_flash..txt\n",
            "\n",
            "2025-08-06 22:06:21  Processing file: lerq1937s01n08_037_plaintext_s15.txt (680 bytes)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files:  90%|█████████ | 9/10 [00:54<00:10, 10.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Response saved to responses\\lerq1937s01n08_037_plaintext_s15_google_gemini_2.5_flash..txt\n",
            "\n",
            "2025-08-06 22:06:23  Processing file: lerq1938s01n04_017_plaintext_s13.txt (3,466 bytes)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files: 100%|██████████| 10/10 [00:55<00:00,  5.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Response saved to responses\\lerq1938s01n04_017_plaintext_s13_google_gemini_2.5_flash..txt\n",
            "\n",
            "Finished processing files.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# let's get a folder that we want to process\n",
        "# it would be data/Latvian_Economic_Review folder here\n",
        "# for now let's only get 10 responses\n",
        "source_folder = \"data/Latvian_Economic_Review\"\n",
        "# now we can call the function using same old agriculture prompt\n",
        "save_responses_from_folder(agriculture_system_prompt,\n",
        "                           source_folder,\n",
        "                           max_files = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74Ds0R4tpsUZ",
      "metadata": {
        "id": "74Ds0R4tpsUZ"
      },
      "source": [
        "## Adjusting system prompt for simpler output\n",
        "\n",
        "Now we see that sometimes instead of JSON we get nonstandard output.\n",
        "\n",
        "This presents a problem for automatic processing, so again we would need to write more code to catch these non standard situations or make our prompt even stronger or change our model.\n",
        "\n",
        "For now we will change prompt to be simpler and to produce simple CSV - comma separated values output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "c056708f",
      "metadata": {},
      "outputs": [],
      "source": [
        "agriculture_system_prompt_list = \"\"\"You are an expert assistant trained to analyze historical economic texts from 1930s Latvia.\n",
        "Your task is to read the input document or paragraph and extract specific concepts related to **Agriculture & Rural Economy**.\n",
        "Return the concepts as a list of single words enclosed in double quotes, each on a new line.\n",
        "- Agricultural modernization return \"modernization\"\n",
        "- Land reform return \"reform\"\n",
        "- Crop yields return \"yields\"\n",
        "- Agricultural exports return \"exports\"\n",
        "- Collective farming return \"collective\"\n",
        "- Peasant cooperatives return \"cooperatives\"\n",
        "- Grain storage and reserves return \"storage\"\n",
        "- Rural credit return \"credit\"\n",
        "- Mechanization of agriculture return \"mechanization\"\n",
        "- Dairy and livestock production return \"dairy\"\n",
        "\n",
        "Example output:\n",
        "\n",
        "\"exports\"\n",
        "\"modernization\"\n",
        "\"reform\"\n",
        "\n",
        "If no relevant concept is found, return a single word \"None\" with quotes around it, like this: \"None\".\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "e0562ad0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files: 100%|██████████| 10/10 [00:12<00:00,  1.28s/it]\n"
          ]
        }
      ],
      "source": [
        "# now let's run this function again with the new system prompt and extract to responses_csv folder\n",
        "save_responses_from_folder(agriculture_system_prompt_list,\n",
        "                            source_folder,\n",
        "                            max_files=10,  # let's limit to 10 files for now\n",
        "                            save_folder=\"responses_list\",  # we will save to responses_list folder\n",
        "                            model=\"google/gemini-2.5-flash\", # using the same model as before\n",
        "                            verbose=False)  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab45a23b",
      "metadata": {},
      "source": [
        "## Class Project - submit for credit\n",
        "\n",
        "Extract all named entities from first 30 files in the LERQ corpus as individual 30 files in responses_ner folder.\n",
        "Create your own system prompt for this purpose.\n",
        "The individual response files should be as close as possible to the following CSV format:\n",
        "entity, category, description\n",
        "Riga, city, capital of Latvia\n",
        "Latvia, country, country in the Baltic region\n",
        "\n",
        "The file name should follow the same format as shown above: for example `lerq1936s01n02_032_plaintext_s18_google_gemini_2.5_flash.txt`\n",
        "\n",
        "### What to submit\n",
        "\n",
        "Submit a **SINGLE** zip file name_surname_bssdh2025.zip with your responses_ner folder that contains all 30 files with named entities extracted from the LERQ corpus. In the same name_surname_bssdh2025.zip file at top level include a file called system_prompt_model_name.txt that contains your system prompt used to extract the named entities.\n",
        "model_name should be replaced with the name of the model you used to extract the named entities, e.g. if model was google/gemini-2.5-flash the file name would be system_prompt_gemini_2.5_flash.txt \n",
        "\n",
        "A list of available models can be found at [OpenRouter](https://openrouter.ai/models).\n",
        "\n",
        "You have close to $1 credit on your API key, so pay attention to the model you choose and how many files you process. In general you have enough credit to process about 1000 files with medium cost models and about 100 files with high cost models. This should be enough to have some fun with testing different models and prompts.\n",
        "\n",
        "Submit what you think is the best result you can get with the model you choose. You can also try different models and prompts, but make sure to submit only one zip file with your best result.\n",
        "\n",
        "### Where to submit\n",
        "\n",
        "We will provide a link to Google Form where you can submit your zip file. The link will be provided in the workshop session.\n",
        "\n",
        "### Deadline\n",
        "\n",
        "We will provide a deadline for the submission of the project. The deadline will be announced during the workshop session.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1c50e58",
      "metadata": {},
      "source": [
        "## What else to do with responses?\n",
        "\n",
        "Now that we have our responses in multiple files, we could analyze them further which is beyond the scope of this workshop. We could plot the results in various ways such as time series bubble chart, perform manual analysis, count the number of occurrences of each concept, or even use another LLM to summarize the results.\n",
        "\n",
        "For the purposes of this workshop you do not have to submit your analysis, but you can do it for your own interest."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
