{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f0e89250",
      "metadata": {
        "id": "f0e89250"
      },
      "source": [
        "# Using LLMs in Humanities Research via API\n",
        "\n",
        "Welcome into deeper world of Large Language Models (LLMs) and their applications in humanities research! In an era where artificial intelligence is transforming every field of study, the humanities are experiencing a revolutionary shift in how we approach text analysis, interpretation, and research methodologies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4c557b7",
      "metadata": {
        "id": "f4c557b7"
      },
      "source": [
        "### Our Team: Instructors and Assistants\n",
        "\n",
        "**Valdis Saulespurƒìns** works as a researcher and developer at the National Library of Latvia. Additionally, he is a lecturer at Riga Technical University, where he teaches Python, JavaScript, and other computer science subjects. Valdis has a specialization in Machine Learning and Data Analysis, and he enjoys transforming disordered data into structured knowledge. With more than 30 years of programming experience, Valdis began his professional career by writing programs for quantum scientists at the University of California, Santa Barbara. Before moving into teaching, he developed software for a radio broadcast equipment manufacturer. Valdis holds a Master's degree in Computer Science from the University of Latvia.\n",
        "\n",
        "**Anda BaklƒÅne** is a researcher and curator of digital research services at the National Library of Latvia. She teaches Introduction to Digital Humanities and Digital Social Sciences and Text Analysis and Visualization courses at the University of Latvia. Anda holds a master's degree in philosophy and a PhD in literary theory. Her research interests include Latvian contemporary literature, metaphor, models, distant reading, and academic data visualization.\n",
        "\n",
        "**Viesturs Vƒìveris** is an analyst at the National Library of Latvia. With training in digital humanities and social sciences, he currently focuses on developing tools and methodologies for text analysis and data visualization.\n",
        "\n",
        "**Haralds Matulis** is a researcher and also organizer of this iteration of Baltic Summer School of Digital Humanities. He has a background in digital humanities and is interested in the intersection of technology and humanities research. Haralds is dedicated to promoting digital literacy and innovation in the humanities."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a25e61f",
      "metadata": {
        "id": "0a25e61f"
      },
      "source": [
        "### Why This Workshop Matters\n",
        "\n",
        "The digital transformation of humanities research has opened unprecedented opportunities for scholars to analyze vast corpora of text, uncover hidden patterns, and gain new insights into human culture and expression. Large Language Models represent the cutting edge of this transformation, offering powerful tools for:\n",
        "\n",
        "- **Automated text analysis** at scale previously impossible for human researchers\n",
        "- **Cross-lingual research** capabilities that break down language barriers\n",
        "- **Pattern recognition** in literary and historical texts\n",
        "- **Assistance with translation and transcription** of historical documents\n",
        "\n",
        "### Workshop Goals\n",
        "\n",
        "By the end of this three-session workshop, you will:\n",
        "\n",
        "1. **Understand the fundamentals** of Large Language Models and their capabilities for humanities research\n",
        "2. **Master API interactions** to programmatically access and utilize various LLM services\n",
        "3. **Learn practical applications** including concept mining, named entity recognition, and text analysis\n",
        "4. **Develop skills** in prompt engineering for humanities-specific tasks\n",
        "5. **Address real challenges** such as working with OCR errors in historical texts\n",
        "6. **Gain hands-on experience** with tools for error correction and translation\n",
        "7. **Build confidence** in integrating AI technologies into your research workflow\n",
        "\n",
        "### What Makes This Approach Special\n",
        "\n",
        "Rather than relying on simple chat interfaces, you'll learn to harness the full power of LLMs through API access, enabling:\n",
        "- **Batch processing** of large document collections\n",
        "- **Customizable workflows** tailored to your specific research needs\n",
        "- **Reproducible research** methods with documented processes\n",
        "- **Integration** with existing digital humanities tools and methodologies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3arv-b1RQ4yQ",
      "metadata": {
        "id": "3arv-b1RQ4yQ"
      },
      "source": [
        "## Session 1 (11.30-13.00) - Introduction to LLMs and APIs\n",
        "\n",
        "In our first session, we will explore the basics of Large Language Models (LLMs) and how to interact with them using APIs. We will cover the following topics:\n",
        "- **What are LLMs?**: An introduction to Large Language Models, their capabilities, and how they can be applied in humanities research.\n",
        "- **Setting Up Your Environment**: Instructions on how to set up your programming environment to interact with LLM APIs.\n",
        "- **Understanding APIs**: A brief overview of what APIs are, how they work, and why they are essential for accessing LLMs.\n",
        "- **Understanding JSON**: An introduction to JSON (JavaScript Object Notation), the data format commonly used for API responses, and how to work with it in Python.\n",
        "- **OpenRouter API**: Introduction to the OpenRouter API, which provides access to various LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e5b2d02",
      "metadata": {
        "id": "1e5b2d02"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7e87398b",
      "metadata": {
        "id": "7e87398b"
      },
      "source": [
        "## What are LLMs?\n",
        "\n",
        "**Large Language Models (LLMs)** are sophisticated artificial intelligence systems trained on vast collections of text data to understand, generate, and manipulate human language. Think of them as extremely well-read digital assistants that have absorbed millions of books, articles, websites, and documents, enabling them to engage with text in remarkably human-like ways.\n",
        "\n",
        "### How LLMs Work: The Basics\n",
        "\n",
        "LLMs use a technology called **transformer architecture** (you don't need to understand the technical details!) that allows them to:\n",
        "\n",
        "1. **Predict the next word** in a sequence based on context\n",
        "2. **Understand relationships** between words, sentences, and concepts\n",
        "3. **Generate coherent text** that follows patterns learned from training data\n",
        "4. **Transfer knowledge** from one domain to another\n",
        "\n",
        "### Key Terms for Digital Humanities\n",
        "\n",
        "#### **Training Data**\n",
        "The massive collection of texts used to teach the LLM. This typically includes:\n",
        "- Books and literature from various periods and cultures\n",
        "- Academic papers and journals\n",
        "- News articles and magazines\n",
        "- Web content and reference materials\n",
        "- **Important**: The quality and diversity of training data affects what the model \"knows\"\n",
        "\n",
        "#### **Tokens**\n",
        "The basic units of text that LLMs process. A token can be:\n",
        "- A whole word (\"humanities\")\n",
        "- Part of a word (\"human\" + \"ities\")\n",
        "- Punctuation marks\n",
        "- **Why it matters**: API costs are often calculated per token\n",
        "\n",
        "#### **Context Window**\n",
        "The amount of text an LLM can \"remember\" at once, measured in tokens. Common sizes:\n",
        "- **GPT-3.5**: ~4,000 tokens (‚âà3,000 words)\n",
        "- **GPT-4**: ~8,000-32,000 tokens\n",
        "- **Claude**: ~100,000+ tokens\n",
        "- **Why it matters**: Determines how much text you can analyze at once\n",
        "\n",
        "#### **Prompt**\n",
        "The input text you give to an LLM to get a response. Effective prompting is crucial for good results.\n",
        "\n",
        "#### **Fine-tuning**\n",
        "The process of further training a model on specific data to improve performance for particular tasks.\n",
        "\n",
        "### Applications in Digital Humanities\n",
        "\n",
        "#### **1. Text Analysis**\n",
        "- **Sentiment analysis** of historical documents\n",
        "- **Thematic analysis** across large corpora\n",
        "- **Stylometric analysis** for authorship attribution\n",
        "- **Content classification** and categorization\n",
        "\n",
        "#### **2. Language Processing**\n",
        "- **Translation** of historical texts\n",
        "- **Transcription** assistance for handwritten documents\n",
        "- **OCR error correction** in digitized materials\n",
        "- **Modernization** of archaic language\n",
        "\n",
        "#### **3. Research Assistance**\n",
        "- **Literature reviews** and source discovery\n",
        "- **Citation analysis** and bibliography generation\n",
        "- **Concept mapping** and knowledge extraction\n",
        "- **Hypothesis generation** from patterns in data\n",
        "\n",
        "#### **4. Content Generation**\n",
        "- **Metadata generation** for digital collections\n",
        "- **Summary creation** for large document sets\n",
        "- **Educational material** development\n",
        "- **Interactive exhibits** and digital storytelling\n",
        "\n",
        "### Limitations and Considerations\n",
        "\n",
        "#### **Accuracy Concerns**\n",
        "- LLMs can generate plausible but incorrect information (**hallucinations**)\n",
        "- Always verify important claims against primary sources\n",
        "- Use multiple models and cross-check results\n",
        "\n",
        "#### **Bias and Representation**\n",
        "- Training data reflects societal biases\n",
        "- May underrepresent certain cultures, languages, or perspectives\n",
        "- Critical evaluation is essential, especially for sensitive topics\n",
        "\n",
        "#### **Temporal Knowledge**\n",
        "- Models have knowledge cutoff dates\n",
        "- May not know about recent events or publications\n",
        "- Historical accuracy varies by period and region\n",
        "\n",
        "#### **Language Coverage**\n",
        "- Performance varies significantly across languages\n",
        "- Better results for well-represented languages (English, major European languages)\n",
        "- Limited effectiveness for minority or historical languages\n",
        "\n",
        "#### **Legal Considerations**\n",
        "- Legality of data acquisition for training models\n",
        "- Protection of user input; consider how your data is handled when using online AI applications\n",
        "\n",
        "### Popular LLM Models for Research\n",
        "\n",
        "#### **OpenAI's GPT Series**\n",
        "- **GPT-3.5**: Fast, cost-effective for many tasks\n",
        "- **GPT-4**: More capable, better reasoning, higher cost\n",
        "- **Strengths**: General knowledge, writing quality\n",
        "- **Best for**: Text generation, analysis, general research tasks\n",
        "\n",
        "#### **Anthropic's Claude**\n",
        "- **Claude-3**: Various sizes (Haiku, Sonnet, Opus)\n",
        "- **Strengths**: Large context windows, careful reasoning\n",
        "- **Best for**: Long document analysis, ethical considerations\n",
        "\n",
        "#### **Google's Gemini**\n",
        "- **Gemini Pro**: Competitive with GPT-4\n",
        "- **Strengths**: Multimodal capabilities, integration with Google services\n",
        "- **Best for**: Research integration, document processing\n",
        "\n",
        "#### **Open Source Models**\n",
        "- **Llama 2/3**: Meta's open-source models\n",
        "- **Mistral**: European open-source alternative\n",
        "- **Benefits**: Transparency, customization, data privacy\n",
        "\n",
        "### Getting Started: Questions to Ask\n",
        "\n",
        "Before using LLMs in your research, consider:\n",
        "\n",
        "1. **What specific task** do you want to accomplish?\n",
        "2. **How much text** will you be processing?\n",
        "3. **What level of accuracy** do you need?\n",
        "4. **Are there privacy concerns** with your data?\n",
        "5. **What's your budget** for API usage?\n",
        "6. **Do you need real-time results** or can processing take time?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jCJqEhHdnqln",
      "metadata": {
        "id": "jCJqEhHdnqln"
      },
      "source": [
        "### Usage of LLMs at the National Library of Latvia: Three Examples\n",
        "\n",
        "- **OCR and normalization of Latvian old script**\n",
        "\n",
        "OCR quality for historical newspapers printed in Fraktur script is often poor in materials digitized in earlier years. LLMs have shown limited effectiveness in correcting when preservation of original, non-contemporary glyphs is required. However, they are very effective when the goal is normalization ‚Äî transforming the text into contemporary script rather than reproducing the original form. The overcorrection and hallucinations are present but the number of errors is lower compared to historical OCR.\n",
        "\n",
        "<img src=\"https://github.com/ValRCS/BSSDH_2025_workshop_LLM_API/blob/main/img/fraktur-normalization.JPG?raw=true\" alt=\"Original OCR (left); processed with Gemini 2.5 Flash Preview\" width=\"500\">\n",
        "\n",
        "- **Building complex, topic-based corpora: Corpus of Latvian Music Texts**\n",
        "\n",
        "Keyword-based methods are commonly used to build topic-specific datasets from databases. When keywords have broad or ambiguous meanings, this approach often results in high rates of false positives that can exceed 50% of total data. LLMs can help address this issue by providing more context-aware filtering.\n",
        "\n",
        "<img src=\"https://github.com/ValRCS/BSSDH_2025_workshop_LLM_API/blob/main/img/system-prompt-music.JPG?raw=true\" alt=\"Example of the prompt\" width=\"500\">\n",
        "\n",
        "- **Extracting semantically related terms: Transportation in Latvian novels**\n",
        "\n",
        "LLM-based extraction from 460 novels identified approximately 160 valid land transportation term types - double the number produced by other methods, which yielded around 80 terms. Testing with GPT-4.0, Gemini 1.5, and Gemini 2.0 confirmed that while this approach generates more useful terms, precision varies significantly, ranging from 46% to 76%. In some cases, more that half of the terms were either hallucinations or were only loosely related\n",
        "to commonly accepted definitions of \"land vehicle\". Full text: 10.26083/tuprints-00030146\n",
        "\n",
        "![Average relative document frequency of terms referring to motorized and horsedrawn vehicles](https://github.com/ValRCS/BSSDH_2025_workshop_LLM_API/blob/main/img/Fig2-relative.png?raw=true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02ce4708",
      "metadata": {
        "id": "02ce4708"
      },
      "source": [
        "## Interactive Version of the Notebook\n",
        "\n",
        "### Open in Google Colab\n",
        "<a href=\"https://colab.research.google.com/github/ValRCS/BSSDH_2025_workshop_LLM_API/blob/main/notebooks/workshop_session_1.ipynb?flush_cache=true\" target=\"_blank\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "### Static vs Interactive Notebooks\n",
        "\n",
        "**Static Notebooks** (like what you might see on GitHub) **are read-only versions** that display the content but don't allow you to execute code cells, modify content, install packages, or save your changes.\n",
        "\n",
        "**Interactive Notebooks** allow you to:\n",
        "- **Execute code cells** by pressing Shift+Enter or clicking the play button\n",
        "- **Edit and experiment** with code in real-time\n",
        "- **Install Python packages** as needed\n",
        "- **Save your work** and download modified notebooks\n",
        "- **See live outputs** including text, tables, and visualizations\n",
        "\n",
        "### About Google Colab\n",
        "\n",
        "**Google Colab** (Colaboratory) is a free, cloud-based Jupyter notebook environment that:\n",
        "\n",
        "- **Requires no setup** - runs entirely in your web browser\n",
        "- **Provides free computational resources** including CPU, GPU, and limited TPU access\n",
        "- **Comes pre-installed** with most common data science and machine learning libraries\n",
        "- **Integrates seamlessly** with Google Drive for saving and sharing notebooks\n",
        "- **Supports real-time collaboration** allowing multiple people to work on the same notebook\n",
        "- **Automatically saves** your progress to Google Drive\n",
        "\n",
        "**Getting Started with Colab:**\n",
        "1. Click the \"Open in Colab\" badge above\n",
        "2. Sign in with your Google account (required)\n",
        "3. The notebook will open in a new tab\n",
        "4. You can immediately start executing cells by clicking the play button (‚ñ∂Ô∏è) or pressing Shift+Enter\n",
        "\n",
        "**üí° Pro Tip:** Right-click the Colab badge and select \"Open link in new tab\" to keep this reference page open while working in the interactive notebook!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd2605c0",
      "metadata": {
        "id": "bd2605c0"
      },
      "source": [
        "## Setting Up Your Environment\n",
        "\n",
        "To interact with LLM APIs effectively, we need to set up our programming environment with the necessary libraries and configurations. This includes installing required packages and setting up API credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0354b1f",
      "metadata": {
        "id": "e0354b1f",
        "outputId": "b879eb97-269b-42ff-891c-5547722aee7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is an interactive notebook for the BSSDH 2025 workshop on LLMs and APIs.\n",
            "Python version: 3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]\n",
            "Today's date and time: 2025-07-30 12:01:19.490619\n",
            "JSON module imported successfully.\n",
            "Path from pathlib imported successfully.\n",
            "Will import external libraries if available.\n",
            "Requests library version: 2.32.4\n",
            "TQDM library version: 4.67.1\n",
            "OpenAI library version: 1.97.1\n"
          ]
        }
      ],
      "source": [
        "# Let's print some basic information about this interactive notebook\n",
        "print(\"This is an interactive notebook for the BSSDH 2025 workshop on LLMs and APIs.\")\n",
        "# first let's see what Python version we are using\n",
        "import sys\n",
        "print(f\"Python version: {sys.version}\")\n",
        "# now today's date and time\n",
        "from datetime import datetime\n",
        "print(f\"Today's date and time: {datetime.now()}\")\n",
        "# we will need to work with JSON data, so let's import the json module\n",
        "import json\n",
        "print(\"JSON module imported successfully.\")\n",
        "# we will need to read and write files so let's import pathlib\n",
        "from pathlib import Path\n",
        "print(\"Path from pathlib imported successfully.\")\n",
        "# TODO for those with some experience it can be useful to print more information about the environment, free memory, drives, etc.\n",
        "print(\"Will import external libraries if available.\")\n",
        "# Let's also check if we have the requests library installed, which is commonly used for making API calls\n",
        "try:\n",
        "    import requests\n",
        "    print(f\"Requests library version: {requests.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"Requests library is not installed. You can install it using 'pip install requests'.\")\n",
        "\n",
        "# let's install tqdm for progress bars if not already installed\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "    # import version\n",
        "    from tqdm import __version__ as tqdm_version\n",
        "    print(f\"TQDM library version: {tqdm_version}\")\n",
        "except ImportError:\n",
        "    print(\"TQDM library is not installed. You can install it using 'pip install tqdm'.\")\n",
        "\n",
        "# now let's try importing OpenAI's library if available\n",
        "try:\n",
        "    import openai\n",
        "    print(f\"OpenAI library version: {openai.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"OpenAI library is not installed. You can install it using 'pip install openai'.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dad0ead",
      "metadata": {
        "id": "6dad0ead"
      },
      "source": [
        "### Why Check System Information and Library Versions?\n",
        "\n",
        "**Environment Documentation** is crucial for reproducible research and troubleshooting. Here's why we print this information:\n",
        "\n",
        "#### **1. Reproducibility**\n",
        "- **Version consistency**: Different library versions can produce different results\n",
        "- **Environment documentation**: Future researchers (including yourself) can recreate the exact same setup\n",
        "- **Research integrity**: Ensures your findings can be validated by others\n",
        "\n",
        "#### **2. Troubleshooting**\n",
        "- **Debugging assistance**: When code doesn't work, version information helps identify compatibility issues\n",
        "- **Support requests**: Technical support often requires knowing your exact environment setup\n",
        "- **Error diagnosis**: Many errors are version-specific and can be quickly resolved with this information\n",
        "\n",
        "#### **3. Best Practices in Digital Humanities**\n",
        "- **Methodological transparency**: Document all tools and versions used in your research\n",
        "- **Collaboration**: Team members can ensure they're using compatible environments\n",
        "- **Publication standards**: Many journals now require detailed technical specifications\n",
        "\n",
        "#### **4. API Compatibility**\n",
        "- **Service requirements**: Different LLM APIs may require specific library versions\n",
        "- **Feature availability**: Newer features might only be available in recent library versions\n",
        "- **Security updates**: Ensures you're using libraries with the latest security patches\n",
        "\n",
        "**üí° Pro Tip**: Always run this environment check at the beginning of your research sessions to catch any changes that might affect your results!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2be804e",
      "metadata": {
        "id": "a2be804e"
      },
      "source": [
        "## Understanding APIs\n",
        "\n",
        "**API (Application Programming Interface)** is a set of rules and protocols that allows different software applications to communicate with each other. Think of an API as a digital messenger that takes your request, tells a system what you want, and then brings the response back to you in a structured format.\n",
        "\n",
        "### The Restaurant Analogy\n",
        "\n",
        "Imagine you're at a restaurant:\n",
        "- **You** (the client) want to order food\n",
        "- **The kitchen** (the server) prepares the food\n",
        "- **The waiter** (the API) takes your order to the kitchen and brings your food back\n",
        "\n",
        "In the digital world:\n",
        "- **Your Python script** (the client) wants data or a service\n",
        "- **The LLM service** (the server) processes your request\n",
        "- **The API** takes your request and returns the results\n",
        "\n",
        "### Why APIs Matter for Digital Humanities\n",
        "\n",
        "#### **1. Programmatic Access**\n",
        "Instead of manually copying and pasting text into web interfaces:\n",
        "- **Batch processing**: Analyze hundreds of documents automatically\n",
        "- **Consistency**: Same processing applied to all texts\n",
        "- **Reproducibility**: Document and repeat your exact methods\n",
        "- **Efficiency**: Save hours or days of manual work\n",
        "\n",
        "#### **2. Integration with Research Workflows**\n",
        "- **Combine with existing tools**: Integrate with databases, spreadsheets, visualization software\n",
        "- **Custom analysis pipelines**: Build workflows tailored to your specific research questions\n",
        "- **Data preservation**: Keep detailed logs of all processing steps\n",
        "- **Scalability**: Handle projects ranging from single documents to massive corpora\n",
        "\n",
        "### Key API Concepts\n",
        "\n",
        "#### **HTTP Methods**\n",
        "APIs use standard web protocols:\n",
        "- **GET**: Retrieve information (like downloading a file)\n",
        "- **POST**: Send data for processing (like submitting a form)\n",
        "- **PUT**: Update existing data\n",
        "- **DELETE**: Remove data\n",
        "\n",
        "For LLM APIs, we primarily use **POST** to send text for analysis.\n",
        "\n",
        "#### **Request and Response**\n",
        "Every API interaction involves:\n",
        "1. **Request**: What you send to the API\n",
        "   - URL (endpoint)\n",
        "   - Headers (metadata like authorization)\n",
        "   - Body (your actual data/text)\n",
        "2. **Response**: What the API sends back\n",
        "   - Status code (200 = success, 404 = not found, etc.)\n",
        "   - Data (usually in JSON format)\n",
        "\n",
        "#### **Authentication**\n",
        "Most APIs require proof of identity:\n",
        "- **API Keys**: Secret strings that identify you\n",
        "- **Tokens**: Temporary credentials with specific permissions\n",
        "- **Rate Limits**: Restrictions on how many requests you can make\n",
        "\n",
        "### API Anatomy for LLM Services\n",
        "\n",
        "#### **Base URL**\n",
        "The main address of the API service:\n",
        "```\n",
        "https://openrouter.ai/api/v1/\n",
        "```\n",
        "\n",
        "#### **Endpoints**\n",
        "Specific functions within the API:\n",
        "```\n",
        "/chat/completions  # For sending messages to LLMs\n",
        "/models           # List available models\n",
        "/usage            # Check your usage statistics\n",
        "```\n",
        "\n",
        "#### **Complete URL**\n",
        "```\n",
        "https://openrouter.ai/api/v1/chat/completions\n",
        "```\n",
        "\n",
        "### Headers: The API's Metadata\n",
        "\n",
        "Headers provide essential information about your request:\n",
        "\n",
        "```python\n",
        "headers = {\n",
        "    \"Authorization\": \"Bearer YOUR_API_KEY\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"HTTP-Referer\": \"https://your-research-project.edu\",\n",
        "    \"X-Title\": \"Digital Humanities Text Analysis\"\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Common Headers Explained**\n",
        "- **Authorization**: Proves you're allowed to use the service\n",
        "- **Content-Type**: Tells the API what format your data is in\n",
        "- **HTTP-Referer**: (Optional) Identifies your project for tracking\n",
        "- **X-Title**: (Optional) Describes your application\n",
        "\n",
        "### Request Body: Your Actual Data\n",
        "\n",
        "The request body contains your instructions and text:\n",
        "\n",
        "```python\n",
        "request_body = {\n",
        "    \"model\": \"openai/gpt-3.5-turbo\",\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Analyze the sentiment of this historical document: [your text here]\"\n",
        "        }\n",
        "    ],\n",
        "    \"max_tokens\": 1000,\n",
        "    \"temperature\": 0.1\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Key Parameters**\n",
        "- **model**: Which LLM to use\n",
        "- **messages**: Your conversation with the AI\n",
        "- **max_tokens**: Maximum length of response\n",
        "- **temperature**: Creativity level (0 = deterministic, 1 = creative)\n",
        "\n",
        "### Common API Response Formats\n",
        "\n",
        "#### **Successful Response (Status 200)**\n",
        "```json\n",
        "{\n",
        "  \"id\": \"chatcmpl-123\",\n",
        "  \"object\": \"chat.completion\",\n",
        "  \"created\": 1677652288,\n",
        "  \"model\": \"openai/gpt-3.5-turbo\",\n",
        "  \"usage\": {\n",
        "    \"prompt_tokens\": 56,\n",
        "    \"completion_tokens\": 31,\n",
        "    \"total_tokens\": 87\n",
        "  },\n",
        "  \"choices\": [\n",
        "    {\n",
        "      \"message\": {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"This historical document expresses predominantly negative sentiment regarding the economic policies...\"\n",
        "      },\n",
        "      \"finish_reason\": \"stop\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Error Response (Status 400+)**\n",
        "```json\n",
        "{\n",
        "  \"error\": {\n",
        "    \"message\": \"You exceeded your rate limit\",\n",
        "    \"type\": \"rate_limit_exceeded\",\n",
        "    \"code\": \"rate_limit_exceeded\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Digital Humanities Use Cases\n",
        "\n",
        "#### **1. Manuscript Analysis**\n",
        "```python\n",
        "# Analyze medieval manuscript style\n",
        "request = {\n",
        "    \"model\": \"openai/gpt-4\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"Identify the literary devices in this Middle English text: [manuscript text]\"}\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "#### **2. Historical Document Processing**\n",
        "```python\n",
        "# Extract entities from historical records\n",
        "request = {\n",
        "    \"model\": \"anthropic/claude-3-sonnet\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"Extract all person names, places, and dates from this 18th-century letter: [letter text]\"}\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "#### **3. Comparative Literature**\n",
        "```python\n",
        "# Compare themes across texts\n",
        "request = {\n",
        "    \"model\": \"meta-llama/llama-2-70b-chat\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"Compare the themes of exile in these two poems: [poem 1] vs [poem 2]\"}\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "### Best Practices for Research\n",
        "\n",
        "#### **1. Documentation**\n",
        "- **Log all API calls**: Keep records of what models and parameters you used\n",
        "- **Version control**: Track changes to your analysis methods\n",
        "- **Reproducible scripts**: Write code that others can run and verify\n",
        "\n",
        "#### **2. Error Handling**\n",
        "```python\n",
        "import requests\n",
        "\n",
        "try:\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    response.raise_for_status()  # Raises an exception for bad status codes\n",
        "    result = response.json()\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"API request failed: {e}\")\n",
        "```\n",
        "\n",
        "#### **3. Rate Limiting and Costs**\n",
        "- **Respect rate limits**: Don't overwhelm the service\n",
        "- **Monitor usage**: Track your API costs\n",
        "- **Batch efficiently**: Group similar requests when possible\n",
        "\n",
        "#### **4. Data Privacy**\n",
        "- **Sensitive data**: Be cautious with personal or confidential historical materials\n",
        "- **Institutional policies**: Check your institution's data use guidelines\n",
        "- **Terms of service**: Understand how API providers handle your data\n",
        "\n",
        "### Popular APIs for Digital Humanities\n",
        "\n",
        "#### **LLM APIs**\n",
        "- **OpenRouter**: Access to multiple models through one interface\n",
        "- **OpenAI API**: Direct access to GPT models\n",
        "- **Anthropic API**: Claude models with large context windows\n",
        "- **Hugging Face API**: Open-source models\n",
        "\n",
        "#### **Complementary APIs**\n",
        "- **Google Books API**: Access to digitized books\n",
        "- **DPLA API**: Digital Public Library of America\n",
        "- **Europeana API**: European cultural heritage\n",
        "- **Archive.org API**: Internet Archive materials\n",
        "\n",
        "### Security Considerations\n",
        "\n",
        "#### **API Key Management**\n",
        "- **Never commit keys to version control**\n",
        "- **Use environment variables** to store sensitive information\n",
        "- **Rotate keys regularly**\n",
        "- **Limit key permissions** where possible\n",
        "\n",
        "#### **Example: Secure Key Storage**\n",
        "```python\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Safely access your API key\n",
        "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"API key not found in environment variables\")\n",
        "```\n",
        "\n",
        "### Testing and Development\n",
        "\n",
        "#### **Start Small**\n",
        "1. **Test with short texts** before processing large corpora\n",
        "2. **Use cheaper models** for initial experiments\n",
        "3. **Validate outputs** with known examples\n",
        "4. **Compare multiple models** for the same task\n",
        "\n",
        "#### **API Testing Tools**\n",
        "- **Postman**: Visual interface for testing API calls\n",
        "- **curl**: Command-line tool for simple tests\n",
        "- **Python requests library**: For programmatic testing\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "Understanding APIs is crucial because they provide:\n",
        "- **Systematic access** to powerful LLM capabilities\n",
        "- **Integration possibilities** with your existing research tools\n",
        "- **Scalability** for large-scale text analysis projects\n",
        "- **Reproducibility** essential for academic research\n",
        "\n",
        "In the next section, we'll explore JSON format, which is how APIs structure the data they send and receive."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "794ceaeb",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "source": [
        "## Understanding JSON\n",
        "\n",
        "JSON (JavaScript Object Notation) is a lightweight data format commonly used for API responses. It's human-readable and easy to work with in Python, making it ideal for handling structured data from LLM APIs.\n",
        "\n",
        "Official JSON website: [json.org](https://www.json.org/)\n",
        "\n",
        "### A Brief History of JSON\n",
        "\n",
        "#### **Origins (2001-2005)**\n",
        "- **Created by Douglas Crockford** in 2001 while working at State Software - later moved to Yahoo! one of early adopters\n",
        "- **Originally designed** as a data exchange format for web applications\n",
        "- **Name derivation**: \"JavaScript Object Notation\" because it uses JavaScript object syntax\n",
        "- **Problem it solved**: Need for a lightweight alternative to XML for AJAX applications\n",
        "\n",
        "#### **Early Adoption (2005-2010)**\n",
        "- **2005**: First JSON specification published\n",
        "- **Web 2.0 era**: Became popular with rise of dynamic web applications\n",
        "- **AJAX revolution**: JSON enabled faster, more efficient data exchange\n",
        "- **Language support**: Libraries developed for Python, Java, C#, and other languages\n",
        "\n",
        "#### **Standardization (2010-present)**\n",
        "- **2013**: RFC 7159 established JSON as an internet standard\n",
        "- **2017**: Updated standard (RFC 8259) clarified specifications\n",
        "- **Current status**: De facto standard for web APIs and data exchange\n",
        "- **Universal adoption**: Supported by virtually every programming language\n",
        "\n",
        "### Why JSON Became Dominant\n",
        "\n",
        "#### **Advantages over XML**\n",
        "- **Lighter weight**: Less verbose, smaller file sizes\n",
        "- **Faster parsing**: Simpler structure means quicker processing\n",
        "- **Human readable**: Easy to read and debug\n",
        "- **Native JavaScript support**: No additional parsing needed in browsers\n",
        "\n",
        "#### **Comparison Example**\n",
        "**XML Version:**\n",
        "```xml\n",
        "<book>\n",
        "  <title>Digital Humanities Methods</title>\n",
        "  <author>Jane Smith</author>\n",
        "  <year>2024</year>\n",
        "  <topics>\n",
        "    <topic>Text Analysis</topic>\n",
        "    <topic>Data Visualization</topic>\n",
        "  </topics>\n",
        "</book>\n",
        "```\n",
        "\n",
        "**JSON Version:**\n",
        "```json\n",
        "{\n",
        "  \"title\": \"Digital Humanities Methods\",\n",
        "  \"author\": \"Jane Smith\",\n",
        "  \"year\": 2024,\n",
        "  \"topics\": [\"Text Analysis\", \"Data Visualization\"]\n",
        "}\n",
        "```\n",
        "\n",
        "### JSON Syntax: Complete Guide\n",
        "\n",
        "#### **Basic Structure Rules**\n",
        "1. **Data is in name/value pairs**\n",
        "2. **Data is separated by commas**\n",
        "3. **Curly braces hold objects**\n",
        "4. **Square brackets hold arrays**\n",
        "5. **Strings must use double quotes**\n",
        "\n",
        "#### **Data Types**\n",
        "\n",
        "##### **1. Strings**\n",
        "- Must be enclosed in **double quotes** (not single quotes)\n",
        "- Can contain Unicode characters\n",
        "- Escape sequences supported\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"simple_string\": \"Hello World\",\n",
        "  \"unicode_string\": \"Latvian: Sveika, pasaule! üá±üáª\",\n",
        "  \"escaped_string\": \"Quote: \\\"Hello\\\" and newline: \\n\",\n",
        "  \"empty_string\": \"\"\n",
        "}\n",
        "```\n",
        "\n",
        "##### **2. Numbers**\n",
        "- Integer or floating point\n",
        "- No leading zeros (except for decimal numbers)\n",
        "- Scientific notation supported\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"integer\": 42,\n",
        "  \"negative\": -17,\n",
        "  \"float\": 3.14159,\n",
        "  \"scientific\": 1.23e-10,\n",
        "  \"zero\": 0\n",
        "}\n",
        "```\n",
        "\n",
        "##### **3. Booleans**\n",
        "- Only `true` or `false` (lowercase)\n",
        "- No other boolean representations\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"is_published\": true,\n",
        "  \"is_draft\": false\n",
        "}\n",
        "```\n",
        "\n",
        "##### **4. Null**\n",
        "- Represents empty value\n",
        "- Written as `null` (lowercase)\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"optional_field\": null,\n",
        "  \"missing_data\": null\n",
        "}\n",
        "```\n",
        "\n",
        "##### **5. Objects**\n",
        "- Collections of key/value pairs\n",
        "- Keys must be strings in double quotes\n",
        "- Values can be any JSON data type\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"researcher\": {\n",
        "    \"name\": \"Dr. Anda BaklƒÅne\",\n",
        "    \"institution\": \"National Library of Latvia\",\n",
        "    \"specialization\": \"Digital Humanities\",\n",
        "    \"contact\": {\n",
        "      \"email\": \"anda.baklane@lnb.lv\",\n",
        "      \"phone\": null\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "##### **6. Arrays**\n",
        "- Ordered lists of values\n",
        "- Values can be any JSON data type (mixed types allowed)\n",
        "- Zero-indexed\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"research_topics\": [\n",
        "    \"Text Analysis\",\n",
        "    \"Data Visualization\", \n",
        "    \"Machine Learning\"\n",
        "  ],\n",
        "  \"mixed_array\": [\n",
        "    \"string\",\n",
        "    42,\n",
        "    true,\n",
        "    null,\n",
        "    {\"nested\": \"object\"},\n",
        "    [1, 2, 3]\n",
        "  ],\n",
        "  \"empty_array\": []\n",
        "}\n",
        "```\n",
        "\n",
        "#### **Nesting and Complex Structures**\n",
        "\n",
        "JSON supports unlimited nesting of objects and arrays:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"digital_humanities_project\": {\n",
        "    \"title\": \"Latvian Literature Analysis\",\n",
        "    \"metadata\": {\n",
        "      \"created\": \"2025-01-15\",\n",
        "      \"version\": \"1.2\",\n",
        "      \"authors\": [\n",
        "        {\n",
        "          \"name\": \"Valdis Saulespurƒìns\",\n",
        "          \"role\": \"Lead Developer\",\n",
        "          \"skills\": [\"Python\", \"Machine Learning\", \"APIs\"]\n",
        "        },\n",
        "        {\n",
        "          \"name\": \"Anda BaklƒÅne\", \n",
        "          \"role\": \"Research Lead\",\n",
        "          \"skills\": [\"Literary Theory\", \"Text Analysis\", \"Data Visualization\"]\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    \"datasets\": [\n",
        "      {\n",
        "        \"name\": \"19th Century Novels\",\n",
        "        \"size\": 1200,\n",
        "        \"languages\": [\"Latvian\", \"German\"],\n",
        "        \"analysis_results\": {\n",
        "          \"sentiment_scores\": [0.65, 0.72, 0.58],\n",
        "          \"themes\": {\n",
        "            \"exile\": 0.34,\n",
        "            \"identity\": 0.78,\n",
        "            \"nationalism\": 0.45\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### JSON in Digital Humanities Context\n",
        "\n",
        "#### **Research Metadata Example**\n",
        "```json\n",
        "{\n",
        "  \"manuscript_analysis\": {\n",
        "    \"document_id\": \"LNB-MS-1847-034\",\n",
        "    \"title\": \"Personal Letters of Kri≈°jƒÅnis Barons\",\n",
        "    \"date_created\": \"1847-03-15\",\n",
        "    \"language\": \"Latvian\",\n",
        "    \"script\": \"Gothic\",\n",
        "    \"digitization\": {\n",
        "      \"scan_date\": \"2023-08-15\",\n",
        "      \"resolution\": \"600dpi\",\n",
        "      \"format\": \"TIFF\",\n",
        "      \"ocr_confidence\": 0.87\n",
        "    },\n",
        "    \"analysis_results\": {\n",
        "      \"entities\": {\n",
        "        \"persons\": [\"Kri≈°jƒÅnis Barons\", \"Anna Barone\"],\n",
        "        \"places\": [\"Rƒ´ga\", \"Jelgava\", \"Dundaga\"],\n",
        "        \"dates\": [\"1847-03-15\", \"1847-04-02\"]\n",
        "      },\n",
        "      \"themes\": [\n",
        "        {\"theme\": \"family_relations\", \"confidence\": 0.95},\n",
        "        {\"theme\": \"folklore_collection\", \"confidence\": 0.78},\n",
        "        {\"theme\": \"cultural_identity\", \"confidence\": 0.82}\n",
        "      ],\n",
        "      \"sentiment\": {\n",
        "        \"overall\": \"positive\",\n",
        "        \"score\": 0.73,\n",
        "        \"emotions\": {\n",
        "          \"joy\": 0.45,\n",
        "          \"nostalgia\": 0.67,\n",
        "          \"concern\": 0.23\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "#### **LLM API Request Example**\n",
        "```json\n",
        "{\n",
        "  \"model\": \"openai/gpt-4\",\n",
        "  \"messages\": [\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You are a digital humanities expert specializing in 19th-century Latvian literature.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\", \n",
        "      \"content\": \"Analyze the following text excerpt for themes of exile and identity: [text content here]\"\n",
        "    }\n",
        "  ],\n",
        "  \"max_tokens\": 1000,\n",
        "  \"temperature\": 0.1,\n",
        "  \"metadata\": {\n",
        "    \"research_project\": \"Baltic Literary Themes\",\n",
        "    \"researcher\": \"BSSDH Workshop Participant\",\n",
        "    \"date\": \"2025-08-03\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Common JSON Errors and How to Avoid Them\n",
        "\n",
        "#### **1. Syntax Errors**\n",
        "```json\n",
        "// ‚ùå WRONG - Single quotes\n",
        "{ 'author': 'Jane Smith' }\n",
        "\n",
        "// ‚úÖ CORRECT - Double quotes\n",
        "{ \"author\": \"Jane Smith\" }\n",
        "\n",
        "// ‚ùå WRONG - Trailing comma\n",
        "{\n",
        "  \"title\": \"Book\",\n",
        "  \"year\": 2024,\n",
        "}\n",
        "\n",
        "// ‚úÖ CORRECT - No trailing comma\n",
        "{\n",
        "  \"title\": \"Book\", \n",
        "  \"year\": 2024\n",
        "}\n",
        "\n",
        "// ‚ùå WRONG - Comments (not allowed in strict JSON)\n",
        "{\n",
        "  \"title\": \"Book\", // This is a comment\n",
        "  \"year\": 2024\n",
        "}\n",
        "\n",
        "// ‚úÖ CORRECT - No comments\n",
        "{\n",
        "  \"title\": \"Book\",\n",
        "  \"year\": 2024\n",
        "}\n",
        "```\n",
        "\n",
        "#### **2. Data Type Errors**\n",
        "```json\n",
        "// ‚ùå WRONG - Undefined values\n",
        "{\n",
        "  \"value\": undefined\n",
        "}\n",
        "\n",
        "// ‚úÖ CORRECT - Use null for missing values\n",
        "{\n",
        "  \"value\": null\n",
        "}\n",
        "\n",
        "// ‚ùå WRONG - Functions (not valid JSON)\n",
        "{\n",
        "  \"calculate\": function() { return 42; }\n",
        "}\n",
        "\n",
        "// ‚úÖ CORRECT - Only data, no functions\n",
        "{\n",
        "  \"result\": 42\n",
        "}\n",
        "```\n",
        "\n",
        "### Working with JSON in Python\n",
        "\n",
        "#### **Basic Operations**\n",
        "```python\n",
        "import json\n",
        "\n",
        "# Creating JSON from Python data\n",
        "data = {\n",
        "    \"title\": \"Digital Humanities Research\",\n",
        "    \"authors\": [\"Valdis\", \"Anda\"],\n",
        "    \"published\": True,\n",
        "    \"year\": 2025\n",
        "}\n",
        "\n",
        "# Convert to JSON string\n",
        "json_string = json.dumps(data)\n",
        "print(json_string)\n",
        "\n",
        "# Convert back to Python object\n",
        "parsed_data = json.loads(json_string)\n",
        "print(parsed_data[\"title\"])\n",
        "```\n",
        "\n",
        "#### **Pretty Printing**\n",
        "```python\n",
        "# Format JSON nicely\n",
        "pretty_json = json.dumps(data, indent=2, ensure_ascii=False)\n",
        "print(pretty_json)\n",
        "```\n",
        "\n",
        "#### **Reading/Writing JSON Files**\n",
        "```python\n",
        "# Write to file\n",
        "with open('research_data.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# Read from file\n",
        "with open('research_data.json', 'r', encoding='utf-8') as f:\n",
        "    loaded_data = json.load(f)\n",
        "```\n",
        "\n",
        "#### **Handling API Responses**\n",
        "```python\n",
        "import requests\n",
        "\n",
        "response = requests.post(api_url, headers=headers, json=request_data)\n",
        "if response.status_code == 200:\n",
        "    result = response.json()  # Automatically parses JSON\n",
        "    content = result['choices'][0]['message']['content']\n",
        "    print(content)\n",
        "```\n",
        "\n",
        "### JSON Validation and Tools\n",
        "\n",
        "#### **Online Validators**\n",
        "- **JSONLint**: [jsonlint.com](https://jsonlint.com) - Validate and format JSON\n",
        "- **JSON Formatter**: [jsonformatter.org](https://jsonformatter.org) - Format and validate\n",
        "- **JSON Schema Validator**: Validate against specific schemas\n",
        "\n",
        "#### **Python Validation**\n",
        "```python\n",
        "import json\n",
        "\n",
        "def validate_json(json_string):\n",
        "    try:\n",
        "        json.loads(json_string)\n",
        "        return True, \"Valid JSON\"\n",
        "    except json.JSONDecodeError as e:\n",
        "        return False, f\"Invalid JSON: {e}\"\n",
        "\n",
        "# Test validation\n",
        "test_json = '{\"name\": \"Valdis\", \"role\": \"instructor\"}'\n",
        "is_valid, message = validate_json(test_json)\n",
        "print(f\"Valid: {is_valid}, Message: {message}\")\n",
        "```\n",
        "\n",
        "### JSON Schema for Data Validation\n",
        "\n",
        "For research projects, you can define schemas to ensure data consistency:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
        "  \"type\": \"object\",\n",
        "  \"properties\": {\n",
        "    \"document_id\": {\n",
        "      \"type\": \"string\",\n",
        "      \"pattern\": \"^[A-Z]{3}-[A-Z]{2}-[0-9]{4}-[0-9]{3}$\"\n",
        "    },\n",
        "    \"analysis_date\": {\n",
        "      \"type\": \"string\",\n",
        "      \"format\": \"date\"\n",
        "    },\n",
        "    \"confidence_score\": {\n",
        "      \"type\": \"number\",\n",
        "      \"minimum\": 0,\n",
        "      \"maximum\": 1\n",
        "    }\n",
        "  },\n",
        "  \"required\": [\"document_id\", \"analysis_date\"]\n",
        "}\n",
        "```\n",
        "\n",
        "### Best Practices for Digital Humanities\n",
        "\n",
        "#### **1. Consistent Structure**\n",
        "- Use consistent naming conventions (snake_case or camelCase)\n",
        "- Maintain consistent data types across similar fields\n",
        "- Document your JSON structure for team members\n",
        "\n",
        "#### **2. Meaningful Keys**\n",
        "```json\n",
        "// ‚ùå Unclear\n",
        "{\"d\": \"2025-08-03\", \"a\": \"Valdis\", \"s\": 0.85}\n",
        "\n",
        "// ‚úÖ Clear and descriptive\n",
        "{\n",
        "  \"analysis_date\": \"2025-08-03\",\n",
        "  \"analyst\": \"Valdis\", \n",
        "  \"confidence_score\": 0.85\n",
        "}\n",
        "```\n",
        "\n",
        "#### **3. Version Your Data Formats**\n",
        "```json\n",
        "{\n",
        "  \"format_version\": \"1.2\",\n",
        "  \"created_by\": \"BSSDH Workshop Tools\",\n",
        "  \"data\": {\n",
        "    // Your actual research data\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "#### **4. Include Metadata**\n",
        "```json\n",
        "{\n",
        "  \"metadata\": {\n",
        "    \"created\": \"2025-08-03T10:30:00Z\",\n",
        "    \"tool\": \"GPT-4 via OpenRouter\",\n",
        "    \"researcher\": \"Workshop Participant\",\n",
        "    \"institution\": \"Baltic Summer School of Digital Humanities\"\n",
        "  },\n",
        "  \"analysis_results\": {\n",
        "    // Your analysis data\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Current State and Future of JSON\n",
        "\n",
        "#### **Current Usage (2025)**\n",
        "- **Dominant format** for REST APIs and web services\n",
        "- **Standard format** for configuration files in many tools\n",
        "- **Primary format** for NoSQL databases (MongoDB, CouchDB)\n",
        "- **Essential skill** for data science and digital humanities\n",
        "\n",
        "#### **Alternatives and Competitors**\n",
        "- **YAML**: More human-readable, used for configuration\n",
        "- **TOML**: Growing popularity for configuration files\n",
        "- **Protocol Buffers**: Google's binary format for performance\n",
        "- **MessagePack**: Binary format that's more compact than JSON\n",
        "\n",
        "#### **JSON's Continued Relevance**\n",
        "- **Universal support**: Every programming language supports JSON\n",
        "- **Simplicity**: Easy to learn and implement\n",
        "- **Web standard**: Built into browsers and web technologies\n",
        "- **Ecosystem**: Vast ecosystem of tools and libraries\n",
        "\n",
        "JSON remains the go-to format for data exchange in digital humanities research because of its simplicity, readability, and universal support. Understanding JSON thoroughly will serve you well in any computational research project!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd9e412a",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### JSON in Digital Humanities\n",
        "\n",
        "- **Metadata**: Store information about texts, authors, publication dates.\n",
        "- **Analysis Results**: Save outputs from LLMs (e.g., named entities, summaries).\n",
        "- **Data Exchange**: Share research datasets between tools and collaborators.\n",
        "\n",
        "### Working with JSON in Python\n",
        "\n",
        "Import the `json` module:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e1aec7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Convert Python dict to JSON string\n",
        "data = {\"author\": \"Valdis\", \"topic\": \"Digital Humanities\"}\n",
        "json_str = json.dumps(data)\n",
        "\n",
        "# Convert JSON string back to Python dict\n",
        "parsed = json.loads(json_str)\n",
        "print(parsed[\"author\"])  # Output: Valdis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccf2d809",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Example: LLM API Response\n",
        "\n",
        "A typical LLM API response in JSON:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6275b044",
      "metadata": {},
      "outputs": [],
      "source": [
        "{\n",
        "  \"choices\": [\n",
        "    {\n",
        "      \"message\": {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"The main theme of this novel is exile and identity.\"\n",
        "      }\n",
        "    }\n",
        "  ],\n",
        "  \"usage\": {\n",
        "    \"prompt_tokens\": 50,\n",
        "    \"completion_tokens\": 20\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00c5a24a",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Tips for Digital Humanities Researchers\n",
        "\n",
        "- **Validate your JSON**: Use online tools like [jsonlint.com](https://jsonlint.com).\n",
        "- **Document your data**: Add clear keys and structure for future use.\n",
        "- **Use JSON for reproducibility**: Save analysis results for sharing and publication.\n",
        "\n",
        "JSON is a foundational skill for working with APIs and digital research tools!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a54a5bc0",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "## OpenRouter API\n",
        "\n",
        "OpenRouter is a unified API that provides access to multiple LLM providers through a single interface. This makes it convenient to experiment with different models and compare their performance for humanities research tasks.\n",
        "\n",
        "### What is OpenRouter?\n",
        "\n",
        "**OpenRouter** acts as a gateway to dozens of different LLM providers, allowing you to:\n",
        "- **Access multiple models** through a single API interface\n",
        "- **Compare performance** across different LLMs for the same task\n",
        "- **Switch between models** without changing your code structure\n",
        "- **Manage costs** by choosing models based on budget and performance needs\n",
        "\n",
        "### Key Advantages for Digital Humanities Research\n",
        "\n",
        "#### **1. Model Diversity**\n",
        "- **OpenAI models**: GPT-3.5, GPT-4, GPT-4 Turbo\n",
        "- **Anthropic models**: Claude-3 Haiku, Sonnet, Opus\n",
        "- **Google models**: Gemini Pro, Gemini Flash\n",
        "- **Open source models**: Llama, Mistral, and many others\n",
        "- **Specialized models**: Fine-tuned for specific tasks\n",
        "\n",
        "#### **2. Cost Optimization**\n",
        "- **Transparent pricing**: See exact costs per model\n",
        "- **Choose by budget**: Use cheaper models for initial testing\n",
        "- **Scale appropriately**: Use powerful models only when needed\n",
        "- **Usage tracking**: Monitor your spending in real-time\n",
        "\n",
        "#### **3. Unified Interface**\n",
        "- **Consistent API**: Same request format for all models\n",
        "- **Easy switching**: Change models by modifying one parameter\n",
        "- **Standard responses**: Uniform JSON response structure\n",
        "- **Simplified authentication**: One API key for all providers\n",
        "\n",
        "### OpenRouter API Structure\n",
        "\n",
        "#### **Base URL**\n",
        "```\n",
        "https://openrouter.ai/api/v1/chat/completions\n",
        "```\n",
        "\n",
        "#### **Request Format**\n",
        "All requests use the same JSON structure, regardless of the underlying model:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"model\": \"model-provider/model-name\",\n",
        "  \"messages\": [\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"System instructions here\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\", \n",
        "      \"content\": \"User query here\"\n",
        "    }\n",
        "  ],\n",
        "  \"max_tokens\": 1000,\n",
        "  \"temperature\": 0.1\n",
        "}\n",
        "```\n",
        "\n",
        "### Authentication and Security\n",
        "\n",
        "#### **API Key Management**\n",
        "OpenRouter requires an API key for authentication. For security:\n",
        "- **Never hardcode** API keys in your scripts\n",
        "- **Use environment variables** to store sensitive credentials\n",
        "- **Rotate keys regularly** for enhanced security\n",
        "- **Monitor usage** to detect unauthorized access\n",
        "\n",
        "#### **Setting Up Environment Variables**\n",
        "Store your API key in an environment variable for secure access:\n",
        "\n",
        "```python\n",
        "import os\n",
        "\n",
        "# Retrieve API key from environment variable\n",
        "api_key = os.getenv(\"OPENROUTER_API_KEY_LNB\")\n",
        "if not api_key:\n",
        "    print(\"‚ùå Error: OPENROUTER_API_KEY_LNB environment variable not found\")\n",
        "    print(\"Please set your OpenRouter API key as an environment variable\")\n",
        "else:\n",
        "    print(\"‚úÖ API key loaded successfully\")\n",
        "```\n",
        "\n",
        "### Practical Example: Analyzing Latvian Literature\n",
        "\n",
        "Let's create a practical example that demonstrates how to use OpenRouter for digital humanities research focused on Latvian literature:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "88e3821a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üá±üáª LATVIAN LITERATURE ANALYSIS WITH OPENROUTER API\n",
            "============================================================\n",
            "‚úÖ API key loaded successfully\n",
            "üîÑ Sending request to OpenRouter API...\n",
            "üìù Model: openai/gpt-3.5-turbo\n",
            "üìä Max tokens: 1000\n",
            "üå°Ô∏è Temperature: 0.1\n",
            "--------------------------------------------------\n",
            "‚úÖ Analysis completed successfully!\n",
            "============================================================\n",
            "üìñ LITERARY ANALYSIS RESULTS\n",
            "============================================================\n",
            "1. Main themes:\n",
            "The main themes in this text excerpt revolve around exile, identity, and belonging. The mention of the protagonist, MƒÅrti≈Ü≈°, looking at the distant horizon where the sun is setting behind the forest wall evokes a sense of longing and nostalgia for his homeland (\"dzimteni\"). The juxtaposition of sadness for the homeland left behind and hope for a new life in a foreign land reflects the complex emotions of exile and the search for belonging. The questions posed about whether he will return home and if his children will speak Latvian also highlight the themes of identity and cultural preservation in the face of displacement.\n",
            "\n",
            "2. Emotional tone and sentiment:\n",
            "The emotional tone of the text is melancholic and reflective. The description of the autumn wind blowing over the sea of yellow leaves sets a somber mood, mirroring the protagonist's feelings of sadness for his homeland. The mention of hope for a new life in a foreign land adds a touch of optimism amidst the nostalgia and longing, creating a bittersweet emotional sentiment.\n",
            "\n",
            "3. Cultural and geographical references:\n",
            "The text contains cultural references to Latvian identity and the sense of belonging to a specific homeland. The mention of the protagonist's attachment to his \"dzimteni\" (homeland) and the uncertainty about whether his children will continue to speak Latvian allude to the importance of cultural heritage and language in maintaining a sense of identity in exile. The mention of the sun setting behind the forest wall and the yellow sea of leaves also evoke a vivid autumnal Latvian landscape.\n",
            "\n",
            "4. Literary devices used:\n",
            "The text employs imagery to create a vivid sensory experience for the reader, such as the autumn wind blowing over the sea of yellow leaves and the sun slowly sinking behind the forest wall. These visual and tactile descriptions enhance the emotional depth of the text. The use of rhetorical questions (\"Vai vi≈Ü≈° kad atgriezƒ´sies mƒÅjƒÅs? Vai vi≈Üa bƒìrni runƒÅs latviski?\") engages the reader and emphasizes the protagonist's internal conflict and uncertainty about his future.\n",
            "\n",
            "5. Historical or social context suggested:\n",
            "The text suggests a historical context of exile and displacement, possibly referencing the experiences of Latvians who were forced to leave their homeland during periods of political turmoil or occupation. The mention of the protagonist's concerns about his children speaking Latvian hints at the challenges faced by diaspora communities in preserving their cultural heritage and language across generations. The themes of exile and longing for the homeland resonate with the broader historical narrative of Latvian history marked by periods of occupation and emigration.\n",
            "============================================================\n",
            "\n",
            "üìä API Usage Statistics:\n",
            "   ‚Ä¢ Prompt tokens: 276\n",
            "   ‚Ä¢ Completion tokens: 538\n",
            "   ‚Ä¢ Total tokens: 814\n",
            "\n",
            "üíæ Analysis completed at: 2025-08-03T23:57:09.450199\n",
            "You can now save this analysis to a file or database for your research.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "def analyze_latvian_text_with_openrouter():\n",
        "    \"\"\"\n",
        "    Demonstrate OpenRouter API usage for Latvian literature analysis\n",
        "    \"\"\"\n",
        "    \n",
        "    # Step 1: Get API key from environment\n",
        "    api_key = os.getenv(\"OPENROUTER_API_KEY_LNB\")\n",
        "    \n",
        "    if not api_key:\n",
        "        print(\"‚ùå Error: OPENROUTER_API_KEY_LNB environment variable not found\")\n",
        "        print(\"\\nTo set up your API key:\")\n",
        "        print(\"1. Get an API key from https://openrouter.ai/\")\n",
        "        print(\"2. Set environment variable: OPENROUTER_API_KEY_LNB=your_key_here\")\n",
        "        return None\n",
        "    \n",
        "    print(\"‚úÖ API key loaded successfully\")\n",
        "    \n",
        "    # Step 2: Set up the API endpoint and headers\n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "    \n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": \"https://www.digitalhumanities.lv/bssdh/2025/\",  # Your project URL\n",
        "        \"X-Title\": \"BSSDH 2025 LLM Workshop - Latvian Literature Analysis\"\n",
        "    }\n",
        "    \n",
        "    # Step 3: Sample Latvian text for analysis\n",
        "    # This is a fictional excerpt inspired by Latvian literary traditions\n",
        "    latvian_text = \"\"\"\n",
        "    Rudens vƒìj≈° p≈´ta pƒÅr dzelteno lapu j≈´ru. MƒÅrti≈Ü≈° skatƒ´jƒÅs uz tƒÅlo horizontu, \n",
        "    kur saule lƒìni iegrimst aiz me≈æu sienas. Vi≈Üa sirdƒ´ valdƒ´ja skumjas par atstƒÅto \n",
        "    dzimteni, bet arƒ´ cerƒ´ba uz jaunu dzƒ´vi sve≈°umƒÅ. Vai vi≈Ü≈° kad atgriezƒ´sies \n",
        "    mƒÅjƒÅs? Vai vi≈Üa bƒìrni runƒÅs latviski?\n",
        "    \"\"\"\n",
        "    \n",
        "    # Step 4: Create the request payload\n",
        "    request_data = {\n",
        "        \"model\": \"openai/gpt-3.5-turbo\",  # Using GPT-3.5 for cost-effectiveness\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"\"\"You are a digital humanities expert specializing in Latvian literature \n",
        "                and culture. Analyze texts for themes, emotional content, cultural references, \n",
        "                and literary devices. Provide detailed, scholarly analysis.\"\"\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"Please analyze this Latvian text excerpt for the following elements:\n",
        "\n",
        "1. Main themes (especially themes of exile, identity, belonging)\n",
        "2. Emotional tone and sentiment\n",
        "3. Cultural and geographical references\n",
        "4. Literary devices used\n",
        "5. Historical or social context suggested\n",
        "\n",
        "Text to analyze:\n",
        "{latvian_text}\n",
        "\n",
        "Please provide your analysis in English, with specific references to the Latvian text.\"\"\"\n",
        "            }\n",
        "        ],\n",
        "        \"max_tokens\": 1000,\n",
        "        \"temperature\": 0.1,  # Low temperature for consistent, analytical responses\n",
        "        \"top_p\": 0.9\n",
        "    }\n",
        "    \n",
        "    # Step 5: Make the API request\n",
        "    try:\n",
        "        print(\"üîÑ Sending request to OpenRouter API...\")\n",
        "        print(f\"üìù Model: {request_data['model']}\")\n",
        "        print(f\"üìä Max tokens: {request_data['max_tokens']}\")\n",
        "        print(f\"üå°Ô∏è Temperature: {request_data['temperature']}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        response = requests.post(url, headers=headers, json=request_data, timeout=30)\n",
        "        \n",
        "        # Check if request was successful\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        # Step 6: Parse the JSON response\n",
        "        result = response.json()\n",
        "        \n",
        "        # Step 7: Extract and display the analysis\n",
        "        if 'choices' in result and len(result['choices']) > 0:\n",
        "            analysis = result['choices'][0]['message']['content']\n",
        "            \n",
        "            print(\"‚úÖ Analysis completed successfully!\")\n",
        "            print(\"=\" * 60)\n",
        "            print(\"üìñ LITERARY ANALYSIS RESULTS\")\n",
        "            print(\"=\" * 60)\n",
        "            print(analysis)\n",
        "            print(\"=\" * 60)\n",
        "            \n",
        "            # Display usage statistics\n",
        "            if 'usage' in result:\n",
        "                usage = result['usage']\n",
        "                print(f\"\\nüìä API Usage Statistics:\")\n",
        "                print(f\"   ‚Ä¢ Prompt tokens: {usage.get('prompt_tokens', 'N/A')}\")\n",
        "                print(f\"   ‚Ä¢ Completion tokens: {usage.get('completion_tokens', 'N/A')}\")\n",
        "                print(f\"   ‚Ä¢ Total tokens: {usage.get('total_tokens', 'N/A')}\")\n",
        "            \n",
        "            # Return the full response for further processing\n",
        "            return {\n",
        "                'text_analyzed': latvian_text,\n",
        "                'analysis': analysis,\n",
        "                'model_used': request_data['model'],\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'usage_stats': result.get('usage', {}),\n",
        "                'full_response': result\n",
        "            }\n",
        "        \n",
        "        else:\n",
        "            print(\"‚ùå Error: No analysis returned from the API\")\n",
        "            return None\n",
        "            \n",
        "    except requests.exceptions.Timeout:\n",
        "        print(\"‚ùå Error: Request timed out. Please try again.\")\n",
        "        return None\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"‚ùå HTTP Error: {e}\")\n",
        "        if response.status_code == 401:\n",
        "            print(\"   This usually means your API key is invalid or expired.\")\n",
        "        elif response.status_code == 429:\n",
        "            print(\"   Rate limit exceeded. Please wait before making another request.\")\n",
        "        return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Request Error: {e}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"‚ùå Error: Invalid JSON response from API\")\n",
        "        return None\n",
        "\n",
        "# Run the analysis\n",
        "print(\"üá±üáª LATVIAN LITERATURE ANALYSIS WITH OPENROUTER API\")\n",
        "print(\"=\" * 60)\n",
        "result = analyze_latvian_text_with_openrouter()\n",
        "\n",
        "if result:\n",
        "    print(f\"\\nüíæ Analysis completed at: {result['timestamp']}\")\n",
        "    print(\"You can now save this analysis to a file or database for your research.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f20ba0ed",
      "metadata": {},
      "source": [
        "### Understanding the Code Structure\n",
        "\n",
        "#### **1. Environment Variable Setup**\n",
        "```python\n",
        "api_key = os.getenv(\"OPENROUTER_API_KEY_LNB\")\n",
        "```\n",
        "- **Secure access**: API key stored in environment variable\n",
        "- **Error handling**: Graceful failure if key not found\n",
        "- **Best practice**: Never hardcode sensitive credentials\n",
        "\n",
        "#### **2. Request Headers**\n",
        "```python\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {api_key}\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"HTTP-Referer\": \"https://bssdh.eu/\",\n",
        "    \"X-Title\": \"BSSDH 2025 LLM Workshop - Latvian Literature Analysis\"\n",
        "}\n",
        "```\n",
        "- **Authorization**: Bearer token authentication\n",
        "- **Content-Type**: Tells API we're sending JSON data\n",
        "- **HTTP-Referer**: Identifies your project (optional but recommended)\n",
        "- **X-Title**: Descriptive title for usage tracking\n",
        "\n",
        "#### **3. JSON Request Structure**\n",
        "```python\n",
        "request_data = {\n",
        "    \"model\": \"openai/gpt-3.5-turbo\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"System instructions...\"},\n",
        "        {\"role\": \"user\", \"content\": \"User query...\"}\n",
        "    ],\n",
        "    \"max_tokens\": 1000,\n",
        "    \"temperature\": 0.1\n",
        "}\n",
        "```\n",
        "- **model**: Specifies which LLM to use\n",
        "- **messages**: Conversation format with system and user roles\n",
        "- **max_tokens**: Limits response length (controls cost)\n",
        "- **temperature**: Controls creativity (0 = deterministic, 1 = creative)\n",
        "\n",
        "#### **4. Error Handling**\n",
        "The code includes comprehensive error handling for:\n",
        "- **Authentication errors** (401): Invalid API key\n",
        "- **Rate limiting** (429): Too many requests\n",
        "- **Network timeouts**: Connection issues\n",
        "- **JSON parsing errors**: Malformed responses\n",
        "\n",
        "### Popular Models for Digital Humanities\n",
        "\n",
        "#### **For Analysis Tasks**\n",
        "```python\n",
        "# Cost-effective for bulk analysis\n",
        "\"openai/gpt-3.5-turbo\"\n",
        "\n",
        "# More sophisticated analysis\n",
        "\"openai/gpt-4-turbo\"\n",
        "\n",
        "# Large context for long documents\n",
        "\"anthropic/claude-3-sonnet\"\n",
        "\n",
        "# Fast and economical\n",
        "\"google/gemini-flash-1.5\"\n",
        "```\n",
        "\n",
        "#### **For Multilingual Tasks**\n",
        "```python\n",
        "# Strong multilingual capabilities\n",
        "\"openai/gpt-4\"\n",
        "\n",
        "# Good for European languages\n",
        "\"anthropic/claude-3-opus\"\n",
        "\n",
        "# Open source alternative\n",
        "\"meta-llama/llama-3-70b-instruct\"\n",
        "```\n",
        "\n",
        "### Customizing for Your Research\n",
        "\n",
        "#### **System Prompts for Different Tasks**\n",
        "```python\n",
        "# For sentiment analysis\n",
        "system_prompt = \"\"\"You are an expert in sentiment analysis of historical texts. \n",
        "Analyze the emotional content and provide numerical scores for different emotions.\"\"\"\n",
        "\n",
        "# For named entity recognition\n",
        "system_prompt = \"\"\"You are a specialist in extracting names, places, and dates \n",
        "from historical documents. Focus on accurate identification and categorization.\"\"\"\n",
        "\n",
        "# For thematic analysis\n",
        "system_prompt = \"\"\"You are a literary scholar specializing in thematic analysis. \n",
        "Identify recurring themes, motifs, and symbolic elements in the text.\"\"\"\n",
        "```\n",
        "\n",
        "#### **Adjusting Parameters for Different Goals**\n",
        "```python\n",
        "# For creative interpretation (higher temperature)\n",
        "request_data[\"temperature\"] = 0.7\n",
        "\n",
        "# For factual analysis (lower temperature)\n",
        "request_data[\"temperature\"] = 0.1\n",
        "\n",
        "# For longer analysis (more tokens)\n",
        "request_data[\"max_tokens\"] = 2000\n",
        "\n",
        "# For concise summaries (fewer tokens)\n",
        "request_data[\"max_tokens\"] = 300\n",
        "```\n",
        "\n",
        "### Saving and Managing Results\n",
        "\n",
        "#### **Save Analysis to File**\n",
        "```python\n",
        "def save_analysis_to_file(result, filename):\n",
        "    \"\"\"Save analysis results to JSON file\"\"\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(result, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"‚úÖ Analysis saved to {filename}\")\n",
        "\n",
        "# Usage\n",
        "if result:\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"latvian_analysis_{timestamp}.json\"\n",
        "    save_analysis_to_file(result, filename)\n",
        "```\n",
        "\n",
        "#### **Create Research Database**\n",
        "```python\n",
        "def add_to_research_database(result, database_file=\"research_analyses.json\"):\n",
        "    \"\"\"Add analysis to cumulative research database\"\"\"\n",
        "    try:\n",
        "        # Load existing database\n",
        "        with open(database_file, 'r', encoding='utf-8') as f:\n",
        "            database = json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        # Create new database\n",
        "        database = {\"analyses\": []}\n",
        "    \n",
        "    # Add new analysis\n",
        "    database[\"analyses\"].append(result)\n",
        "    \n",
        "    # Save updated database\n",
        "    with open(database_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(database, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"‚úÖ Analysis added to database. Total analyses: {len(database['analyses'])}\")\n",
        "```\n",
        "\n",
        "### Cost Management Tips\n",
        "\n",
        "#### **Monitor Token Usage**\n",
        "```python\n",
        "def estimate_cost(usage_stats, model_name):\n",
        "    \"\"\"Estimate cost based on token usage\"\"\"\n",
        "    # These are example rates - check OpenRouter for current pricing\n",
        "    pricing = {\n",
        "        \"openai/gpt-3.5-turbo\": {\"input\": 0.0015, \"output\": 0.002},  # per 1K tokens\n",
        "        \"openai/gpt-4\": {\"input\": 0.03, \"output\": 0.06},\n",
        "        \"anthropic/claude-3-sonnet\": {\"input\": 0.003, \"output\": 0.015}\n",
        "    }\n",
        "    \n",
        "    if model_name in pricing:\n",
        "        input_cost = (usage_stats.get('prompt_tokens', 0) / 1000) * pricing[model_name]['input']\n",
        "        output_cost = (usage_stats.get('completion_tokens', 0) / 1000) * pricing[model_name]['output']\n",
        "        total_cost = input_cost + output_cost\n",
        "        print(f\"üí∞ Estimated cost: ${total_cost:.6f}\")\n",
        "        return total_cost\n",
        "    else:\n",
        "        print(\"üí∞ Cost estimation not available for this model\")\n",
        "        return None\n",
        "```\n",
        "\n",
        "#### **Batch Processing Strategy**\n",
        "```python\n",
        "def analyze_multiple_texts(texts, model=\"openai/gpt-3.5-turbo\", delay=1):\n",
        "    \"\"\"Analyze multiple texts with rate limiting\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for i, text in enumerate(texts):\n",
        "        print(f\"Processing text {i+1}/{len(texts)}...\")\n",
        "        \n",
        "        # Modify the request for each text\n",
        "        request_data[\"messages\"][1][\"content\"] = f\"Analyze this text: {text}\"\n",
        "        \n",
        "        # Make request (using the same function structure as above)\n",
        "        result = make_api_request(request_data)\n",
        "        if result:\n",
        "            results.append(result)\n",
        "        \n",
        "        # Rate limiting - wait between requests\n",
        "        if i < len(texts) - 1:  # Don't wait after the last request\n",
        "            time.sleep(delay)\n",
        "    \n",
        "    return results\n",
        "```\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "This OpenRouter API introduction provides the foundation for:\n",
        "1. **Batch processing** multiple documents\n",
        "2. **Comparative analysis** using different models\n",
        "3. **Cost-effective research** with appropriate model selection\n",
        "4. **Reproducible workflows** with documented API calls\n",
        "\n",
        "In the next sessions, we'll build on this foundation to create more sophisticated analysis pipelines for specific digital humanities tasks.\n",
        "\n",
        "### Troubleshooting Common Issues\n",
        "\n",
        "#### **Authentication Problems**\n",
        "- Verify API key is correct and active\n",
        "- Check environment variable name matches exactly\n",
        "- Ensure no extra spaces in the API key\n",
        "\n",
        "#### **Rate Limiting**\n",
        "- Add delays between requests (time.sleep())\n",
        "- Use exponential backoff for retries\n",
        "- Monitor your usage on OpenRouter dashboard\n",
        "\n",
        "#### **Model Selection**\n",
        "- Start with cheaper models for testing\n",
        "- Use more powerful models for final analysis\n",
        "- Check model availability and pricing regularly\n",
        "\n",
        "The OpenRouter API provides a powerful, flexible gateway to state-of-the-art language models, making it an ideal choice for digital humanities research requiring sophisticated text analysis capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79b78144",
      "metadata": {},
      "source": [
        "## Practical Assignment - Create your own LLM API request\n",
        "\n",
        "You have all been e-mailed an API key for the OpenRouter API. Your task is to create a new query that analyzes a document of your choice using the OpenRouter API."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "075d809b",
      "metadata": {},
      "source": [
        "### Alternatives to system environment variables - load API keys by copy pasting them into variable\n",
        "\n",
        "Your API keys are valuable and should be kept secure. One way to do this is to paste them into a prompt that asks for the key, rather than hardcoding them into your script. This way, you can keep your keys out of version control and avoid accidental exposure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "81793df8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ API key set successfully. You can now use it in your scripts.\n"
          ]
        }
      ],
      "source": [
        "# let's ask for API key using user input that does not show the input\n",
        "import getpass\n",
        "api_key = getpass.getpass(\"Please enter your OpenRouter API key: \")\n",
        "os.environ[\"OPENROUTER_API_KEY_LNB\"] = api_key # this is not strictly necessary, but it is a good practice to keep your API keys in environment variables\n",
        "print(\"‚úÖ API key set successfully. You can now use it in your scripts.\")\n",
        "# main thing is not to print the key to the console because in notebooks you can inadvertently expose it and then submit it to a public repository"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
