{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a25e61f",
   "metadata": {},
   "source": [
    "# Using LLMs in Humanities Research via API\n",
    "\n",
    "## Welcome to the Workshop on Using LLMs in Humanities Research via API!\n",
    "\n",
    "Welcome into deeper world of Large Language Models (LLMs) and their applications in humanities research! In an era where artificial intelligence is transforming every field of study, the humanities are experiencing a revolutionary shift in how we approach text analysis, interpretation, and research methodologies.\n",
    "\n",
    "### Why This Workshop Matters\n",
    "\n",
    "The digital transformation of humanities research has opened unprecedented opportunities for scholars to analyze vast corpora of text, uncover hidden patterns, and gain new insights into human culture and expression. Large Language Models represent the cutting edge of this transformation, offering powerful tools for:\n",
    "\n",
    "- **Automated text analysis** at scale previously impossible for human researchers\n",
    "- **Cross-lingual research** capabilities that break down language barriers\n",
    "- **Pattern recognition** in literary and historical texts\n",
    "- **Assistance with translation and transcription** of historical documents\n",
    "- **Enhanced accessibility** to digitized cultural heritage materials\n",
    "\n",
    "### Workshop Goals\n",
    "\n",
    "By the end of this three-session workshop, you will:\n",
    "\n",
    "1. **Understand the fundamentals** of Large Language Models and their capabilities for humanities research\n",
    "2. **Master API interactions** to programmatically access and utilize various LLM services\n",
    "3. **Learn practical applications** including concept mining, named entity recognition, and text analysis\n",
    "4. **Develop skills** in prompt engineering for humanities-specific tasks\n",
    "5. **Address real challenges** such as working with OCR errors in historical texts\n",
    "6. **Gain hands-on experience** with tools for error correction and translation\n",
    "7. **Build confidence** in integrating AI technologies into your research workflow\n",
    "\n",
    "### What Makes This Approach Special\n",
    "\n",
    "Rather than relying on simple chat interfaces, you'll learn to harness the full power of LLMs through API access, enabling:\n",
    "- **Batch processing** of large document collections\n",
    "- **Customizable workflows** tailored to your specific research needs\n",
    "- **Reproducible research** methods with documented processes\n",
    "- **Integration** with existing digital humanities tools and methodologies\n",
    "\n",
    "## Session 1 11.30-13.00 - Introduction to LLMs and APIs\n",
    "\n",
    "In our first session, we will explore the basics of Large Language Models (LLMs) and how to interact with them using APIs. We will cover the following topics:\n",
    "- **Setting Up Your Environment**: Instructions on how to set up your programming environment to interact with LLM APIs.\n",
    "- **What are LLMs?**: An introduction to Large Language Models, their capabilities, and how they can be applied in humanities research.\n",
    "- **Understanding APIs**: A brief overview of what APIs are, how they work, and why they are essential for accessing LLMs.\n",
    "- **Understanding JSON**: An introduction to JSON (JavaScript Object Notation), the data format commonly used for API responses, and how to work with it in Python.\n",
    "- **OpenRouter API**: Introduction to the OpenRouter API, which provides access to various LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5b2d02",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4c557b7",
   "metadata": {},
   "source": [
    "## About the Instructors and Assistants\n",
    "\n",
    "**Valdis Saulespurƒìns** works as a researcher and developer at the National Library of Latvia. Additionally, he is a lecturer at Riga Technical University, where he teaches Python, JavaScript, and other computer science subjects. Valdis has a specialization in Machine Learning and Data Analysis, and he enjoys transforming disordered data into structured knowledge. With more than 30 years of programming experience, Valdis began his professional career by writing programs for quantum scientists at the University of California, Santa Barbara. Before moving into teaching, he developed software for a radio broadcast equipment manufacturer. Valdis holds a Master's degree in Computer Science from the University of Latvia.\n",
    "\n",
    "**Anda BaklƒÅne** is a researcher and curator of digital research services at the National Library of Latvia. She teaches Introduction to Digital Humanities and Digital Social Sciences and Text Analysis and Visualization courses at the University of Latvia. Anda holds a master's degree in philosophy and a PhD in literary theory. Her research interests include Latvian contemporary literature, metaphor, models, distant reading, and academic data visualization.\n",
    "\n",
    "**Viesturs Vƒìveris** is a researcher and developer at the National Library of Latvia. He has a background in computer science and digital humanities, with a focus on developing tools and methodologies for text analysis and data visualization. Viesturs is passionate about making digital research more accessible and effective for scholars in the humanities.\n",
    "\n",
    "**Haralds Matulis** is a researcher and also organizer of this iteration of Baltic Summer School of Digital Humanities. He has a background in digital humanities and is interested in the intersection of technology and humanities research. Haralds is dedicated to promoting digital literacy and innovation in the humanities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce4708",
   "metadata": {},
   "source": [
    "## Interactive Version of the Notebook\n",
    "\n",
    "### Open in Google Colab\n",
    "<a href=\"https://colab.research.google.com/github/ValRCS/BSSDH_2025_workshop_LLM_API/blob/main/notebooks/workshop_session_1.ipynb?flush_cache=true\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "### Static vs Interactive Notebooks\n",
    "\n",
    "**Static Notebooks** (like what you might see on GitHub) are read-only versions that display the content but don't allow you to:\n",
    "- Execute code cells\n",
    "- Modify content\n",
    "- Install packages\n",
    "- Save your changes\n",
    "\n",
    "**Interactive Notebooks** allow you to:\n",
    "- **Execute code cells** by pressing Shift+Enter or clicking the play button\n",
    "- **Edit and experiment** with code in real-time\n",
    "- **Install Python packages** as needed\n",
    "- **Save your work** and download modified notebooks\n",
    "- **See live outputs** including text, tables, and visualizations\n",
    "\n",
    "### About Google Colab\n",
    "\n",
    "**Google Colab** (Colaboratory) is a free, cloud-based Jupyter notebook environment that:\n",
    "\n",
    "- **Requires no setup** - runs entirely in your web browser\n",
    "- **Provides free computational resources** including CPU, GPU, and limited TPU access\n",
    "- **Comes pre-installed** with most common data science and machine learning libraries\n",
    "- **Integrates seamlessly** with Google Drive for saving and sharing notebooks\n",
    "- **Supports real-time collaboration** allowing multiple people to work on the same notebook\n",
    "- **Automatically saves** your progress to Google Drive\n",
    "\n",
    "**Getting Started with Colab:**\n",
    "1. Click the \"Open in Colab\" badge above\n",
    "2. Sign in with your Google account (required)\n",
    "3. The notebook will open in a new tab\n",
    "4. You can immediately start executing cells by clicking the play button (‚ñ∂Ô∏è) or pressing Shift+Enter\n",
    "\n",
    "**üí° Pro Tip:** Right-click the Colab badge and select \"Open link in new tab\" to keep this reference page open while working in the interactive notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2605c0",
   "metadata": {},
   "source": [
    "## Setting Up Your Environment\n",
    "\n",
    "To interact with LLM APIs effectively, we need to set up our programming environment with the necessary libraries and configurations. This includes installing required packages and setting up API credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0354b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an interactive notebook for the BSSDH 2025 workshop on LLMs and APIs.\n",
      "Python version: 3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]\n",
      "Today's date and time: 2025-07-30 12:01:19.490619\n",
      "JSON module imported successfully.\n",
      "Path from pathlib imported successfully.\n",
      "Will import external libraries if available.\n",
      "Requests library version: 2.32.4\n",
      "TQDM library version: 4.67.1\n",
      "OpenAI library version: 1.97.1\n"
     ]
    }
   ],
   "source": [
    "# Let's print some basic information about this interactive notebook\n",
    "print(\"This is an interactive notebook for the BSSDH 2025 workshop on LLMs and APIs.\")\n",
    "# first let's see what Python version we are using\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "# now today's date and time\n",
    "from datetime import datetime\n",
    "print(f\"Today's date and time: {datetime.now()}\")\n",
    "# we will need to work with JSON data, so let's import the json module\n",
    "import json\n",
    "print(\"JSON module imported successfully.\")\n",
    "# we will need to read and write files so let's import pathlib\n",
    "from pathlib import Path\n",
    "print(\"Path from pathlib imported successfully.\")\n",
    "# TODO for those with some experience it can be useful to print more information about the environment, free memory, drives, etc.\n",
    "print(\"Will import external libraries if available.\")\n",
    "# Let's also check if we have the requests library installed, which is commonly used for making API calls\n",
    "try:\n",
    "    import requests\n",
    "    print(f\"Requests library version: {requests.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Requests library is not installed. You can install it using 'pip install requests'.\")\n",
    "\n",
    "# let's install tqdm for progress bars if not already installed\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    # import version\n",
    "    from tqdm import __version__ as tqdm_version\n",
    "    print(f\"TQDM library version: {tqdm_version}\")\n",
    "except ImportError:\n",
    "    print(\"TQDM library is not installed. You can install it using 'pip install tqdm'.\")\n",
    "\n",
    "# now let's try importing OpenAI's library if available\n",
    "try:\n",
    "    import openai\n",
    "    print(f\"OpenAI library version: {openai.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"OpenAI library is not installed. You can install it using 'pip install openai'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dad0ead",
   "metadata": {},
   "source": [
    "### Why Check System Information and Library Versions?\n",
    "\n",
    "**Environment Documentation** is crucial for reproducible research and troubleshooting. Here's why we print this information:\n",
    "\n",
    "#### **1. Reproducibility**\n",
    "- **Version consistency**: Different library versions can produce different results\n",
    "- **Environment documentation**: Future researchers (including yourself) can recreate the exact same setup\n",
    "- **Research integrity**: Ensures your findings can be validated by others\n",
    "\n",
    "#### **2. Troubleshooting**\n",
    "- **Debugging assistance**: When code doesn't work, version information helps identify compatibility issues\n",
    "- **Support requests**: Technical support often requires knowing your exact environment setup\n",
    "- **Error diagnosis**: Many errors are version-specific and can be quickly resolved with this information\n",
    "\n",
    "#### **3. Best Practices in Digital Humanities**\n",
    "- **Methodological transparency**: Document all tools and versions used in your research\n",
    "- **Collaboration**: Team members can ensure they're using compatible environments\n",
    "- **Publication standards**: Many journals now require detailed technical specifications\n",
    "\n",
    "#### **4. API Compatibility**\n",
    "- **Service requirements**: Different LLM APIs may require specific library versions\n",
    "- **Feature availability**: Newer features might only be available in recent library versions\n",
    "- **Security updates**: Ensures you're using libraries with the latest security patches\n",
    "\n",
    "**üí° Pro Tip**: Always run this environment check at the beginning of your research sessions to catch any changes that might affect your results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e87398b",
   "metadata": {},
   "source": [
    "## What are LLMs?\n",
    "\n",
    "**Large Language Models (LLMs)** are sophisticated artificial intelligence systems trained on vast collections of text data to understand, generate, and manipulate human language. Think of them as extremely well-read digital assistants that have absorbed millions of books, articles, websites, and documents, enabling them to engage with text in remarkably human-like ways.\n",
    "\n",
    "### How LLMs Work: The Basics\n",
    "\n",
    "LLMs use a technology called **transformer architecture** (you don't need to understand the technical details!) that allows them to:\n",
    "\n",
    "1. **Predict the next word** in a sequence based on context\n",
    "2. **Understand relationships** between words, sentences, and concepts\n",
    "3. **Generate coherent text** that follows patterns learned from training data\n",
    "4. **Transfer knowledge** from one domain to another\n",
    "\n",
    "### Key Terms for Digital Humanities\n",
    "\n",
    "#### **Training Data**\n",
    "The massive collection of texts used to teach the LLM. This typically includes:\n",
    "- Books and literature from various periods and cultures\n",
    "- Academic papers and journals\n",
    "- News articles and magazines\n",
    "- Web content and reference materials\n",
    "- **Important**: The quality and diversity of training data affects what the model \"knows\"\n",
    "\n",
    "#### **Tokens**\n",
    "The basic units of text that LLMs process. A token can be:\n",
    "- A whole word (\"humanities\")\n",
    "- Part of a word (\"human\" + \"ities\")\n",
    "- Punctuation marks\n",
    "- **Why it matters**: API costs are often calculated per token\n",
    "\n",
    "#### **Context Window**\n",
    "The amount of text an LLM can \"remember\" at once, measured in tokens. Common sizes:\n",
    "- **GPT-3.5**: ~4,000 tokens (‚âà3,000 words)\n",
    "- **GPT-4**: ~8,000-32,000 tokens\n",
    "- **Claude**: ~100,000+ tokens\n",
    "- **Why it matters**: Determines how much text you can analyze at once\n",
    "\n",
    "#### **Prompt**\n",
    "The input text you give to an LLM to get a response. Effective prompting is crucial for good results.\n",
    "\n",
    "#### **Fine-tuning**\n",
    "The process of further training a model on specific data to improve performance for particular tasks.\n",
    "\n",
    "### Applications in Digital Humanities\n",
    "\n",
    "#### **1. Text Analysis**\n",
    "- **Sentiment analysis** of historical documents\n",
    "- **Thematic analysis** across large corpora\n",
    "- **Stylometric analysis** for authorship attribution\n",
    "- **Content classification** and categorization\n",
    "\n",
    "#### **2. Language Processing**\n",
    "- **Translation** of historical texts\n",
    "- **Transcription** assistance for handwritten documents\n",
    "- **OCR error correction** in digitized materials\n",
    "- **Modernization** of archaic language\n",
    "\n",
    "#### **3. Research Assistance**\n",
    "- **Literature reviews** and source discovery\n",
    "- **Citation analysis** and bibliography generation\n",
    "- **Concept mapping** and knowledge extraction\n",
    "- **Hypothesis generation** from patterns in data\n",
    "\n",
    "#### **4. Content Generation**\n",
    "- **Metadata generation** for digital collections\n",
    "- **Summary creation** for large document sets\n",
    "- **Educational material** development\n",
    "- **Interactive exhibits** and digital storytelling\n",
    "\n",
    "### Limitations and Considerations\n",
    "\n",
    "#### **Accuracy Concerns**\n",
    "- LLMs can generate plausible but incorrect information (**hallucinations**)\n",
    "- Always verify important claims against primary sources\n",
    "- Use multiple models and cross-check results\n",
    "\n",
    "#### **Bias and Representation**\n",
    "- Training data reflects societal biases\n",
    "- May underrepresent certain cultures, languages, or perspectives\n",
    "- Critical evaluation is essential, especially for sensitive topics\n",
    "\n",
    "#### **Temporal Knowledge**\n",
    "- Models have knowledge cutoff dates\n",
    "- May not know about recent events or publications\n",
    "- Historical accuracy varies by period and region\n",
    "\n",
    "#### **Language Coverage**\n",
    "- Performance varies significantly across languages\n",
    "- Better results for well-represented languages (English, major European languages)\n",
    "- Limited effectiveness for minority or historical languages\n",
    "\n",
    "### Popular LLM Models for Research\n",
    "\n",
    "#### **OpenAI's GPT Series**\n",
    "- **GPT-3.5**: Fast, cost-effective for many tasks\n",
    "- **GPT-4**: More capable, better reasoning, higher cost\n",
    "- **Strengths**: General knowledge, writing quality\n",
    "- **Best for**: Text generation, analysis, general research tasks\n",
    "\n",
    "#### **Anthropic's Claude**\n",
    "- **Claude-3**: Various sizes (Haiku, Sonnet, Opus)\n",
    "- **Strengths**: Large context windows, careful reasoning\n",
    "- **Best for**: Long document analysis, ethical considerations\n",
    "\n",
    "#### **Google's Gemini**\n",
    "- **Gemini Pro**: Competitive with GPT-4\n",
    "- **Strengths**: Multimodal capabilities, integration with Google services\n",
    "- **Best for**: Research integration, document processing\n",
    "\n",
    "#### **Open Source Models**\n",
    "- **Llama 2/3**: Meta's open-source models\n",
    "- **Mistral**: European open-source alternative\n",
    "- **Benefits**: Transparency, customization, data privacy\n",
    "\n",
    "### Getting Started: Questions to Ask\n",
    "\n",
    "Before using LLMs in your research, consider:\n",
    "\n",
    "1. **What specific task** do you want to accomplish?\n",
    "2. **How much text** will you be processing?\n",
    "3. **What level of accuracy** do you need?\n",
    "4. **Are there privacy concerns** with your data?\n",
    "5. **What's your budget** for API usage?\n",
    "6. **Do you need real-time results** or can processing take time?\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the following sections, we'll explore how to interact with these powerful models through APIs, enabling you to integrate LLM capabilities into your research workflows systematically and reproducibly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be804e",
   "metadata": {},
   "source": [
    "## Understanding APIs\n",
    "\n",
    "APIs (Application Programming Interfaces) are interfaces that allow different software applications to communicate with each other. They provide a standardized way to access services and data from external systems, making them essential for accessing LLMs programmatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dae5b36",
   "metadata": {},
   "source": [
    "## Understanding JSON\n",
    "\n",
    "JSON (JavaScript Object Notation) is a lightweight data format commonly used for API responses. It's human-readable and easy to work with in Python, making it ideal for handling structured data from LLM APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a282a2f",
   "metadata": {},
   "source": [
    "## OpenRouter API\n",
    "\n",
    "OpenRouter is a unified API that provides access to multiple LLM providers through a single interface. This makes it convenient to experiment with different models and compare their performance for humanities research tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
